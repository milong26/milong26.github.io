---
title: AN EMULATOR FOR FINE-TUNING LARGE LANGUAGE  MODELS USING SMALL LANGUAGE MODELS
tags: fine-tune
mathjax: true
categories: paper
date: 2024-11-04 16:20:53
---


百篇paper计划(11/100)，用小模型微调大模型。

<!--more-->
- 论文标题：AN EMULATOR FOR FINE-TUNING LARGE LANGUAGE  MODELS USING SMALL LANGUAGE MODELS
- rank：NeurIPS 2023 poster
- 打标签：微调，小模型
- 时间：2023年10月19日


# abstract
1. 目前：LMs通常2个训练管道
   1. 第一个预训练阶段：用很大、多样化的文本数据集
   2. 第二个微调（有时候是对齐）：用有针对性地例子或其他期望行为的规范
2. 问题：lm的知识和技能到底是来自预训练还是微调？
3. 解决：引入一个方法：模拟微调 emulated fine-tune
   1. 目的：解耦lm在预训练时和微调学到的知识
   2. 测试：如果我们在预训练时将一个大模型学习到的知识与在微调(反之亦然)时将一个小模型学习到的知识结合起来会发生什么?
   3. 来源：从人类偏好学习中发展出来的基于RL的框架
   4. 是啥：从分布中采样的方法
   5. 效果：在不同尺度上近似与训练和微调的结果。
4. 实验
   1. 在EFT上的实验表明，增加微调倾向于提高有用性，而增加预训练倾向于提高事实性。
   2. 在去耦尺度之外，证明了EFT可以在不需要额外训练的情况下，对有用性和无害性等相互竞争的行为特质进行测试时间调整。
   3. 最后，仿真微调的一个特例，我们称之为LM尺度扩展，通过将大的预训练模型与小的微调模型集成，避免了大的预训练模型的资源密集型微调，本质上是模仿大的预训练模型微调的结果。
   4. 尺度扩展在Llama、Llama - 2和Falcon系列中不断提高指令跟随模型的有用性和真实性，无需额外的超参数或训练。


# INTRODUCTION
前面重复叙述的就不说了，就是拓展abstract里面的背景和问题，老生常谈。但是EFT部分扩充了许多。

1. 功能（可以实现什么，下文的”阶段”是预训练或者微调）
   1. 直接研究当一个阶段被放大或缩小时变化的能力
   2. 在不增加计算成本的情况下逼近大模型微调结果的实际效益；
   3. 在测试时刻修改微调目标(例如,有益性和无害性之间的权衡)的能力，无需额外训练。
2. 怎么做的
   1. 基于简单因式分解，将微调后的语言模型的logits分解为
      1. 预训练的基模型的基对数概率 base log probabilities
      2.  "行为delta "，或基模型与微调模型的对数概率之差。该delta是微调中学习到的行为变化的紧凑表示，可以通过强化学习或贝叶斯推理框架进行验证。
   2. EFT可以模拟在一个尺度上预训练、在另一个尺度上微调的结果，通过增加模型在一个尺寸上计算的基对数概率和在另一个不同尺寸的模型上计算的行为delta。
   3. 例如，使用Llama - 2家族的模型，我们可以模拟70B尺度下的预训练和7B尺度下的微调结果，通过执行对数概率代数Llama-2-base 70B + (Llama-2-chat 7B - Llama-2-base 7B)，其中第一个项是基本对数概率，括号中的项是行为delta。
3. 实验分析
   1. 实验：利用eft，对多个模型族和数据集在不同尺度下的预训练和微调结果进行了分析。
   2. 结果：在规模上进行预训练可以获得更多的原始知识积累(提高了事实正确性)，而在更大的规模上进行微调可以获得更多的有用性(提高了用户满意度) 
   3. 其它结果：
      1. 还发现EFT可以通过一个我们称之为升尺度（up-scaling）的过程来提高小微调模型的性能，本质上是将小微调模型与一个更大的预训练模型集成在一起，而不需要对任何一个模型进行任何微调或修改。
      2. 我们的实验表明，在微调一个小的语言模型是可行的(如Falcon - 7B)，但微调一个大的语言模型不是由于资源限制(如Falcon - 180B)的场景中，上缩可以捕获微调大模型的大部分好处，而不需要执行任何模型微调。
      3. 最后，我们证明了EFT还可以通过混合不同权重的行为delta来模拟在测试时刻对微调目标的修改。
4. 总结贡献
   1. EFT框架
   2. 实验证明了缩放预训练导致事实性知识的提高，而缩放微调导致任务依从性的提高
   3. 模型升尺度技术，该技术使得一个小的微调模型和一个大的基模型能够近似微调一个大的基模型的计算密集型结果。



# RELATED WORK
1. 之前工作
   1. 通过无监督生成建模预训练的语言模型可以进行微调以进行通用对话生成很好的模型。
   2. 增加模型规模
2. 和本文相关的3个
    1. 对比解码
       1. 以前的：改进语言模型采样，将小语言模型(用一个小的常数超参数来刻度)的对数概率从大语言模型的对数概率中减去。
       2. 我们的：于将这种对数概率差异解释为对数重要度权重，并将其用于二次加权中另一个模型的对数概率，而不需要增加缩放超参数。
    2. 尺度对RLHF期间使用的奖励模型的影响，这可以解释为在我们的工作中对微调阶段进行缩放；然而，他们没有探索预训练规模，也没有调查任何一个规模对独立模型能力的影响。
    3. 训练了一个模型在抽样过程中对基模型的条件分布重新加权。我们的工作不同之处在于，EFT不需要训练新的奖赏模型，在强化学习中具有原则性的基础，并且由于将奖赏参数化为对数概率(拉斐洛夫等, 2023)的比例，因此可以更有效地根据词汇量进行扩展。



# EMULATED FINE-TUNING: DECOUPLING PRE-TRAINING & FINE-TUNING
包含以下内容

1. 描述模拟微调( EFT )的框架
2. 它如何使预训练和微调的规模解耦
3. 在实际中特别有用的模拟微调的特例——升尺度

## Preliminaries
eft将微调过程视为具有KL散度约束的强化学习( RL )，以防止从参考模型中发散，在这种情况下，参考模型指预训练模型。也就是说，我们把微调π ft的结果看成是问题的解

$$\pi_{\mathfrak{l}}=\pi^*(r,\pi_{\mathrm{ref}})=\arg\max_{\pi}\mathbb{E}_{x\sim p(x),y\sim\pi(\cdot|x)}\left[r(x,y)-\beta\mathrm{KL}(\pi(\cdot\mid x)\|\pi_{\mathrm{ref}}(\cdot\mid x))\right]$$

1. 目标函数：公式右侧的部分表示一个最大化问题，这里的argmax 表示我们要找到使得整个表达式达到最大值的策略𝜋
2. 期望：E表示对随机变量x和y的期望，x是按照p(x)采样的，P (x)是一个固定的提示分布(或数据集)
3. 奖励函数：r(x,y) 是一个奖励函数，它根据状态 x 和动作 y 计算得到奖励。目标是最大化这个奖励。
4. KL 散度：Kullback-Leibler 散度，用于测量策略 π 与参考策略 π_ref之间的差异。这个项的存在通常是为了引入某种正则化，使得优化的策略不会偏离参考策略太远。
5. 超参数：β控制KL对预训练模型（参考模型）的约束强度

之前已经有工作求出来这个解了：
$$\pi^{*}(r,\pi_{\mathrm{ref}})(y\mid x)={\frac{1}{Z(x)}}\pi_{\mathrm{ref}}(y\mid x)\exp\left({\frac{1}{\beta}}r(x,y)\right)$$

其中$Z(x)\ =\ \sum_{y}\pi_{\mathrm{ref}}(y\ \mid\ x)\exp\Big(\frac{1}{\beta}r(x,y)\Big)$

-----


关键的是，虽然EFT框架是基于RL的微调解释的，但它适用于任何微调模型，因为任何语言模型都可以看作是对KL约束的RL的解决方案，并对预训练模型进行约束。

具体来说，任何微调的语言模型π ft和预训练模型π Ref都可以通过一个奖励函数来建立映射关系，这个函数是$r_{π_{ft}}(x,y)$，使得KL约束的RL问题的解是微调模型
$$\pi^{*}(r_{\pi_{\mathrm{ft}}},\pi_{\mathrm{ref}})=\pi_{\mathrm{ft}}$$
其中

$$r_{\pi_{\mathrm{ft}}}(x,y)=\beta\log\frac{\pi_\mathrm{ft}(y|x)}{\pi_\mathrm{ref}(y|x)}$$

表示微调模型$\pi_{ft}$和参考模型$\pi_{ref}$在给定输入x下生成输出y的相对概率关系

利用语言模型和奖励之间的这种对偶性，对于任何从预训练模型π ref微调过来的语言模型π ft，我们都可以重写

$$\pi_{\mathrm{ft}}(y\mid x)=\pi_{\mathrm{ref}}(y\mid x)\exp\left(\underbrace{\log\frac{\pi_{\mathrm{ft}}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}}_{\mathrm{Implicit~reward}}\right)=\pi_{\mathrm{ref}}(y\mid x)\exp\left(r_{\pi_{\mathrm{ft}}}(x,y)\right)$$

换句话说，**微调模型π ft是以π ref为参考模型的KL约束报酬最大化问题的最优策略**，其报酬函数为
$$r_{\pi_\mathrm{ft}}(x,y)=\log{\frac{\pi_\mathrm{ft}(y|x)}{\pi_\mathrm{ref}(y|x)}}$$
我们现在已经清楚地描述了从预训练和微调中获得的信息的位置：预训练知识以基本的对数概率表示，而从微调中获得的能力则在奖励(从微调后的模型对数概率中减去基本对数概率的行为差值)中捕获。这种划分使得这些成分可以独立缩放，我们将在下面介绍。

----
这一节总共可以看成2部分，也就是之前用分割线画出来的2部分。

第一部分讲述EFT强调用RL约束下的微调后的模型用参考模型求解的计算公式。

第二部分对所有参考->微调模型来说，也可以看作屎用一个RL约束求最优策略。（虽然这部分的公式写得很混乱，作者真的推导出来了吗？）

## Scale Decoupling with EFT

给上一节最后的公式增加上下标，以明确用于计算相应条件句的模型大小，表示用于计算每个数量的模型的规模：

$$\pi_{M}^{N}(y\mid x)=\frac{1}{Z_{M}^{N}(x)}\pi_{\mathrm{pef}}^{N}(y\mid x)\exp\Bigl(r_{\pi}^{M}(x,y)\Bigr)\propto\pi_{\mathrm{ef}}^{N}(y\mid x)\frac{\pi^{M}(y\mid x)}{\pi_{\mathrm{nf}}^{M}(y\mid x)}$$

公式解读：

1. M规模的reward函数$r_{\pi}^{M}(x,y)=\mathrm{log}\,\frac{\pi^{M}(y|x)}{\pi_{\mathrm{ref}}^{M}(y|x)}$
2. $\propto$是近似的意思
3. 尺度解耦的配分函数是$Z_{M}^{N}(x)\:=\:\sum_{y}\pi_{\mathrm{ref}}^{N}(y\:\mid x)\exp\left(r^{M}(x,y)\right).$，它和上节最后一个公式不同，因为参考模型的大小不一样。

也就是说，$\pi^N_M$对应于模拟混合大小为N的模型在预训练时学习到的知识和大小为M的模型在微调时学习到的知识。（这个是根据公式定义给出来的，因为公式的上标）

而设定N = M对应的是对原政策的简单抽样，在本文中，我们特别探讨N 不等于 M的设定。

1. 对于N < M，我们模拟将小参考(预训练)模型的知识与大模型在微调过程中学习到的知识混合；
2. 对于N > M，我们模拟将一个大的预训练模型的知识与一个小模型在微调过程中学习到的知识混合。

### 模拟微调采样
我们的实验依赖于从EFT模型中抽取样本。为此，我们根据eq4(就是Scale Decoupling with EFT开头的第一个公式)计算每个token的条件，但使用了一个(难以解决的)序列级配分函数的逐时步近似a per-timestep approximation of the (intractable) sequence-level partition function：

$$\tilde{\pi}(y t\mid x,y_{<t})=\frac{1}{Z(x,y_{\lt t})}\pi_{\mathrm{ref}}^{N}(y_t\mid x,y_{\lt t})\frac{\pi^{M}(y_{t}\mid x,y_{\lt t})}{\pi_{\mathrm{ref}}^{M}(y_{t}\mid x,y_{\lt t})}$$

其中$Z(x,y_{<t})\:=\:\sum_{y_{t}}\:\pi_{\mathrm{ref}}^{N}(y_t\:\mid x,y_{\lt t})\frac{\pi^{M}(y_{t}|x,y_{\lt t})}{\pi_{\mathrm{ref}}^M(y_t \mid x,y_{<t}) }$是逐时步配分函数

以上公式都是把之前的公式填充了关于时间步t的下标。

最近在偏好学习中出现了类似的时间贪婪近似，它将偏好学习解释为不是学习一个奖励函数，而是学习一个优势函数。

## Computational Factors and Language Model Up-Scaling
定义：

1. N > M：up-scaling,，因为我们模拟了微调一个大模型的结果；
2. N < M：down-scaling，因为我们模拟了微调一个小模型的结果。

分析：

1. down-scaling假定在更大尺度上获得实际的微调模型，为了模拟在更小尺度上微调的结果。在这种情况下，简单地从大的微调模型中采样将在计算上更便宜和更有效。相比之下，up-scaling假设针对特定的任务或感兴趣的领域访问一个小的微调模型(计算成本低,可得)和一个大的预训练模型(其中许多是由拥有相当资源的组织自由释放的)。
2.  从N >> M的EFT模型中采样效率更高：EFT采样需要计算一个大小为N ( N -尺度预训练模型)的模型的前向传递，以及两个大小为M ( N尺度微调模型和N尺度预训练模型)的模型的前向传递。当N比M大得多时，这种计算成本与从实际的N尺度微调模型中采样的成本基本相同。
   > 这里第二个括号里面写错了吧，应该是 M尺度微调模型和M尺度预训练模型
3.若M相对于N较小，则存在投机解码(speculative decoding)对EFT的自然适应，其中M尺度微调模型为全EFT模型提出大块令牌进行检验。4.3节证实了投机解码可以在不改变模型样本的情况下，使从扩大模型中采样的速度提高近2.5倍。

结论是eft up-scaling更好


# EXPERIMENTS
1. 问题：当独立地调整预训练和微调时，能力会发生什么变化?
2. 解决：用EFT来评估各种量表组合的有用性和真实性。还尝试用EFT在不同的行为差值之间进行插值，例如在测试时间改变有用性和无害性之间的期望平衡，而不需要额外的训练。
3. 证明：利用EFT进行尺度上推需要修改小的微调模型对稀疏时间步长集的条件，通过使用投机解码来适应EFT尺度上推，从而在采样上获得了较大的加速比。
4. 消融：以显示过滤噪声令牌重加权的一些潜在好处。
5. 对模型生成的响应进行了人工评估，以验证基于GPT - 4的事实检查的准确性。
6. 基础设置
   1. 两个数据集来评估对话代理向用户提供有帮助的事实性帮助的能力。
      1. Anthropic Helpful- Harmless ( HH )对话数据集
      2. ELI5数据集的提示
   2. 三个独立的预训练语言模型和相应的微调模型。
      1. Llama - 1
         1. Llama - 1基模型 ( 7B和65B尺度)
         2. Vicuna微调模型 ( 7B和33B尺度) (没有70B Vicuna模型可用)计算隐式奖励。Vicuna模型是从Llama - 1基础模型微调过来的，基于用户与ChatGPT共享的对话。
      2. Llama - 2实验
         1. 7B和70B尺度下的Llama - 2基模型
         2. 7B和70B尺度下的Llama - 2 - chat模型来计算隐式奖励。Llama - 2 - chat模型是在Llama - 2基模型的基础上，结合监督学习和基于人类反馈的强化学习进行微调的。
      3. Falcon实验
         1. 7B和180B尺度下的Falcon基础模型
         2. 7B和180B尺度下的Falcon指令/聊天模型来计算隐式奖励。
   3. 以GPT - 4作为人类评价的代理指标来评价有用性、事实性和无害性。
      1. 通过提示GPT - 4来衡量有用性，以估计关键用户对聊天机器人给出的响应感到满意的概率；
      2. 通过促使GPT - 4统计给定答案中的事实错误来测量有用性；
      3. 通过提示GPT - 4来衡量有害性，以估计响应会对用户或社会造成伤害的可能性。在这两种情况下，GPT - 4都需要在做出决定前提供推理，以辅助可解释性。我们对温度为0的响应进行采样。
      4. 进一步，我们在4.5节中与众包注释者进行了比较，发现在GPT - 4与人类分歧的案例中，人类判断的错误，而不是GPT - 4的分析，导致了将近80 %的分歧。

## 从Scaling Pre-training 和 Fine - tuning这两个里面产生了什么能力?
1. 研究对象：独立伸缩(independently scaling)预训练和使用模拟微调进行微调的结果。
2. 咋搞：对于每个数据集和模型族，我们使用4个模型来生成对所有256个评价提示的响应，这4个模型包括
   1. 单独的小微调模型
   2. 单独的大微调模型
   3. Eft up scaling模型，模拟小规模微调和大规模预训练知识的结合；
   4. EFT down scaling模型，模拟大规模微调与小规模预训练知识的结合。
3. 其他设置：所有实验均采用温度为1.0的温度采样，没有使用top - p或top - k (除非另有说明)。
4. 结果：图3/图4/图6 数据稍微有点差别，但过程和结果是相似的
   1. 小的base+大的reward 和 大base+小reward 这两组相比
   2. 比较helpfulness和factuality
   3. 缩放预训练主要带来事实性的提升，而缩放微调主要带来感知有用性的提升。

## Eft实现了动态Test - time奖励插值
虽然解耦尺度（decoupling scale）是EFT的一个明显特征，但显式解耦预训练和微调的另一个好处是能够在采样时刻对奖励函数进行修改。考虑竞争性微调目标的情况，如有用性目标和无害性目标。因此，微调一般对话代理的一种观点是**试图在特定的有害预算下提供最大的帮助**。通过改变有害性预算，我们可以产生一个有益-有害的边界。然而，现有的微调过程在微调时刻存在特定的有益性和危害性折衷关系，并且这种折衷关系在采样时刻不容易被修改。

相比之下，通过模拟微调，这种对奖赏的测试时间调制是自然而直接的。图5展示了在7B预训练和微调尺度下，以及将预训练模型升级到70B时，在有用性和无害性之间进行插值的结果。我们看到清晰的、平滑的边界，并且扩大规模提供了一个帕累托改进（Pareto improvement），所有这些都不需要对每个权衡进行重新训练。

为了使用EFT对测试时刻的行为进行插值，我们假设存在两个小规模的微调模型，一个微调为纯有用性$\pi_{help}$，一个微调为纯无害性$\pi_{safe}$。在本实验中，我们使用Llama - 2 - 7B作为基模型，并用Anthropic-HH- HH数据集的有益基和无害基分割，使用DPO对这两个模型进行微调。

在测试时，对公式4（也就是scale decoupling with EFT，首次加入NM上下标的那个公式）中的r，我们使用插值奖励$r_{\lambda}^{M}(x,y)=\lambda r_{\mathrm{help}}^{M}(x,y)+(1-\lambda)\pi_{\mathrm{safe}}^{M}$，其中λ = 1对应纯有益性，λ = 0纯无害性。当λ∈( 0、1 )时，对应于一些有益和无害的混合。我们还可以将奖励插值与模型升尺度相结合，以模拟对一个大的预训练进行微调


## 基于推测解码的Up - Scaleed模型的高效采样
从本质上讲，EFT升尺度(小规模微调+大规模预训练模型)需要每个令牌从"小"模型中向前传递2次，从"大"模型中向前传递1次。然而EFT的尺寸不对称使得猜测性解码成为加速推理的自然选择。猜测解码使用一个小的代理模型加速LLM的自回归生成，以自回归地提出一个令牌块，然后大模型可以并行检查。如果小模型能很好地逼近大模型，并产生与大模型相同的令牌，则大模型中总的前向道次数可以大大减少。对于EFT升尺度，我们假设对于大多数令牌，单独的小微调模型可能近似升尺度模型；我们在图7中定性地验证了这一假设，图7表明，小的微调模型与升尺度模型之间的总变化距离对大多数令牌来说很小，而对少数令牌来说非常大。因此，投机性解码很可能会加速EFT的扩大。

我们将猜测解码应用于EFT，发现当Llama - 2 - 7B - chat以Llama - 2 - 70B - base进行扩展时，猜测EFT解码可以将采样加速近2.5倍，同时产生与正常自回归生成相同的样本。这一改进是仅采样7B聊天模型相比仅采样70B聊天模型加速比的50 %以上。为了从一个放大的模型中推测解码，小的微调模型提出了一个带有正常自回归采样的k个令牌块。然后，大、小基模型都在该块上运行一个前向通道(由于变压器的并联性质)，这允许在事后计算每个时间步的真实EFT条件。如果从真实条件句中采样产生相同的tokens，我们只需继续采样一个新提出的块。在出现分歧的情况下，我们将世代回溯到小的微调模型和完整的升尺度模型达成一致的最后一个令牌。如果没有令牌同意，我们使用从第一个真实的后见扩大条件中采样的令牌。



## up sacling模型的保守解码策略
我们之前的所有实验都只是简单地从式( 4 )所描述的原始重加权条件句中采样，没有引入任何新的解码策略或超参数。在这一部分中，我们探索了能否通过后处理含噪预测来进一步改善EFT样本。EFT上采样本质上是从一个小的微调语言模型中提取条件句，并使用一个大基模型的条件句除以一个小基模型的条件句对它们进行重加权(上采样)。然而，对于低概率的(和可能是劣质的模型)令牌，升尺度比率$\frac{P_{base - large} ( x_t \mid x _{< t} )}{P_{base - small} ( x_t \mid x _{< t} )}$可能变得非常大，导致分配给低质量令牌的概率很高。

为了解决这个潜在的问题，我们对升尺度权重进行top - p滤波。完整的结果见表1，表明与从未过滤的条件句中采样相比，对升尺度权重进行top - p滤波会在真实性和有用性方面产生轻微的改善。为了进行top - p滤波，我们首先从只有小微调模型的条件中计算出" top-p "的令牌集合，即概率和超过p的最小令牌集合。然而，与传统的top - p译码(霍尔茨曼等, 2020)不同，我们并没有将其他令牌的条件设置为零。相反，对于这些令牌，我们简单地将升尺度权重设置为1，以防止对极不可能的延续进行无意的升尺度。

## 将GPT - 4事实判断与人类评价者进行比较而使用大型语言模型对人类偏好或有用性进行评价是有的

为了证实我们的GPT - 4真实性判断是有意义的，我们在一组数据上比较了人类和GPT - 4提供的注释。人体标签采集的详细信息见附录。我们从ELI5和Falcon - 40binstruct (根据GPT - 4选择,因为其产生事实错误的比率接近0.5)中生成了100条提示和相应的响应的评估数据集。我们获得了人类和GPT - 4的标签，用于100个回复中每个事实错误的数量。然后，我们对这些预测进行二值化处理，以解释人类或GPT - 4如何评估单一事实的差异；也就是说，我们比较了该响应中是否存在事实错误，或者根本没有事实错误所对应的二元变量?除了计算一致率外，我们还考察了30个人类和GPT - 4不一致的例子，并仔细地标注了一个"基本真值"值，以确定响应是否包含事实错误。我们发现人类和GPT - 4的标签有61 %的时间是一致的；当人和GPT - 4不一致时，作者仔细收集的金标发现GPT - 4有77 %的时间是正确的，标准误差为7.8 %。这一结果表明，GPT - 4是一个比时间限制的人类众包工人更准确的事实正确性的注释器。


# 结论
扩展预训练和微调(或"对齐")的两阶段流水线仍然是构建更强大的语言系统的主要策略。在本文中，我们提出了一种模拟微调的方法，可以对这两个阶段的结果进行直接的实证探索。使用这种方法，我们证明了微调一个大的预训练语言模型的大部分真实性增益可以通过上缩放获得，它将一个大的基模型和一个小的微调模型结合起来，以模拟当这种大规模微调在计算上令人望而却步时微调大的基模型的结果。此外，我们还发现，在没有额外训练的情况下，可以动态调整行为，例如权衡有用性和无害性。未来的工作可以使用模拟微调来研究模型能力的额外维度，在不需要额外微调的情况下在其他测试时间模型行为之间进行插值，或者探索从EFT结构模型中采样的替代方法，以提高效率或性能。
