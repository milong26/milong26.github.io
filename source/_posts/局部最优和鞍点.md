---
title: 局部最优和鞍点
tags:
  - optimization
categories: math
mathjax: true
date: 2024-04-02 20:14:43
---

局部最优和鞍点都是处在grandient为0的位置，首先是找到这个位置，然后判断它是局部最优还是鞍点，利用海森矩阵从数学公式上就能判定。最后为了让损失函数更新能逃离鞍点，可以用海森矩阵或者动量两种方法。

<!-- more -->

太长不看版：**文章总结**

1. 从gradient=0找到critical point的点
   令gradent=0，就是求loss对参数的微分
2. 利用海森矩阵判断这个点是局部最优（最大、最小）还是鞍点：
   求海森矩阵的特征值，当有正有负的时候就是鞍点。
3. 逃出鞍点：
   - 海森矩阵
   - 动量

# critical point
也就是gradient=0时有几种情况

## 局部最小值（local minima）
如果是卡在local minima,那可能就没有路可以走了，因为四周都比较高，你现在所在的位置已经是最低的点，loss最低的点了，往四周走 loss都会比较高，你会不知道怎么走到其他地方去。

## 鞍点（saddle point）
如图可看出，左右是比红点高，前后比红点低，红点既不是local minima,也不是local maxima的地方。如果是卡在saddle point，saddle point旁边还是有其他路可以让你的loss更低的，你只要逃离saddle point，你就有可能让你的loss更低。
![鞍点](https://ooo.0x0.ooo/2024/04/03/OmWQFM.png)

# 判断
怎么知道critical point的类型，到底是local minima,还是saddle point？
对$L(\theta)$ 进行泰勒展开：$\theta'$在$\theta$附近。
$$
L(\theta)\approx L(\theta')+(\theta-\theta')^Tg+\frac{1}{2}(\theta-\theta')^TH(\theta-\theta')
$$

对公式相加的每一项分析：

1. 表示当$\theta'$离$\theta$很近的时候，L近似相等
2. g就是梯度Gradient。这个gradient会弥补θ跟θ′的差距，虽然我们刚才说θ跟θ′很接近，但它们之间还是有一定的差距。
3. 第三项跟Hessian(海森)矩阵有关。这个H就是海森矩阵。第三项会再弥补θ跟θ′的差距。H里面放的是参数对L的二次微分。

当走到一个critical point，也就是说gradient为0，所以g这AAAAAAAA一项就可以去掉了。这时候的L(θ) ≈ L(θ′) + 第三项。

我们可以第三项来判断，在θ′附近的error surface（误差曲面），到底长什么样。知道error surface，我们就可以判断θ′是属于局部最小值点还是鞍点。

为了符号方便起见,我们把(θ−θ′)用v这个向量来表示。所以公式可以写成：
$$
L(\theta)\approx L(\theta')+\frac{1}{2}v^THv
$$

对每个v来说，当$v^THv>0$时，$L(\theta)>L(\theta')$ 局部最小，也就是此时H是正定矩阵=特征值都是正数。

相反的，当$v^THv<0$时，$L(\theta)<L(\theta')$ 局部最大，也就是此时H特征值都是负数。

特征值有正有负就表示它是鞍点。

# 逃离鞍点
## 海森矩阵
由线性代数的知识：A为矩阵，α为特征向量，λ为特征值。有Aα=λα。我们假设u是H的特征向量，λ是u的特征值。所以有：
$$
u^THu=u^T(\lambda u)=\lambda ||u||^2
$$

又由鞍点的特征值有正有负，所以存在特征值λ使得λ‖u‖²<0，所以$u^THu$小于0，也就是$L(\theta)<L(\theta')$。即假设$\theta-\theta'=u$，你在$\theta'$的位置加上u，沿着u的方向做update得到θ，你就可以让loss变小。

**对于saddle point，我们只需要找出特征值λ小于0的点，再找出它对应的特征向量u，用$u+\theta'$，就可以找到一个新的点，这个点的loss比原来低**

## momentum
其实就是动量。 实质上就是说我们在每一次移动的时候，考虑Gradient的反方向 + 上一步移动的方向，两者加起来的结果去调整我们的参数。

又因为每一步的动量就相当于所有步骤里gradient求和，所谓的 Momentum，当加上 Momentum 的时候，我们 Update 的方向，不是只考虑现在的 Gradient，而是考虑过去所有 Gradient 的总和。

当我们走到一个 Local Minima时，一般 Gradient Descent 就无法向前走了，因为当前的 Gradient 已经为0，那走到 Saddle Point 也一样。如果有 Momentum 的话,你还是有办法继续走下去，因为Momentum考虑的是所有Gradient的总和，那么它的值有可能让你继续往前走