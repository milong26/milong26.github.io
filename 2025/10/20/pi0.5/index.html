<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhon.fun","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="pi0.5也开源了，看一下原文">
<meta property="og:type" content="article">
<meta property="og:title" content="pi0.5">
<meta property="og:url" content="http://zhon.fun/2025/10/20/pi0.5/index.html">
<meta property="og:site_name" content="没啥标题">
<meta property="og:description" content="pi0.5也开源了，看一下原文">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-20T07:05:54.000Z">
<meta property="article:modified_time" content="2025-10-23T08:23:30.741Z">
<meta property="article:author" content="milong26">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://zhon.fun/2025/10/20/pi0.5/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://zhon.fun/2025/10/20/pi0.5/","path":"2025/10/20/pi0.5/","title":"pi0.5"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>pi0.5 | 没啥标题</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">没啥标题</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">活下去</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%A7%E6%A6%82"><span class="nav-number">1.</span> <span class="nav-text">大概</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#generalist-robot-manipulation"><span class="nav-number">2.</span> <span class="nav-text">generalist robot
manipulation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9D%9Erobot%E6%95%B0%E6%8D%AE%E7%9A%84co-train"><span class="nav-number">3.</span> <span class="nav-text">非robot数据的co-train</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%94%A8%E8%AF%AD%E8%A8%80%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86%E5%92%8C%E8%A7%84%E5%88%92"><span class="nav-number">4.</span> <span class="nav-text">机器人用语言进行推理和规划</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B7%E6%9C%89%E5%BC%80%E6%94%BE%E4%B8%96%E7%95%8C%E6%B3%9B%E5%8C%96%E7%9A%84%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F"><span class="nav-number">5.</span> <span class="nav-text">具有开放世界泛化的机器人学习系统</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6"><span class="nav-number">6.</span> <span class="nav-text">框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%93%E5%90%88%E7%A6%BB%E6%95%A3%E5%92%8C%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C%E8%A1%A8%E7%A4%BA"><span class="nav-number">7.</span> <span class="nav-text">结合离散和连续动作表示</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pre-train"><span class="nav-number">8.</span> <span class="nav-text">pre-train</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#post-train"><span class="nav-number">9.</span> <span class="nav-text">post-train</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB%E7%BB%9F%E8%AF%A6%E6%83%85"><span class="nav-number">10.</span> <span class="nav-text">机器人系统详情</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%9B%E5%8C%96%E5%88%B0%E7%9C%9F%E5%AE%9E%E5%9C%BA%E6%99%AF%E6%96%B0%E7%8E%AF%E5%A2%83"><span class="nav-number">11.</span> <span class="nav-text">泛化到真实场景新环境</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%9B%E5%8C%96%E5%A6%82%E4%BD%95%E9%9A%8F%E5%9C%BA%E6%99%AF%E6%95%B0%E9%87%8F%E7%BC%A9%E6%94%BE"><span class="nav-number">12.</span> <span class="nav-text">泛化如何随场景数量缩放</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%90%84%E4%B8%AA%E5%8D%8F%E5%90%8C%E8%AE%AD%E7%BB%83%E6%88%90%E5%88%86%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E5%85%B6%E6%9C%80%E7%BB%88%E6%80%A7%E8%83%BD"><span class="nav-number">13.</span> <span class="nav-text">各个协同训练成分如何影响其最终性能</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%93%E8%AE%BA"><span class="nav-number">13.0.1.</span> <span class="nav-text">核心结论</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%AF%94%E8%BE%83%E5%85%B6%E5%AE%83vla"><span class="nav-number">14.</span> <span class="nav-text">比较其它vla</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E6%80%BB%E7%BB%93"><span class="nav-number">14.1.</span> <span class="nav-text">实验结果总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%93%E8%AE%BA-1"><span class="nav-number">14.1.1.</span> <span class="nav-text">核心结论</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7%E6%8E%A8%E7%90%86%E6%9C%89%E5%A4%9A%E9%87%8D%E8%A6%81"><span class="nav-number">15.</span> <span class="nav-text">高级推理有多重要</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E6%80%BB%E7%BB%93-1"><span class="nav-number">15.1.</span> <span class="nav-text">实验结果总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%93%E8%AE%BA-2"><span class="nav-number">15.2.</span> <span class="nav-text">核心结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%CF%800.5-%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%BB%E7%BB%93%E4%B8%8E%E4%BC%98%E5%8A%BF"><span class="nav-number">15.3.</span> <span class="nav-text">π0.5 模型的总结与优势</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-number">15.4.</span> <span class="nav-text">2️⃣ 局限性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C%E6%96%B9%E5%90%91"><span class="nav-number">15.5.</span> <span class="nav-text">3️⃣ 未来工作方向</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E6%80%A7%E8%A7%82%E7%82%B9"><span class="nav-number">15.6.</span> <span class="nav-text">4️⃣ 总结性观点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%92%8Cpi0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">16.</span> <span class="nav-text">和pi0的区别</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">milong26</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">86</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">76</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhon.fun/2025/10/20/pi0.5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="milong26">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没啥标题">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="pi0.5 | 没啥标题">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          pi0.5
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-10-20 15:05:54" itemprop="dateCreated datePublished" datetime="2025-10-20T15:05:54+08:00">2025-10-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-10-23 16:23:30" itemprop="dateModified" datetime="2025-10-23T16:23:30+08:00">2025-10-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>pi0.5也开源了，看一下原文</p>
<span id="more"></span>
<h1 id="大概">大概</h1>
<p>2025-10-20 15:04:17开始，看这篇的目的： 1.
了解pi0.5怎么做的，能实现什么效果，研究的问题是什么 2. 和pi0比较
2025-10-23 14:15:43 看完了，现在思路比较清晰了但是不知道怎么改。</p>
<p>任务拆解的实时性怎么样？<br />
比如碰倒下了，越过了……这种改变了场景的形态</p>
<p>关键词：</p>
<p>generalization</p>
<p>open-world generalization</p>
<p>问题：以便实现跨环境的广泛通用</p>
<p>这个和pi0一样，泛化性很强，能做到在新场景中做长时间任务，也就是wild上的比较好</p>
<p>也就是zero-shot下的能力</p>
<p>之前pi0也能，但是效果一般？不清楚，里面的hard的结果？而且那些任务不够长时序</p>
<p>pi0.5的一个关键词是co-training
的框架，pi0当时只说了end-to-end的。</p>
<p>co在哪里：异构的数据、不同的知识资源</p>
<p>模型架构：</p>
<p>训练：</p>
<ol type="1">
<li><p>在训练任务的异构混合上对模型进行预训练</p></li>
<li><p>通过低级动作示例和高级“语义”动作对应于预测子任务标签，专门针对移动作进行微调。</p></li>
</ol>
<p>推理：模型首先预测语义子任务，根据任务结构和场景的语义推断出接下来适合执行的行为，然后基于该子任务预测低级机器人动作块。这种简单的架构既提供了推理长期多阶段任务的能力，也提供了利用两个级别的不同知识来源的能力：低级动作推理过程很容易从其他机器人收集的动作数据中受益，包括其他环境中更简单的静态机器人，而高级推理过程则受益于来自网络的语义示例，
高级注释预测，甚至是人类“主管”可以向机器人提供的口头命令，引导机器人逐步完成复杂的任务，指示它（很像他们可能指示一个人的方式）执行适当的子任务以完成复杂的任务，例如打扫房间。我们在图
1 中说明了这种设计。</p>
<p>pi0我记得也有高中低级的任务指令啊，没这么复杂？</p>
<p>最后的贡献总结是</p>
<p>我们的核心贡献是一个用于训练高度可泛化的 VLA π0.5
的系统，以及一个概念验证，即当该模型在适当多样化的数据上进行训练时，可以从该模型中产生泛化。</p>
<blockquote>
<p>怎样设计实验验证generalizaion capabalities的</p>
</blockquote>
<hr />
<p>相关工作</p>
<h1 id="generalist-robot-manipulation">generalist robot
manipulation</h1>
<p>vlm里面的一个结论：</p>
<p>将机器人纵策略的训练数据分布范围从狭窄的单任务数据集拓宽到跨越多个场景和任务的多样化数据集提高他们推广到新场景和任务的能力</p>
<blockquote>
<p>为什么不会引入噪声？（针对同一个任务的）</p>
<p>single里面会学到spurious correlations也就是表面相关性</p>
</blockquote>
<p>也就是训练数据集的多样性。从“表示学习（representation
learning）”的角度看，它们其实提供了<strong>结构化的辅助信息（structured
signal）</strong>，帮助模型更好地理解“什么是可抓取的物体、如何控制手爪、如何对齐接触面</p>
<p>需要尽量避免negative transfer</p>
<p>工作的问题：，但将相同的方法应用于更复杂、更长远的任务（如清理厨房）具有挑战性</p>
<p>pi0.5的改进：在我们的实验中，我们在全新的场景中评估了π0.5，例如训练中没有看到的新厨房和卧室，这表明我们的VLA不仅可以利用目标移动机械手平台上的直接第一手经验，还可以利用来自其他数据源的信息，推广到全新的场景</p>
<h1 id="非robot数据的co-train">非robot数据的co-train</h1>
<p>前提：将VLA与用于VLM训练的数据混合[23,92,86]共同训练VLA可以提高其泛化能力</p>
<p>问题：超越了 VLM 数据协同训练(隐晦)</p>
<p>不同：设计了一个系统，用于与更广泛的机器人相关监督源（包括来自其他机器人的数据、高级语义子任务预测和口头语言指令）共同训练
VLA</p>
<h1 id="机器人用语言进行推理和规划">机器人用语言进行推理和规划</h1>
<ol type="1">
<li><p>通过高级推理增强端到端策略可以显着提高长期任务的性能，特别是当高级子任务推理可以从大型预训练
LLM 和 VLM 中受益时。</p></li>
<li><p>许多先前的方法采用了两个独立的模型，一个VLM预测语义步骤，一个单独的低级策略执行这些步骤，但没有两个一起的</p></li>
<li><p>我们的方法对高级和低级推理使用相同的精确模型</p></li>
</ol>
<h1
id="具有开放世界泛化的机器人学习系统">具有开放世界泛化的机器人学习系统</h1>
<p>2类相关工作</p>
<ol type="1">
<li><p>以前的工作是用primitive，but不容易推广</p></li>
<li><p>跨许多领域收集的大规模数据集端到端，but时间短</p></li>
<li><p>本文工作同时<strong>克服了两类相关工作的局限性：</strong>不局限于
primitive-based
的简单任务，且超越了大规模数据集方法的短时任务限制</p></li>
</ol>
<hr />
<p>“PRELIMINARIES” (<a
href="zotero://open-pdf/library/items/RZP8BZMX?page=4">pdf</a>)</p>
<p>这一章说了下vla的基础结</p>
<ol type="1">
<li><p>基本设置
D是训练用的数据集，t是当前时间步，H是步长，o是观察，l是自然语言指令，thta是参数，pi是策略，E是期望。</p></li>
<li><p>架构：具有特定于模态的分词器，可将输入和输出映射到离散（“硬”）或连续（“软”）标记表示，以及经过训练以从输入标记映射到输出标记的大型自回归转换器主干网。连续token是什么？我记得以前transformer给语言分词不就是为了变成离散的吗？</p>
<ol type="1">
<li><p>离散token：Token ID → embedding lookup → 向量输入
transformer</p></li>
<li><p>连续token：一般针对非文本模态</p></li>
<li><p>本质上 transformer 处理的是向量序列，而不要求输入必须是离散
token</p></li>
<li><p>通过将策略输入和输出编码为标记化表示，上述模仿学习问题可以转换为一系列观察、指令和作标记上的简单下一个标记预测问题，我们可以利用现代机器学习的可扩展工具来优化它。</p>
<ol type="1">
<li>怎样转化的：VLA里面模仿学习的目标是最大化那个概率</li>
</ol></li>
</ol></li>
<li><p>tokenize细节：视觉和语言 tokenizer 沿用现有 VLM
的做法，然后动作的做法比较特殊：它本来是连续的信号，需要压缩程token，使用<strong>compression-based
tokenization</strong>，把连续动作压缩为离散或连续
token，降低序列长度和维度。这个方法就是FAST那个</p></li>
<li><p>post-train与动作专家：用flow matching表示动作分布</p>
<ol type="1">
<li><strong>动作分布</strong>（action
distribution）指的是 <strong>在给定观测 <em>ot</em>​ 和指令 <em>l</em> 的条件下，策略
πθ 输出动作的概率分布</strong>。</li>
</ol></li>
</ol>
<p>第四部分才提到post-train，前面三个都是pre-train吗？</p>
<p><strong>预训练阶段</strong>：</p>
<ul>
<li><p>模型学习 <strong>general VLA 能力</strong></p></li>
<li><p>对图像、语言、动作进行 token 化</p></li>
<li><p>通过模仿学习或者 diffusion/flow matching 方式训练 transformer
backbone</p></li>
<li><p>学到的是<strong>一般的观察 → 动作映射能力</strong></p></li>
</ul>
<p>第四部分提到 <strong>post-training</strong>：</p>
<ul>
<li><p>建立在 pre-trained backbone 之上</p></li>
<li><p><strong>专门优化动作生成</strong>，引入 flow matching + action
expert</p></li>
<li><p>目标是 <strong>在连续动作生成上更精确、更高效</strong></p></li>
<li><p>也就是说，这一阶段是 <strong>fine-tuning /
专门训练动作生成模块</strong></p></li>
</ul>
<hr />
<p>pi0.5</p>
<h1 id="框架">框架</h1>
<p>动作还是用fast变成离散的token了啊，也没有变成连续的</p>
<p>pre-train，粗粒度的模仿学习，还没用到flow
matching，最大化之前那个log函数的值就足够了</p>
<p>在 post-training 阶段：</p>
<ul>
<li><p>专门训练模型
<strong>先推断高层子任务，再生成动作</strong></p></li>
<li><p>使用任务相关数据和口头指令（verbal instruction）进行
fine-tuning</p></li>
<li><p>使用 <strong>flow matching</strong>
表示连续动作，让低层动作生成更精细</p></li>
</ul>
<blockquote>
<p>没懂怎么做到同时训练2个目标的：1. 推理动作指令，2.
利用指令推理动作</p>
</blockquote>
<p>然后是第二段，关于transformer的多模态处理，每一层transformer，都有多模态输入和多模态输出。</p>
<ol type="1">
<li><p>根据公式，好像就是输出俩，一个textual output，一个action
output</p></li>
<li><p>πθ​(at:t+H​,l^∣ot​,l)条件概率链式分解之后变成</p></li>
</ol>
<p>“πθ(at:t+H , lˆ|ot, l) = πθ(at:t+H |ot, lˆ)πθ(lˆ|ot, l),” (<a
href="zotero://select/library/items/PGRM3N9G">Intelligence et al., 2025,
p. 5</a>) (<a
href="zotero://open-pdf/library/items/RZP8BZMX?page=5">pdf</a>)</p>
<p>就是所谓的指令转换</p>
<p>Transformer 可以针对不同 token 类型使用不同子网络</p>
<p>用attention matrix指示 token
之间是否可以互相注意（attend），一般的llm用<strong>causal
attention</strong>（只能看前面的 token），pi0.5π0.5 对不同模态 token
使用 <strong>bidirectional attention</strong>：图像 patch、文本
prompt、动作 token 可以互相看到</p>
<ol type="1">
<li><p>attend是什么意思</p>
<ol type="1">
<li>在 Transformer 中，<strong>attention</strong> 指的是：<strong>每个
token 在计算表示时可以参考（attend to）序列中其他 token
的信息。</strong></li>
</ol></li>
<li><p>为什么多模态需要bidirectional attention，语言模型只要causal
attention就足够了？</p>
<ol type="1">
<li><p>因果（causal）attention 约束：</p>
<ul>
<li><p>Token i 只能看到 1,2,…,i 的 token</p></li>
<li><p>目的是保证 <strong>autoregressive 生成</strong>：预测下一个 token
时不能“偷看未来”保证生成顺序正确</p></li>
</ul></li>
<li><p>bidirectional attention</p>
<p><strong>多模态交互</strong>：图像、语言、动作三种模态可能互相依赖</p>
<ul>
<li><p>例如动作 token 的预测可能依赖图像 patch 和文本 token</p></li>
<li><p>同时，高层动作 token 也可能影响低层动作 token 的表示</p></li>
</ul>
<p><strong>流行的 flow matching / diffusion 方法</strong>：</p>
<ul>
<li><p>在 post-training 阶段，动作 token
是 <strong>去噪序列的一部分</strong></p></li>
<li><p>去噪动作 token 之间也需要互相参考 → 需要 bidirectional</p></li>
</ul>
<p><strong>图像或文本之间可能有信息依赖</strong>：</p>
<ul>
<li><p>文本描述和图像可能对应关系</p></li>
<li><p>让图像 patch 和文本 token
互相看到，有助于模型更好融合信息</p></li>
</ul></li>
</ol></li>
<li><p>怎么做到的？attention mask
M=0不能看，M=1能看，attention(Q,K,V)=softmax(QK^T/sqrt(d_k)+logM)V差不多就行</p></li>
</ol>
<p>最后一个：怎样做到输出2个目标的：</p>
<p>f 的输出被拆分为文本标记 logits 和作输出标记，分别为 yl 1：M ， ya
1：H 。前一个 M 对应于可用于采样 lˆ 的文本标记 logit，后面的 H
标记由单独的动作专家生成，如 π0，并通过线性映射投影到用于获得 at：t+H
的连续输出。</p>
<h1 id="结合离散和连续动作表示">结合离散和连续动作表示</h1>
<p>这个我看的时候也比较稀里糊涂，是不是pre-train的时候discrete，然后post-train的时候向量场比较接近continuous的了？</p>
<p>第一段：结论是训练的时候用离散token，推理的时候用flow
matching生成连续的</p>
<p>flowmatching是生成连续动作序列，而不仅仅是离散token</p>
<p>FAST又发现离散token对训练更加高效</p>
<p>但是离散的action有3个问题：需要逐步autoregressive，效率低，且动作不够平滑。</p>
<p>所以采用一种思想：训练的时候用离散token，推理的时候用flow
matching生成连续的</p>
<p>问题：</p>
<ol type="1">
<li><p>action是离散还是连续的？在机器人学习中，<strong>一个动作（action）不是一个离散标签，而是一个向量</strong></p>
<blockquote>
<p>在数学和统计学中，"离散"（discrete）和"连续"（continuous）是描述数据或变量类型的两个基本术语</p>
<p>离散（Discrete）：离散数据或变量只能取特定、分离的值。这些值通常是整数（如人口数量、汽车数量）或有限集合中的成员（如血型、眼睛颜色）。离散变量之间没有“中间”值；它们是不连续的。例如，如果你有一个表示家庭孩子数量的离散变量，那么可能的值是0、1、2、3等，而不可能是1.5或2.3。</p>
<p>在统计建模中，离散变量经常用于分类任务，因为它们表示的是有限的、不连续的类别或标签。</p>
<p>连续（Continuous）：连续数据或变量可以在一个给定的范围内取任意值，这个范围通常是一个实数区间。例如，温度、身高、体重或时间都可以是连续变量。连续变量具有无限多个可能的值，并且这些值之间的变化是平滑的。在理论上，两个连续值之间总是可以找到一个更小的间隔，使得变量可以取这个间隔内的任何一个值。在统计建模中，连续变量经常用于回归任务，因为回归模型旨在预测一个连续的结果或输出值。</p>
</blockquote></li>
</ol>
<p>2. 离散 token 只能用于 <strong>模仿学习阶段（imitation
pretraining）</strong>，<br />
因为它们需要 <strong>autoregressive 解码</strong>，逐个预测下一个
token，速度慢</p>
<p>3. flow matching 是一种 <strong>连续生成方法</strong>，和
autoregressive 不同。<br />
它不是预测下一个 token，而是：</p>
<ul>
<li>学习从高斯噪声流向真实动作的“流动场”（vector field）；</li>
</ul>
<p>3. 怎样把连续的action给离散化表示 <strong>FAST tokenizer</strong></p>
<p>4. 连续动作为什么不需要auto-regressive？</p>
<p>    Autoregressive的目标：预测下一个 token 的概率</p>
<p>流程是这样的：输入action连续，先离散化用于加快训练速度（pre-train），然后post-train的时候用flow
matching更改动作专家的权重。推理的时候也用flow matching</p>
<p>所以模型的优化目标变成最小化：</p>
<p>交叉熵of 指令 and
预测出来的logits（应该意思是保证输出的指令和原始指令相差不太大？）</p>
<p>flθ​(ot​,l)代表模型从输入的视觉观测 ot<em>ot</em>​ 和上下文文本 l<em>l</em> 中，<strong>预测出每个
token 的分布</strong></p>
<p><strong>logits 就是模型在每个时间步预测的未归一化 token
分数</strong>，包括文本和离散动作的预测分布。</p>
<p>+alpha * flow matching的l2 loss</p>
<p>alpha=0的时候就是pre-train，训练出来一个VLM的模型。</p>
<p>在这种情况下：</p>
<ul>
<li><p>模型的任务是：<br />
“给定视觉输入 ot<em>ot</em>​，预测下一个 token（文本或离散动作
token）。”</p></li>
<li><p>所以它的训练目标与一个标准的 <strong>视觉-语言 Transformer
(VLM)</strong> 完全一致：<br />
学习从视觉输入到文本 token（或离散化后的动作 token）的映射。</p></li>
</ul>
<p>L2 loss（也称为<strong>均方误差损失 / Mean Squared Error,
MSE</strong>）是机器学习中最常用的<strong>回归类目标函数</strong>之一。</p>
<p><strong>L2
loss</strong> 衡量的是模型预测值与真实值之间的<strong>欧几里得距离平方（squared
Euclidean distance）</strong>。<br />
它让模型学会让预测结果尽可能靠近真实值。</p>
<p>∥<em>y</em>−<em>y</em><sup>​∥</sup>2​</p>
<table>
<tbody>
<tr>
<td>
<p>
<strong><span style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">Cross-Entropy
Loss</span></span></strong>
</p>
</td>
<td>
<p>
<span style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">−ylog⁡(p^)−</span></span><em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">y</span></span></em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">log(</span></span><em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">p</span></span></em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">^​)</span></span>
</p>
</td>
<td>
<p>
<span style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">用于分类任务（概率分布）</span></span>
</p>
</td>
<td>
<p>
<span style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">文本/图像分类，token
预测</span></span>
</p>
</td>
</tr>
<tr>
<td>
<p>
<strong><span style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">KL
Divergence</span></span></strong>
</p>
</td>
<td>
<p>
<span style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">∑plog⁡(p/q)∑</span></span><em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">p</span></span></em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">log(</span></span><em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">p</span></span></em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">/</span></span><em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">q</span></span></em><span
style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">)</span></span>
</p>
</td>
<td>
<p>
<span style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">比较两个分布的差异</span></span>
</p>
</td>
<td>
<p>
<span style="color: rgb(13, 13, 13)"><span
style="background-color: rgb(255, 255, 255)">生成模型、对齐分布任</span></span>
</p>
</td>
</tr>
</tbody>
</table>
<p>还有几种损失，l1 loss就不说了</p>
<h1 id="pre-train">pre-train</h1>
<p>一个标准的自回归转换器，对文本、对象位置和 FAST
编码的作令牌执行下一个令牌预测</p>
<p>对于所有动作数据，我们训练模型以预测目标关节和末端执行器姿势。为了区分两者，我们在文本提示中添加了“<控制模式>关节/末端执行器<控制模式>”。使用单个数据集的每个动作维度的
1% 和 99% 分位数将所有动作数据归一化为 [−1， 1]。我们将动作 a
的维数设置为固定数字，以容纳所有数据集中最大的动作空间。对于具有低维配置和动作空间的机器人，我们将动作向量置零。</p>
<h1 id="post-train">post-train</h1>
<p>VI这个数据集什么形式？</p>
<h1 id="机器人系统详情">机器人系统详情</h1>
<p>18-19 个 DoF 状态和动作空间</p>
<hr />
<p>实验分析</p>
<p>之前有一个问题：怎样验证generalization capability？</p>
<p>最终评估是在三个不属于训练集的真实家庭中进行的，侧重5个问题：</p>
<ol type="1">
<li><p>π0.5 能否有效地推广到全新家庭中复杂的多阶段任务？</p></li>
<li><p>π0.5 的泛化如何与训练数据中不同环境的数量成比例？</p></li>
<li><p>π0.5 训练混合物中的各个协同训练成分如何影响其最终性能？</p></li>
<li><p>π0.5 与 π0 VLA 相比如何？</p></li>
<li><p>π0.5
的高级推理组件有多重要，它与平面、低级推理以及预言机高级基线相比如何？</p></li>
</ol>
<h1 id="泛化到真实场景新环境">泛化到真实场景新环境</h1>
<p>成功分数评估，结果：这种水平的野外泛化大大超出了先前视觉-语言-行动模型所展示的结果，无论是在模型必须处理的新颖程度方面，还是在任务持续时间和复杂性方面</p>
<h1 id="泛化如何随场景数量缩放">泛化如何随场景数量缩放</h1>
<p>测量泛化如何随训练数据中看到的环境数量而扩展。也就是模型在训练中看到的环境越多，泛化能力是否越强？</p>
<p>指标：<strong>多阶段任务的端到端性能</strong>（如放碗到水槽、收纳物品、叠衣服、整理床铺）。</p>
<p><strong>语言指令跟随能力</strong>：机器人是否能根据语言命令拾取特定物体并放到正确位置，包括对新物体（out-of-distribution
objects）的处理能力</p>
<p><strong>训练过程：后训练（post-training）</strong>：每组训练数据包含不同数量的移动操作环境。</p>
<p>测试的时候还是mock环境</p>
<p>结果：</p>
<ol type="1">
<li><p><strong>泛化随环境数量增加而增强</strong>：模型在多阶段任务和语言指令跟随上表现更好。</p></li>
<li><p><strong>多数据源联合训练（co-training）是关键</strong>：仅依赖测试环境或单一训练环境数据，模型泛化能力有限。</p></li>
<li><p><strong>语义泛化需要更多场景</strong>：模型对未见物体类别的表现随着环境数量增加而改善，但速度慢于已见类别。</p></li>
</ol>
<div class="line-block"></div>
<p>实验编号</p>
<p>|</p>
<p>实验名称 / 目的</p>
<p>|</p>
<p>实验设置</p>
<p>|</p>
<p>训练数据 / 对比</p>
<p>|</p>
<p>评估指标</p>
<p>|</p>
<p>结果图 / 表格</p>
<p>|</p>
<p>简短分析</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>1</p>
<p>|</p>
<p>多阶段任务端到端泛化</p>
<p>|</p>
<p>训练环境数量变化：3, 12, 22, 53, 82, 104</p>
<p>|</p>
<p>预训练：机器人动作数据（无移动操作数据）；后训练：不同数量移动操作环境</p>
<p>|</p>
<p>端到端任务成功率（放碗、收纳、叠衣服、整理床铺）</p>
<p>|</p>
<p>Figure 8</p>
<p>|</p>
<p>平均任务性能随训练环境数量增加提升；104
环境模型接近直接在测试环境训练的模型，说明多环境训练可有效提升泛化。</p>
<p>| |</p>
<p>2</p>
<p>|</p>
<p>语言指令跟随能力</p>
<p>|</p>
<p>同上</p>
<p>|</p>
<p>同上</p>
<p>|</p>
<p>1）Language following rate（选择正确物体频率）；2）Success
rate（物体放置正确率）；分 in-distribution 和 out-of-distribution
对象</p>
<p>|</p>
<p>Figure 9</p>
<p>|</p>
<p>随训练环境数量增加，语言跟随和成功率均提高；in-distribution
对象提升快，out-of-distribution
对象提升慢但明显改善，表明模型语义泛化能力随环境增多而增强。</p>
<p>|</p>
<h1
id="各个协同训练成分如何影响其最终性能">各个协同训练成分如何影响其最终性能</h1>
<p><strong>目的</strong>：研究 π0.5
模型在多任务、多环境下的泛化能力，以及不同训练数据来源（mixture
components）对性能的重要性。</p>
<p><strong>训练数据组成</strong>：</p>
<ol type="1">
<li><p><strong>MM</strong>：多环境移动操作机器人数据</p></li>
<li><p><strong>ME</strong>：多环境非移动（静态）机器人数据</p></li>
<li><p><strong>CE</strong>：实验室跨机器人/跨任务数据</p></li>
<li><p><strong>HL</strong>：高层语言指令数据</p></li>
<li><p><strong>WD</strong>：Web 数据（图像描述、VQA、物体定位）</p></li>
<li><p><strong>VI</strong>：后训练阶段的 verbal instruction
数据</p></li>
</ol>
<p><strong>实验方法</strong>：对比全模型（full
recipe）与去掉不同数据源的 ablation 版本：</p>
<ul>
<li><p><strong>no WD</strong>：去掉 Web 数据</p></li>
<li><p><strong>no ME</strong>：去掉多环境非移动数据</p></li>
<li><p><strong>no CE</strong>：去掉实验室跨机器人数据</p></li>
<li><p><strong>no ME or CE</strong>：同时去掉 ME 和
CE，只保留目标移动机器人数据 + Web 数据</p></li>
</ul>
<p><strong>评估指标</strong>：</p>
<ol type="1">
<li><p><strong>端到端任务性能</strong>：在 mock home
中完成多阶段任务（Figure 10 + Appendix D）</p></li>
<li><p><strong>语言指令跟随能力</strong>：语言指令驱动的物体抓取/放置任务（Figure
11），包括 in-distribution 和 out-of-distribution 对象</p></li>
</ol>
<div class="line-block"></div>
<p>Ablation / 模型</p>
<p>|</p>
<p>端到端任务性能（mock home）</p>
<p>|</p>
<p>语言跟随能力</p>
<p>|</p>
<p>结论 / 分析</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Full recipe (MM+ME+CE+HL+WD+VI)</p>
<p>|</p>
<p>最好</p>
<p>|</p>
<p>最好</p>
<p>|</p>
<p>利用跨环境、跨任务、Web 和语言数据，表现最优</p>
<p>| |</p>
<p>no WD</p>
<p>|</p>
<p>无显著下降</p>
<p>|</p>
<p>OOD 对象明显下降</p>
<p>|</p>
<p>Web 数据对端到端任务影响不大，但对未见对象和高层语言推理至关重要</p>
<p>| |</p>
<p>no ME</p>
<p>|</p>
<p>显著下降</p>
<p>|</p>
<p>显著下降</p>
<p>|</p>
<p>多环境非移动数据有助于跨环境泛化和语言跟随能力</p>
<p>| |</p>
<p>no CE</p>
<p>|</p>
<p>显著下降</p>
<p>|</p>
<p>显著下降</p>
<p>|</p>
<p>跨任务/跨机器人数据对模型泛化和任务性能同样重要</p>
<p>| |</p>
<p>no ME or CE</p>
<p>|</p>
<p>最差</p>
<p>|</p>
<p>最差</p>
<p>|</p>
<p>同时去掉两个跨机器人数据源严重削弱性能，表明跨任务与跨环境知识传递对
π0.5 至关重要</p>
<p>|</p>
<h3 id="核心结论">核心结论</h3>
<ol type="1">
<li><p><strong>跨环境与跨任务数据至关重要</strong>：ME 和 CE
数据为模型提供了跨环境、跨机器人和跨任务的迁移能力。</p></li>
<li><p><strong>Web
数据影响语言泛化</strong>：虽然对端到端任务影响有限，但对未见类别的语言指令理解和推理能力显著提高。</p></li>
<li><p><strong>联合训练效果最好</strong>：full recipe
利用所有数据源，性能优于任何 ablation 版本。</p></li>
</ol>
<hr />
<p>💡 <strong>一句话总结</strong>：<br />
π0.5
的泛化能力依赖于<strong>跨环境/跨任务数据提供的迁移能力</strong>以及
<strong>Web
数据提供的广泛物体语义知识</strong>，去掉任何关键数据源都会显著降低性能，尤其是在语言指令跟随和未见对象任务上。</p>
<h1 id="比较其它vla">比较其它vla</h1>
<p><strong>目的</strong>：比较 π0.5 与现有 VLA
模型在移动操作任务上的表现，并验证 co-training
和额外数据源（HL、WD）对性能的贡献。</p>
<p><strong>对比模型</strong>：</p>
<ol type="1">
<li><p><strong>π0</strong>：原始 VLA
模型，强于复杂移动操作任务，始终使用 action
expert（连续动作预测）。</p></li>
<li><p><strong>π0-FAST+Flow</strong>：</p>
<ul>
<li><p>使用 Equation (1) 的联合 FAST + diffusion 方法训练</p></li>
<li><p>仅使用机器人动作数据，不包含 HL 或 WD</p></li>
<li><p>遵循 hybrid 训练：预训练离散 token，post-training 使用 flow
action expert</p></li>
<li><p>无法做高层推理（因为缺少 HL 数据）</p></li>
</ul></li>
<li><p><strong>π0.5</strong>（目标模型）：</p>
<ul>
<li><p>使用 <strong>co-training</strong>：动作数据 + 高层语言数据（HL）+
Web 数据（WD）</p></li>
<li><p>训练流程：预训练离散 token → post-training 使用 flow action
expert</p></li>
</ul></li>
</ol>
<p><strong>训练控制</strong>：</p>
<ul>
<li><p>所有模型使用同样的跨机器人（cross-embodiment）训练数据</p></li>
<li><p>训练步数可比较，保证公平性</p></li>
</ul>
<p><strong>评估指标</strong>：</p>
<ul>
<li><p>多阶段移动操作任务端到端性能</p></li>
<li><p>高层推理能力（HL 数据任务）</p></li>
<li><p>语言指令跟随（WD 数据任务）</p></li>
</ul>
<hr />
<h2 id="实验结果总结">实验结果总结</h2>
<div class="line-block"></div>
<p>模型</p>
<p>|</p>
<p>数据源</p>
<p>|</p>
<p>训练方式</p>
<p>|</p>
<p>高层推理能力</p>
<p>|</p>
<p>端到端性能</p>
<p>|</p>
<p>分析 / 结果</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>π0</p>
<p>|</p>
<p>动作数据</p>
<p>|</p>
<p>action expert 全程</p>
<p>|</p>
<p>可以做低层动作，但无 HL / WD，高层推理能力有限</p>
<p>|</p>
<p>基线水平</p>
<p>|</p>
<p>原始 VLA 表现良好，但缺少高层语义任务能力</p>
<p>| |</p>
<p>π0-FAST+Flow</p>
<p>|</p>
<p>动作数据</p>
<p>|</p>
<p>hybrid 训练（离散 token + flow）</p>
<p>|</p>
<p>无 HL / WD → 无高层推理能力</p>
<p>|</p>
<p>略优于 π0（计算效率更高）</p>
<p>|</p>
<p>FAST + flow 提高了训练效率，但仍无法处理高层任务</p>
<p>| |</p>
<p>π0.5</p>
<p>|</p>
<p>动作 + HL + WD</p>
<p>|</p>
<p>hybrid 训练 + co-training</p>
<p>|</p>
<p>支持高层指令推理和语言任务</p>
<p>|</p>
<p>显著优于 π0 和 π0-FAST+Flow</p>
<p>|</p>
<p>高层数据 + co-training 提升了泛化能力和高层任务表现，即使 π0
训练步数增加到 300k 也无法匹配 π0.5</p>
<p>|</p>
<hr />
<h3 id="核心结论-1">核心结论</h3>
<ol type="1">
<li><p><strong>π0.5
优势明显</strong>：在端到端任务和高层指令任务上，性能显著优于原始 π0 和
π0-FAST+Flow。</p></li>
<li><p><strong>原因分析</strong>：</p>
<ul>
<li><p>**高层语言数据（HL）<strong>和</strong>Web
数据（WD）**增强了高层推理能力</p></li>
<li><p><strong>co-training + hybrid 训练</strong>
提高了泛化能力和训练效率</p></li>
<li><p>仅动作数据训练的模型无法处理 HL 任务，也缺乏语言推理能力</p></li>
</ul></li>
<li><p><strong>计算效率</strong>：使用 FAST tokens 的 hybrid 训练比纯
diffusion 更节省计算资源，同时保持性能。</p></li>
</ol>
<hr />
<p>💡 <strong>一句话总结</strong>：<br />
π0.5 利用动作数据 + 高层语言数据 + Web 数据的联合训练，以及离散 token +
flow 的 hybrid
训练流程，在移动操作任务上不仅端到端表现更好，还能处理高层语言推理任务，全面超越现有
VLA 模型。</p>
<h1 id="高级推理有多重要">高级推理有多重要</h1>
<p><strong>目的</strong>：评估高层推理机制对模型整体性能的贡献，并分析不同训练数据和高层策略的作用。</p>
<p><strong>高层推理机制（π0.5）</strong>：</p>
<ul>
<li><p>输入：高层命令（如 “clean the bedroom”）</p></li>
<li><p>输出：对应子任务（如 “pick up pillow”）</p></li>
<li><p>该子任务作为上下文提供给低层动作推理模块（类似 chain-of-thought
推理）</p></li>
</ul>
<p><strong>对比方法 / Ablation</strong>（所有方法低层动作推理保持 π0.5
流程一致）：</p>
<div class="line-block"></div>
<p>方法</p>
<p>|</p>
<p>高层策略</p>
<p>|</p>
<p>训练数据 / 说明</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>1. Full π0.5</p>
<p>|</p>
<p>π0.5 模型本身</p>
<p>|</p>
<p>高层 + 低层推理，使用全部数据（MM+ME+CE+HL+WD+VI）</p>
<p>| |</p>
<p>2. no WD</p>
<p>|</p>
<p>π0.5 模型</p>
<p>|</p>
<p>去掉 Web 数据</p>
<p>| |</p>
<p>3. no VI</p>
<p>|</p>
<p>π0.5 模型</p>
<p>|</p>
<p>去掉 verbal instruction 数据</p>
<p>| |</p>
<p>4. implicit HL</p>
<p>|</p>
<p>无高层推理</p>
<p>|</p>
<p>训练包含高层任务数据，但运行时不显式推理高层子任务</p>
<p>| |</p>
<p>5. no HL</p>
<p>|</p>
<p>无高层推理</p>
<p>|</p>
<p>训练和运行都不包含高层任务数据</p>
<p>| |</p>
<p>6. GPT-4</p>
<p>|</p>
<p>GPT-4 作为高层策略</p>
<p>|</p>
<p>零样本使用 GPT-4，高层选择子任务列表，未在机器人数据上训练</p>
<p>| |</p>
<p>7. human HL</p>
<p>|</p>
<p>人类专家</p>
<p>|</p>
<p>上限性能（oracle）</p>
<p>|</p>
<p><strong>评估指标</strong>：</p>
<ul>
<li><p>多阶段移动操作任务端到端成功率</p></li>
<li><p>语言指令跟随性能</p></li>
<li><p>子任务预测准确率（高层策略效果）</p></li>
<li><p>参考 Figure 13 + Appendix D, Figure 17</p></li>
</ul>
<hr />
<h2 id="实验结果总结-1">实验结果总结</h2>
<div class="line-block"></div>
<p>方法</p>
<p>|</p>
<p>性能表现</p>
<p>|</p>
<p>结论 / 分析</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Full π0.5</p>
<p>|</p>
<p>最好，甚至超过 human HL</p>
<p>|</p>
<p>高层推理 + co-training 数据混合提升性能，显示显式高层推理的重要性</p>
<p>| |</p>
<p>implicit HL</p>
<p>|</p>
<p>第二</p>
<p>|</p>
<p>不执行显式高层推理，但训练包含高层数据 → co-training
数据本身已经提供大部分收益</p>
<p>| |</p>
<p>no HL</p>
<p>|</p>
<p>明显下降</p>
<p>|</p>
<p>缺少高层数据和推理 → 子任务预测能力缺失，性能大幅下降</p>
<p>| |</p>
<p>no VI</p>
<p>|</p>
<p>显著下降</p>
<p>|</p>
<p>虽然 verbal instruction 数据量小 (~11%)，但对高层策略表现至关重要</p>
<p>| |</p>
<p>no WD</p>
<p>|</p>
<p>显著下降</p>
<p>|</p>
<p>Web 数据对高层推理和任务语义理解有重要贡献</p>
<p>| |</p>
<p>GPT-4</p>
<p>|</p>
<p>最差</p>
<p>|</p>
<p>零样本 GPT-4 无法适应机器人数据 → 高层策略需要针对机器人环境训练</p>
<p>| |</p>
<p>human HL</p>
<p>|</p>
<p>接近 Full π0.5</p>
<p>|</p>
<p>作为上限参考，Full π0.5 已达到甚至超过人类专家表现</p>
<p>|</p>
<hr />
<h2 id="核心结论-2">核心结论</h2>
<ol type="1">
<li><p><strong>显式高层推理提升性能</strong>：Full π0.5
显著优于不执行高层推理的 ablation（no HL / implicit HL）。</p></li>
<li><p><strong>co-training
数据混合关键</strong>：即使不执行显式推理，训练中包含高层任务数据也能获得大部分高层策略收益。</p></li>
<li><p><strong>小型 verbal instruction 数据影响大</strong>：即便仅占
11%，去掉 VI 数据会显著降低性能。</p></li>
<li><p><strong>Web
数据提升高层策略</strong>：提高语义理解和子任务选择能力。</p></li>
<li><p><strong>零样本 GPT-4
不够用</strong>：高层策略必须结合机器人数据训练，否则性能很差。</p></li>
</ol>
<hr />
<p>💡 <strong>一句话总结</strong>：<br />
高层推理在 π0.5 中极大地提升了子任务预测和端到端任务表现，而 co-training
数据（HL + VI +
WD）对学习高层策略至关重要，即便显式推理未执行，也能从训练数据中获得大部分高层能力。</p>
<hr />
<p>最后</p>
<h2 id="π0.5-模型的总结与优势">π0.5 模型的总结与优势</h2>
<ul>
<li><p>π0.5 是在 π0 VLA 基础上开发的
<strong>联合训练（co-trained）模型</strong>，整合了多种数据源：</p>
<ul>
<li><p>移动机器人操作数据 (~400小时)</p></li>
<li><p>其他机器人数据（非移动操纵器、多环境、实验室数据）</p></li>
<li><p>Web 数据</p></li>
<li><p>高层语言指令预测数据（HL &amp; VI）</p></li>
</ul></li>
<li><p><strong>泛化能力</strong>：</p>
<ul>
<li><p>能够在<strong>训练中未见过的家庭环境</strong>执行任务</p></li>
<li><p>可完成多阶段、精细操作任务，如清理厨房/卧室、叠床、挂毛巾等</p></li>
</ul></li>
<li><p><strong>关键原因</strong>：联合训练不同来源的数据，使得模型能够有效迁移知识，即使移动操作数据量中等，也能实现广泛泛化</p></li>
</ul>
<hr />
<h2 id="局限性">2️⃣ 局限性</h2>
<ul>
<li><p><strong>特定环境难点</strong>：</p>
<ul>
<li>机器人对不熟悉的把手或难开的柜子操作困难</li>
</ul></li>
<li><p><strong>部分可观测性问题</strong>：</p>
<ul>
<li>机器人手臂可能遮挡任务对象（如溢出物）</li>
</ul></li>
<li><p><strong>高层子任务推理容易分心</strong>：</p>
<ul>
<li>如收纳物品时反复开关抽屉</li>
</ul></li>
<li><p><strong>提示（prompts）复杂度有限</strong>：</p>
<ul>
<li><p>模型当前只能处理较简单指令</p></li>
<li><p>更复杂或多样化的指令需要更多训练数据或人工/合成标注</p></li>
</ul></li>
</ul>
<hr />
<h2 id="未来工作方向">3️⃣ 未来工作方向</h2>
<ol type="1">
<li><p><strong>改善联合训练与迁移能力</strong>：</p>
<ul>
<li><p>提高模型在困难环境和部分可观测情况下的稳定性</p></li>
<li><p>利用更大规模或更多样的数据</p></li>
</ul></li>
<li><p><strong>增强高层语言和指令理解能力</strong>：</p>
<ul>
<li><p>支持更复杂和多样化的指令</p></li>
<li><p>通过人类标注或合成数据扩充训练</p></li>
</ul></li>
<li><p><strong>提升记忆和上下文能力</strong>：</p>
<ul>
<li>处理部分可观测场景（跨房间任务、对象位置记忆）</li>
</ul></li>
<li><p><strong>探索更多异质数据源</strong>：</p>
<ul>
<li><p>利用 verbal instruction 作为新的监督信号</p></li>
<li><p>尝试更多人类提供上下文知识的方式</p></li>
</ul></li>
</ol>
<hr />
<h2 id="总结性观点">4️⃣ 总结性观点</h2>
<ul>
<li><p>π0.5 展示了通过联合训练不同数据源，VLA
可以在真实世界环境中实现广泛泛化</p></li>
<li><p>当前模型仍有改进空间（环境复杂度、部分可观测、指令复杂度）</p></li>
<li><p>未来的 VLA
可以进一步利用更多异质数据源、增强记忆和语言理解能力，向更广泛的现实场景泛化</p></li>
</ul>
<h1 id="和pi0的区别">和pi0的区别</h1>
<ol type="1">
<li>pi05目标侧重开放世界泛化，pi0没到这种程度，当时只是构建一个基础模型。可以说π₀.₅更强调多源知识迁移，π₀更强调模型可扩展性</li>
<li>所以，0.5更强调异构数据，多样性；0.5只是大量，虽然也挺多，但是没有用到网络规模的数据作为vlm的参考。</li>
<li>π₀.₅使用混合离散-连续行动表示，而π₀专注于流匹配。</li>
</ol>
<ul>
<li><p>​<strong>​π₀.₅论文​</strong>​：使用分层架构，预训练阶段用离散令牌（FAST编码），后训练阶段加入流匹配专家。方法包括高级推理（语义子任务预测）和低级行动推理。</p></li>
<li><p>​<strong>​π₀论文​</strong>​：基于预训练VLM（PaliGemma），添加流匹配专家用于连续行动块预测。使用双向注意力掩码和时间步采样。</p></li>
</ul>
<p>​<strong>​差异​</strong>​：π₀.₅的分析方法更复杂，结合了离散令牌和流匹配，以平衡训练效率和推理速度；π₀则简化了流程，纯流匹配用于高频控制。两者都使用Transformer骨干，但π₀.₅增加了高级推理组件。
## 结果 1.
pi0.5用了FAST：一种名为FAST的基于离散余弦变换的新型机器人动作标记方案，通过压缩连续动作信号，实现高频率机器人数据的有效训练，进而构建自回归视觉-语言-动作（VLA）模型
2.
比pi0泛化性更强，而且用了高级/低级语言指令，但是也没办法摘出来，不是所有vla的共性，更何况是post-train的</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/10/11/%E7%9C%8B%E6%88%BF%E8%A6%81%E7%9C%8B%E4%BB%80%E4%B9%88/" rel="prev" title="看房要看什么">
                  <i class="fa fa-angle-left"></i> 看房要看什么
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/10/20/pi0/" rel="next" title="pi0">
                  pi0 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">milong26</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
