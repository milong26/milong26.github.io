<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhon.fun","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="主要是代码理解和修改">
<meta property="og:type" content="article">
<meta property="og:title" content="smolvla">
<meta property="og:url" content="http://zhon.fun/2025/08/17/smolvla/index.html">
<meta property="og:site_name" content="没啥标题">
<meta property="og:description" content="主要是代码理解和修改">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-17T14:39:53.000Z">
<meta property="article:modified_time" content="2025-09-15T14:58:49.529Z">
<meta property="article:author" content="milong26">
<meta property="article:tag" content="vla">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://zhon.fun/2025/08/17/smolvla/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://zhon.fun/2025/08/17/smolvla/","path":"2025/08/17/smolvla/","title":"smolvla"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>smolvla | 没啥标题</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">没啥标题</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">活下去</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5%E7%90%86%E8%A7%A3"><span class="nav-number">1.</span> <span class="nav-text">概念理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80"><span class="nav-number">1.1.</span> <span class="nav-text">基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-atten%E5%92%8Ccross-atten"><span class="nav-number">1.2.</span> <span class="nav-text">self-atten和cross-atten</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#train"><span class="nav-number">2.</span> <span class="nav-text">train</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text">主函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#validate"><span class="nav-number">2.1.1.</span> <span class="nav-text">validate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dataset"><span class="nav-number">2.1.2.</span> <span class="nav-text">dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy"><span class="nav-number">2.1.3.</span> <span class="nav-text">policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train-1"><span class="nav-number">2.1.4.</span> <span class="nav-text">train</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vlaflowmatching"><span class="nav-number">2.1.5.</span> <span class="nav-text">VLAFlowMatching</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#init"><span class="nav-number">2.1.5.1.</span> <span class="nav-text">init</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#embed_prefix"><span class="nav-number">2.1.5.2.</span> <span class="nav-text">embed_prefix</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#embed_suffix"><span class="nav-number">2.1.5.3.</span> <span class="nav-text">embed_suffix</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#forward"><span class="nav-number">2.1.5.4.</span> <span class="nav-text">forward</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#smolvlmwithexpertmodel"><span class="nav-number">2.1.6.</span> <span class="nav-text">SmolVLMWithExpertModel</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#init-1"><span class="nav-number">2.1.6.1.</span> <span class="nav-text">init</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#forward-1"><span class="nav-number">2.1.6.2.</span> <span class="nav-text">forward</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="nav-number">2.1.7.</span> <span class="nav-text">训练流程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dataloader"><span class="nav-number">3.</span> <span class="nav-text">dataloader</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#evaluate"><span class="nav-number">4.</span> <span class="nav-text">evaluate</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#config"><span class="nav-number">4.1.</span> <span class="nav-text">config</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#policy_server"><span class="nav-number">4.1.1.</span> <span class="nav-text">policy_server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#robot_clinet"><span class="nav-number">4.1.2.</span> <span class="nav-text">robot_clinet</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">milong26</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">83</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhon.fun/2025/08/17/smolvla/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="milong26">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没啥标题">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="smolvla | 没啥标题">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          smolvla
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-17 22:39:53" itemprop="dateCreated datePublished" datetime="2025-08-17T22:39:53+08:00">2025-08-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-15 22:58:49" itemprop="dateModified" datetime="2025-09-15T22:58:49+08:00">2025-09-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>主要是代码理解和修改</p>
<span id="more"></span>
<h1 id="概念理解">概念理解</h1>
<h2 id="基础">基础</h2>
<ol type="1">
<li>smolvla的核心训练范式始终是模仿学习。</li>
<li>推理的时候，smolvla是只用当前的信息推理，完全没有用之前的，但async
inference有融合一点点以前的action
<ol type="1">
<li>不显式引入历史状态或动作序列</li>
<li>但是Action Chunking是有一点的</li>
<li>然后sa有一点：</li>
</ol>
<blockquote>
<p>We employ a causal attention mask for the SA layers, ensuring that
each action token can only attend to past tokens within the chunk,
preventing future action dependencies. Empirically, we find that
interleaving CA and SA layers provides higher success rates and faster
inference time.我们为 SA
层采用因果注意力掩码，确保每个作令牌只能关注块内过去的令牌，从而防止未来的作依赖。根据经验，我们发现交错
CA 和 SA 层提供了更高的成功率和更快的推理时间。</p>
</blockquote>
翻译一下：SA层仅允许每个动作Token关注​​同一序列中的历史Token​​，无法访问其他样本或未来Token。</li>
</ol>
<h2 id="self-atten和cross-atten">self-atten和cross-atten</h2>
<p>看得稀里糊涂的</p>
<p>假设一个batch是一个车间的工人（假设64个），输入零件，输出组装的东西。工人利用vlm得到高级特征token，比如螺丝和螺母搭好算一个。
cross-attention：对比生成的半成品和图纸
self-attention：确保当前动作和历史动作连贯。仅关注​​自身序列的历史​​</p>
<p>state在整个结构里面处在什么位置 SmolVLA
由一个紧凑的预训练视觉语言模型组成，丢弃最后的 L − N
层（剪刀图标）。其余层嵌入了三个输入：（i）语言指令，（ii）RGB图像，以及（iii）机器人感觉运动状态。它们合并的标记为交替交叉注意力（金色）和自我注意力（浅黄色）块的动作专家提供信息，这些块通过流匹配进行训练，以输出
n 个低级动作块，在 . . . ， at+n。SmolVLA
在公共社区数据集上进行预训练，并在低成本机器人上进行评估。</p>
<h1 id="train">train</h1>
<ol type="1">
<li>除了基础参数外，我需要加的
<ol type="1">
<li>训练的时候用/不用深度图，需要在输入特征里面</li>
<li>训练的时候用哪一种language方法
<ol type="1">
<li>"":baseline</li>
<li>mtask_relative</li>
<li>mtask_grid_xxcm</li>
<li>add_task_to_state好像得单独一个</li>
</ol></li>
</ol></li>
</ol>
<h2 id="主函数">主函数</h2>
<p>def train(cfg: TrainPipelineConfig):</p>
<h3 id="validate">validate</h3>
<p>对policy_path</p>
<p>从 CLI（命令行参数）里获取 --policy
参数对应的路径。会再解析命令行里的 policy
相关覆盖参数（cli_overrides）。用 PreTrainedConfig.from_pretrained(...)
加载 policy 的配置（只加载配置，不加载权重）。记录
self.policy.pretrained_path = policy_path。</p>
<p>也就是当train的cfg有policy的时候，cfg.policy=PreTrainedConfig.from_pretrained(policy_path...)创建了一个类，包含的通常是超参数、路径、模型结构信息等，然后
self.policy.pretrained_path = policy_path。</p>
<p>此时还没有加载模型的配置，那模型的输入特征是在这里决定的吗？</p>
<p>--policy.path=models/forsmolvla/smolvla_base这个是训练的时候的设置，所以只是加载预训练的smolvla_base的输入设置</p>
<p>我看了这个smolvla_base的input
feature规定state是6维，所以我如果想fine-tune的话，得改这个base的设置🤔</p>
<h3 id="dataset">dataset</h3>
<p>dataset = make_dataset(cfg)</p>
<p>应该就是提取cfg里面dataset的部分吧？感觉训练的时候关于datset的处理（用不用深度/文本处理应该写在dataset里面，而不是写在train.py里面）</p>
<p>这个要改就得改lerobotdataset的处理，希望可以自定义一个配置类，先这么些</p>
<ol type="1">
<li>lerobotdataset类的init新增 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面是为了train的自定义内容</span></span><br><span class="line">language_tip_mode: <span class="built_in">str</span>=<span class="string">&quot;&quot;</span>, <span class="comment"># 当空的时候就等于baseline</span></span><br><span class="line">add_location_to_state: <span class="built_in">bool</span> = <span class="literal">False</span>,   <span class="comment"># 控制是将location加到state里面</span></span><br><span class="line">exclude_features: <span class="built_in">list</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,    <span class="comment"># 控制需要过滤的 key，最终的训练（主要是为了过滤掉depth）</span></span><br><span class="line">obj_detector=<span class="literal">None</span>,                     <span class="comment"># 可选的目标检测器，为了state加的</span></span><br></pre></td></tr></table></figure></li>
<li>把 FilteredBatchLoader.add_location_to_state 移到
lerobotDataset里面：_add_location_to_state函数（dataset是无辜的要怪就怪模型结构），调用加在geitem函数里面</li>
<li>filter加载getitem函数的最后，return 之前</li>
<li>make_dataset传入参数</li>
<li>train清理之前的config，传入参数到make_dataset里边</li>
<li>正向流程
<ol type="1">
<li>整理train cfg的模板，增加，同样config/train.py里面也要修改
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">language_tip_mode:</span> <span class="string">str=&quot;&quot;,</span> <span class="comment"># 当空的时候就等于baseline</span></span><br><span class="line"><span class="attr">add_location_to_state:</span> <span class="string">bool</span> <span class="string">=</span> <span class="literal">False</span><span class="string">,</span>   <span class="comment"># 控制是将location加到state里面</span></span><br></pre></td></tr></table></figure></li>
<li>train里面dataste=make_dataset(cfg)</li>
<li>调用lerobotdataset初始化</li>
<li>getitem得到一帧对应的color，depth，state，force，action等</li>
<li>返回一帧的color，state，action</li>
</ol></li>
<li>测试：
<ol type="1">
<li>能不能正常train baseline</li>
<li>能不能正常train language_mode 看task就行</li>
<li>能不能train state（显然不行） 看state</li>
</ol></li>
</ol>
<h3 id="policy">policy</h3>
<ol type="1">
<li>用cfg.policy（就是validate的时候cfg.policy=PreTrainedConfig.from_pretrained(policy_path...)创建的类）和dataset.meta创建policy
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">policy = make_policy(</span><br><span class="line">   cfg=cfg.policy,</span><br><span class="line">   ds_meta=dataset.meta,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>make_policy用meta干什么，meta也要修改，但不能在lerobotdataset里面，而是make_policy的时候。这个policy是我最后要得到的policy吧。meta就规定了input
feature和output feature
<ol type="1">
<li>过滤depth和force这两个feature</li>
<li>当且仅当state要求改的时候才改</li>
</ol></li>
<li>policy = policy_cls.from_pretrained(**kwargs)
<ol type="1">
<li>smolvla里面没有代码，应该调用的是Pretrainedpolicy这个基类的from_pretrained</li>
<li>能保证传入的kwargs是正确的。然后instance = cls(config,
**kwargs)是生成SmolVLAPolicy实例的，为什么这里的cls里面已经没有10那个信息了？
<ol type="1">
<li>cls 实际上是 SmolVLAPolicy
类的初始化，初始化里面有一个加载空flowmatching模型的，就是cls.model=VLAFlowMatching(config)。然后VLAFlowMatching的初始化里面有self.state_proj
= nn.Linear( self.config.max_state_dim,
self.vlm_with_expert.config.text_config.hidden_size )。</li>
<li>所以instance的结果里面只有两个包含state的内容
<ol type="1">
<li>SmolVLAPolicy( (normalize_inputs): Normalize(
(buffer_observation_state): ParameterDict( (mean): Parameter containing:
[torch.FloatTensor of size 6] (std): Parameter containing:
[torch.FloatTensor of size 6] )
)这个是预训练模型的前6维的权重，后面还需要用</li>
<li>(state_proj): Linear(in_features=32, out_features=960,
bias=True)这个是投影到32维</li>
</ol></li>
</ol></li>
</ol></li>
<li>from_pretrained里面policy = cls._load_as_safetensor(instance,
model_file, config.device, strict)
<ol type="1">
<li>cls还是 SmolVLAPolicy，就是调用_load_as_safetensor这个class
method</li>
<li>用的是smolvlapolicy里面的_load_as_safetensor
<ol type="1">
<li>safetensors.torch.load_model(model, model_file, strict=strict,
device=map_location)把权重加载到模型实例里</li>
<li>return load_smolvla( model, model_file, device=map_location,
checkpoint_keys_mapping="model._orig_mod.//model.", )调用特定的 loader
做额外处理</li>
</ol></li>
</ol></li>
<li>load_smolvla函数，我需要在这里确保state的权重能1.前6位正常加，7-10位初始化一下
<ol type="1">
<li>有没有办法识别input
feature此时是6还是10？目前这个模型传到这里只剩下cls两个关于state的部分了，所以从外部引入</li>
<li>给7-10维重置，变成用 Xavier 均匀初始化覆盖,bias 清零</li>
</ol></li>
</ol>
<h3 id="train-1">train</h3>
<ol type="1">
<li>policy.train()运行的是SmolVLMWithExpertModel的train，设置冻结哪一块</li>
<li>开始按步数训练 train_tracker, output_dict =
update_policy(policy)</li>
<li>update_policy函数
<ol type="1">
<li>训练逻辑 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss, output_dict = policy.forward(batch)</span><br><span class="line">grad_scaler.scale(loss).backward()</span><br><span class="line">grad_scaler.unscale_(optimizer)</span><br><span class="line">torch.nn.utils.clip_grad_norm_(policy.parameters(), ...)</span><br><span class="line">grad_scaler.step(optimizer)</span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
问题可能发生在，state更新参数的时候7-10维变化比较大</li>
</ol></li>
<li>normalize归一化没有匹配维度
<ol type="1">
<li>modeling_smolvla里面SmolVLAPolicy的init有self.normalize_inputs =
Normalize(config.input_features, config.normalization_mapping,
dataset_stats)</li>
<li>state前6维的mean，std是从dataset_stats里来的，dataset_stats是从SmolVLAPolicy的init传入的</li>
<li>之前make_policy传入的kwargs["dataset_stats"] =
meta_for_policy.stats，相当于ds_meta.stats</li>
<li>ds_meta是ds_meta=dataset.meta，所以还是本地传入的，看到episode_stats.jsonl文件里面有类似min，max，std的，get
<ol type="1">
<li>怎么写入save_episode有一个write_episode_stats函数</li>
<li>怎么读 直接从meta里面读的，这个我好像改不了</li>
<li>怎么计算：ep_stats = compute_episode_stats(episode_buffer,
self.features)</li>
<li>形式： "observation.state": {"min": [-47.14912414550781,
-96.49805450439453, 9.134847640991211, -1.6986300945281982,
2.4175825119018555, 0.6323396563529968], "max": [8.552631378173828,
44.06614685058594, 99.51667785644531, 42.136985778808594,
52.91819381713867, 56.36856460571289], "mean": [-13.58438491821289,
-31.481496810913086, 56.781070709228516, 22.7191219329834,
25.275249481201172, 15.44166374206543], "std": [19.134763717651367,
52.86572265625, 35.76657485961914, 15.830253601074219,
18.950584411621094, 18.50875473022461], "count":
[181]},也就是一个episode里面所有state每个维度的统计。</li>
<li>然后变成全局的：aggregate_feature_stats</li>
<li>修改过程，使得可以在算出来
<ol type="1">
<li></li>
<li></li>
<li></li>
<li></li>
<li>在正式normalize之前，需要确保能传入datset["observation.state"]以计算</li>
<li>batch =
self.normalize_inputs(batch)这里用的是Normalize的forward，正好传入了batch
<ol type="1">
<li>一个batch里面有什么？若干个epsiode？</li>
<li>怎么normalize的forward里面的mean和std又变成全局的了，全局还不好算？</li>
<li>normalize类用create_stats_buffers这个函数把所有episode的mean和std算到一个全局的里面</li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol>
<h3 id="vlaflowmatching">VLAFlowMatching</h3>
<h4 id="init">init</h4>
<ol type="1">
<li><p>self.vlm_with_expert =
SmolVLMWithExpertModel(...)构建带专家头的多模态主干，这个对象里包含：</p>
<p>视觉编码器（如 SigLIP）+ 语言嵌入层；</p>
<p>一个 Transformer（“VLM”主体）；</p>
<p>动作“专家（Expert）”分支以及处理器（processor / tokenizer）。</p>
<p>此时的self.config.vlm_model_name="models/forsmolvla/HuggingFaceTB/SmolVLM2-500M-Video-Instruct"</p>
<p>所以vlm_with_expert加载的就是冻结的vlm？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">self.vlm_with_expert = SmolVLMWithExpertModel(</span><br><span class="line">            model_id=self.config.vlm_model_name,</span><br><span class="line">            freeze_vision_encoder=self.config.freeze_vision_encoder,</span><br><span class="line">            train_expert_only=self.config.train_expert_only,</span><br><span class="line">            load_vlm_weights=self.config.load_vlm_weights,</span><br><span class="line">            attention_mode=self.config.attention_mode,</span><br><span class="line">            num_expert_layers=self.config.num_expert_layers,</span><br><span class="line">            num_vlm_layers=self.config.num_vlm_layers,</span><br><span class="line">            self_attn_every_n_layers=self.config.self_attn_every_n_layers,</span><br><span class="line">            expert_width_multiplier=self.config.expert_width_multiplier,</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>
<p>这个代码初始化了一个什么？</p></li>
<li><p>self.state_proj这个是把原来的32维的state投影到文本隐藏维（text_config.hidden_size），这个维度是从
self.vlm_with_expert里面来的</p></li>
<li><p>action_in_proj / action_out_proj：将 动作维 映射到专家隐藏维
D_exp，便于送入专家分支；将专家输出再投回
动作维，得到对目标速度场的预测</p></li>
<li><p>set_requires_grad，如果config.train_state_proj是true的话，就会设置state_proj里面的所有参数require_grad为true，所以这里应该是true</p></li>
<li><p>一些关于图像的token，不关注了</p></li>
<li><p>一个prefix_length，默认是 prefix_length: int = -1</p></li>
</ol>
<h4 id="embed_prefix">embed_prefix</h4>
<ol type="1">
<li>输入state</li>
<li>目的：把图像、（可选的）图像特殊 token、语言
token、状态拼成一段“前缀序列”，并生成对应的 pad mask 与
跨段注意力屏蔽标记。</li>
<li>图像嵌入</li>
<li>语言嵌入</li>
<li>状态嵌入 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 状态嵌入，要改肯定改这里，如果要改的话还要加一个，self.config里面有吗？</span></span><br><span class="line">        <span class="comment"># 可以用self.add_location_to_state（或者改成train_add_location_to_state）</span></span><br><span class="line">        <span class="comment"># 投影到self.vlm_with_expert.config.text_config.hidden_size对应的维数，把状态维度投影到 和语言/图像 token 相同的 hidden_size</span></span><br><span class="line">        state_emb = self.state_proj(state)</span><br><span class="line">        <span class="comment"># 如果只有2维？就加个维。这里也应该肯定是2d的吧</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;prefix_embed这里是1d的&quot;</span>,state_emb.ndim == <span class="number">2</span>)</span><br><span class="line">        state_emb = state_emb[:, <span class="literal">None</span>, :] <span class="keyword">if</span> state_emb.ndim == <span class="number">2</span> <span class="keyword">else</span> state_emb</span><br><span class="line">        <span class="comment"># 追加到总的embs,embs从（image_end_token，lang_emb）变成（image_end_token，lang_emb，state_emb）</span></span><br><span class="line">        embs.append(state_emb)</span><br><span class="line">        <span class="comment"># bsize就是batchsize</span></span><br><span class="line">        bsize = state_emb.shape[<span class="number">0</span>]</span><br><span class="line">        device = state_emb.device</span><br><span class="line">        <span class="comment"># states_seq_len就是状态token的个数</span></span><br><span class="line">        states_seq_len = state_emb.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 这段token没有padding，形状是((B, states_seq_len))</span></span><br><span class="line">        state_mask = torch.ones(bsize, states_seq_len, dtype=torch.<span class="built_in">bool</span>, device=device)</span><br><span class="line">        pad_masks.append(state_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set attention masks so that image and language inputs do not attend to state or actions</span></span><br><span class="line">        att_masks += [<span class="number">1</span>] * (states_seq_len)</span><br></pre></td></tr></table></figure> pad_mask和att_mask</li>
</ol>
<p>pad_masks（padding mask）</p>
<p>作用：标记哪些 token 是“真实有效的”，哪些是 padding（补齐长度）。</p>
<p>取值：布尔值 True/1 = 有效，False/0 = padding。</p>
<p>形状：(B, L_total)，对应整个序列每个位置。</p>
<p>att_masks：attention 域掩码</p>
<p>作用：告诉 make_att_2d_masks 不同 token
属于哪个“域（domain）”，从而控制谁可以 attend 谁。</p>
<p>这个函数最后，pad_masks是表示哪个token是有效的（state这里应该都是有效），att_masks的话，图和task是0，state是1</p>
<h4 id="embed_suffix">embed_suffix</h4>
<p>把 动作序列 与 时间 t 融合，作为“后缀序列”，只包含动作端的
token。跟state倒是没什么关系 att_masks 为长度 T_act 的 1 序列。
配合前缀里的 0/1，通常表示“后缀的动作 token
属于另一个注意力域”，从而避免图像/文本去看动作；也可允许状态/动作之间的相互可见性（具体由
make_att_2d_masks 决定）。</p>
<p>为什么action也要是1？大概懂了点，要分为条件输入和决策输入，emm主要是我不知道还能放在哪儿，理论上来说这个位置信息肯定是放在0比较好的</p>
<h4 id="forward">forward</h4>
<p>目标：学习一个 速度场 v_θ(x_t, t)，去逼近真速度 u_t = ε - x_0
的等价形式（此实现里是 ε - actions），属于 flow
matching/噪声驱动的动作建模思想。</p>
<h3 id="smolvlmwithexpertmodel">SmolVLMWithExpertModel</h3>
<h4 id="init-1">init</h4>
<ol type="1">
<li>self.vlm和self.procesor的区别
<ol type="1">
<li>self.vlm是AutoModelForImageTextToText.from_pretrained(
model_id)，g会加载一个 视觉-语言-文本（VLM）大模型</li>
<li>self.processor =
AutoProcessor.from_pretrained(model_id)看起来都是from_pretrained(model_id)</li>
<li>self.vlm是模型权重 + 前向推理逻辑，类似 transformers 里常见的
AutoModelForCausalLM。这里是 Image+Text → Text
的多模态大模型（视觉-语言模型，里面应该包含
<ol type="1">
<li>model
<ol type="1">
<li>是整个 VLM 的 主体 backbone，直接调用 self.vlm.model(...) 会自动执行
vision_model + connector，给你一个对齐到 text hidden_size 的
embedding</li>
<li>.vision_model视觉编码器（把图片转成
embedding）纯粹的encoder，只负责「把图像转成 patch embedding」
<ol type="1">
<li>和VLAFlowMatching里面的prefix_embed的区别是：embed_prefix
不自己做图像编码，它只是一个「拼接器」</li>
<li>img_emb =
self.vlm_with_expert.embed_image(img)这里调用的就是self.vlm_with_experts.vlm.model</li>
</ol></li>
<li>.text_model：文本编码器/解码器
<ol type="1">
<li>同理，embed_prefix里面对state和language的处理也类似，关注state：</li>
</ol></li>
<li>.connector：模态对齐层（把 vision hidden state 映射到 text hidden
space）</li>
</ol></li>
<li>forward</li>
<li>generate</li>
</ol></li>
<li>self.processor是输入预处理器，负责把原始数据（图像、文字字符串）转成
self.vlm 能接受的张量
<ol type="1">
<li>对图像：resize、归一化、转 tensor</li>
<li>对文本：tokenize → token_id → attention_mask</li>
</ol></li>
</ol></li>
<li>self.vlm.model.text_model.layers减少到前num_vlm_layers（16）层</li>
<li>构建更窄的 expert 部分lm_expert
<ol type="1">
<li>lm_expert_config =
copy.deepcopy(config.text_config)为什么只用text_config？因为专家模型只管
text 部分（不管
vision），vision也被拼接到text里面了。先得到一个大概的config结构，和self.vlm.config.text_config是一样的大小。</li>
<li>更窄
<ol type="1">
<li>原来mlp里面的结构是这样的：hidden_size → intermediate_size →
hidden_size，也就是先把先把 hidden
向量投影到一个更宽的空间（intermediate_size），再投影回
hidden_size，这样模型有更强的表达能力。</li>
<li>hudden_size （每个 token embedding 的维度，Transformer
层的输入/输出维度）</li>
<li>intermediate_size FFN 里间层宽度，挺复杂的不用管这个经验公式</li>
<li>num_hidden_layers 堆多少层 Transformer，因为 Expert 要“对应”VLM
的层结构，方便做跨注意力对齐。这样一层 Expert 对应一层 VLM，更容易设计
cross-attn。如果额外设置了一个num_expert_layers，vlm跟expert不是一层层对应，但是expert_layer要是16的因数，为了方便稀疏交互</li>
</ol></li>
<li>self.lm_expert = AutoModel.from_config(lm_expert_config)结构和
VLM.text_model 一样，但更小、更窄</li>
<li>大小确定了，值是需要自己训练的，也就是expert需要单独训练。</li>
</ol></li>
<li>atten_mode
<ol type="1">
<li>因为 cross attention 模式下，Expert 不直接用自己的 K/V，而是用大 VLM
的 K/V 表示，改值，所以需要维度也匹配一下，不然vlm的值拿不过来：当该层做
cross attention 时 → Expert 的输入是 VLM 传下来的特征（比如
hidden_size=1024），必须经过这个替换过的 Linear，投影到 Expert
自己的宽度（比如 hidden_size=512）。。vlm不是冻结的吗，也有kv？</li>
<li>不是每一层 Expert 都用 cross attention，每 xx 层保留一次
纯自注意力，其他层就把 K/V 换成来自 VLM 的（投影过的）K/V，也就是cross
attention和self-attention</li>
</ol></li>
<li>专家不自己负责词嵌入，统一用 VLM 的词嵌入或上游传入的
inputs_embeds。self.lm_expert.embed_tokens = None</li>
<li>set_requires_grad
<ol type="1">
<li>如果self.freeze_vision_encoder设置为true：整个vlm冻结，expert可以训练，只更新expert的权重</li>
<li>train_expert_only=True</li>
</ol></li>
<li>self.vlm,self.vlm.model,self.vlm.model.vision_encoder</li>
</ol>
<h4 id="forward-1">forward</h4>
<ol type="1">
<li>batch_size</li>
<li>对每一层要么self-atten要么cross-atten，记录结果att_outputs,
past_key_values</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">att_outputs,past_key_values=self_or_cross-attention(</span><br><span class="line">   model_layers,     <span class="comment"># 当前层的子模型层 (vlm 层 + expert 层) 是可训练的部分，包含全部需要更新的权重。</span></span><br><span class="line">inputs_embeds,    <span class="comment"># 该层的输入 hidden states [vlm_embeds, expert_embeds]</span></span><br><span class="line">layer_idx,        <span class="comment"># 当前层编号</span></span><br><span class="line">position_ids,     <span class="comment"># 每个 token 的位置 id (padding 通常是 0 或特殊处理)</span></span><br><span class="line">attention_mask,   <span class="comment"># 注意力 mask，控制哪些 token 能被看到</span></span><br><span class="line">batch_size,       <span class="comment"># 批大小 (B）</span></span><br><span class="line">head_dim,         <span class="comment"># 每个 head 的维度 (D)</span></span><br><span class="line">use_cache,        <span class="comment"># 是否使用 KV cache (推理时加速)</span></span><br><span class="line">fill_kv_cache,    <span class="comment"># 是否往 cache 里填新的 K/V (True=训练; False=推理复用旧KV)</span></span><br><span class="line">past_key_values   <span class="comment"># 存储历史 KV 的字典</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>att_outputs 和 past_key_values 在整个模型中的位置</p>
<p>这两个是 中间结果，在模型 forward 的流水线上扮演不同角色：</p>
<p>att_outputs</p>
<p>这是这一层（vlm + expert）的 attention 输出 hidden states。</p>
<p>会作为 下一层的输入，一路传到最后一层 → 接 decoder head / classifier
head。</p>
<p>所以它处于 「主干计算图」 中，梯度会往回传。</p>
<p>相当于 transformer 层的 hidden_states。</p>
<p>past_key_values</p>
<p>保存每一层的 KV (key, value) 张量。</p>
<p>在训练时 可以不用（因为我们每次都全量计算），</p>
<p>在推理时（自回归生成），用它来避免重复计算旧 token 的 KV。</p>
<p>它处于 「缓存/优化分支」，通常不会参与梯度计算。</p>
<p>只存储 forward 的中间结果，不会影响训练更新</p>
<h3 id="训练流程">训练流程</h3>
<ol type="1">
<li>train
<ol type="1">
<li></li>
</ol></li>
<li>smolvlapolicy
<ol type="1">
<li>init
dataset_stats和config.input_features里面的state维度要对应，改一下dataset_stats
<ol type="1">
<li>policy =
make_policy(ds_meta)这个ds_meta就是dataset_stats吗？加上</li>
</ol></li>
<li>forward此时传入的batch的state是10维</li>
</ol></li>
<li>vlaflowmatching
<ol type="1">
<li>self.vlm_with_expert = SmolVLMWithExpertModel</li>
<li>self.state_proj = nn.Linear( self.config.max_state_dim,
self.vlm_with_expert.config.text_config.hidden_size )</li>
<li>(<em>, suffix_out), </em> = self.vlm_with_expert.forward(
attention_mask=att_2d_masks, position_ids=position_ids,
past_key_values=None, inputs_embeds=[prefix_embs, suffix_embs],
use_cache=False, fill_kv_cache=False, )</li>
</ol></li>
<li>smolvlmwithexpert
<ol type="1">
<li>init</li>
<li>forward，传入的
<ol type="1">
<li>inputs_embeds：外部准备好一个 [batch, seq_len, hidden_dim]
的张量，它都会进入 forward 流程</li>
<li>position_ids：这只影响 RoPE（旋转位置编码），它决定每个 token
在序列里的位置。和你要训练的 state
的值域/维度没有直接关系，只是告诉注意力计算“第 i 个 token
的位置是多少”。</li>
</ol></li>
</ol></li>
</ol>
<h1 id="dataloader">dataloader</h1>
<p>之前写在train.py里面的，for key in batch:之后，为了保存数据到本地
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">states = batch[<span class="string">&quot;observation.state&quot;</span>].detach().cpu()</span><br><span class="line"><span class="keyword">if</span> states.dim() == <span class="number">3</span>:   <span class="comment"># e.g. [B, 1, 10] → squeeze掉</span></span><br><span class="line">    states = states.squeeze(<span class="number">1</span>)</span><br><span class="line">episode_indices = batch[<span class="string">&quot;episode_index&quot;</span>].detach().cpu().tolist()</span><br><span class="line">frame_indices = batch[<span class="string">&quot;frame_index&quot;</span>].detach().cpu().tolist()</span><br><span class="line">states_list = states.numpy().tolist()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;modified_states.jsonl&quot;</span>, <span class="string">&quot;a&quot;</span>) <span class="keyword">as</span> f:  <span class="comment"># 追加写入</span></span><br><span class="line">    <span class="keyword">for</span> ep, fr, st <span class="keyword">in</span> <span class="built_in">zip</span>(episode_indices, frame_indices, states_list):</span><br><span class="line">        record = &#123;</span><br><span class="line">            <span class="string">&quot;episode_index&quot;</span>: <span class="built_in">int</span>(ep),</span><br><span class="line">            <span class="string">&quot;frame_index&quot;</span>: <span class="built_in">int</span>(fr),</span><br><span class="line">            <span class="string">&quot;state&quot;</span>: st</span><br><span class="line">        &#125;</span><br><span class="line">        f.write(json.dumps(record, ensure_ascii=<span class="literal">False</span>) + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="evaluate">evaluate</h1>
<ol type="1">
<li>要新增的功能
<ol type="1">
<li>服务器端推理次数统计：只要在policy_server端增加log就行，init里面搞个计数器</li>
<li>能测试baseline，修改task和修改state
<ol type="1">
<li>task各种形式</li>
<li>state要proj和normalize</li>
</ol></li>
</ol></li>
</ol>
<h2 id="config">config</h2>
<p>基本上处理都可以从pretrained_name_or_path里面看出来吧，只要在配置的yaml文件里面写清楚model的path就行，然后服务器根据名字选择出
mtask,mstate和baseline</p>
<ol type="1">
<li>baseline</li>
<li>mtask_
<ol type="1">
<li>relative</li>
<li>grid_2cm</li>
<li>grid_5cm</li>
</ol></li>
<li>mstate_
<ol type="1">
<li>relative</li>
<li>1m</li>
</ol></li>
</ol>
<h3 id="policy_server">policy_server</h3>
<ol type="1">
<li>根据config得到控制</li>
<li>推理之前_predict_action_chunk里面处理state和task</li>
<li>推理state的时候是否还需要修改normalize？
<ol type="1">
<li>prepare_batch函数里面看到了，来改吧</li>
<li>之前forward函数里面有batch =
self.normalize_inputs(batch,adding_state_stat)</li>
<li>需要确保self.add_location_to_state有对应的值
<ol type="1">
<li>self.add_location_to_state=config.add_location_to_state</li>
<li>cfg是SmolVLAConfig，也就是模型初始化的时候要有add_location_to_state这个设置</li>
</ol></li>
<li>self.add_state_dim加载正确</li>
</ol></li>
<li>推理的时候去掉side_depth这个多余的feature</li>
</ol>
<h3 id="robot_clinet">robot_clinet</h3>
<ol type="1">
<li>注释掉validate_robot_cameras_for_policy(lerobot_features,
policy_image_features)，因为本地不用这个</li>
</ol>
<p>8.29重新整理代码，今晚收集第二个物体+整理代码</p>
<p>git remote add upstream https://github.com/huggingface/lerobot.git
之前更新的时候不小心丢掉了</p>
<ol type="1">
<li>camera:realsense camera更改比较多，主要是新增保存双通道的深度图</li>
<li>scripts/train.py</li>
<li>configs/train.py 增加了3个命令行参数 # 自定义 #
当空的时候就等于baseline<br />
language_tip_mode: str = "" # 改成模式，有 pure和grid #
控制是将location加到state里面 add_location_to_state: str = ""
freeze_except_7_10: bool= False</li>
</ol>
<p>factory.py 里面make_dataset</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/vla/" rel="tag"># vla</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/08/04/cuda/" rel="prev" title="cuda">
                  <i class="fa fa-angle-left"></i> cuda
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/08/20/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%BB%91%E8%AF%9D/" rel="next" title="模型训练黑话">
                  模型训练黑话 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">milong26</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
