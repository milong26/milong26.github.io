<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhon.fun","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"æœç´¢...","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","hits":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœ"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="ä¸»è¦æ˜¯ä»£ç ç†è§£å’Œä¿®æ”¹">
<meta property="og:type" content="article">
<meta property="og:title" content="smolvla">
<meta property="og:url" content="http://zhon.fun/2025/08/17/smolvla/index.html">
<meta property="og:site_name" content="æ²¡å•¥æ ‡é¢˜">
<meta property="og:description" content="ä¸»è¦æ˜¯ä»£ç ç†è§£å’Œä¿®æ”¹">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-17T14:39:53.000Z">
<meta property="article:modified_time" content="2025-12-29T08:23:26.451Z">
<meta property="article:author" content="milong26">
<meta property="article:tag" content="vla">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://zhon.fun/2025/08/17/smolvla/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://zhon.fun/2025/08/17/smolvla/","path":"2025/08/17/smolvla/","title":"smolvla"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>smolvla | æ²¡å•¥æ ‡é¢˜</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ " role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">æ²¡å•¥æ ‡é¢˜</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">æ´»ä¸‹å»</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="æœç´¢" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5%E7%90%86%E8%A7%A3"><span class="nav-number">1.</span> <span class="nav-text">æ¦‚å¿µç†è§£</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80"><span class="nav-number">1.1.</span> <span class="nav-text">åŸºç¡€</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-atten%E5%92%8Ccross-atten"><span class="nav-number">1.2.</span> <span class="nav-text">self-attenå’Œcross-atten</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#train"><span class="nav-number">2.</span> <span class="nav-text">train</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text">ä¸»å‡½æ•°</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#validate"><span class="nav-number">2.1.1.</span> <span class="nav-text">validate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dataset"><span class="nav-number">2.1.2.</span> <span class="nav-text">dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy"><span class="nav-number">2.1.3.</span> <span class="nav-text">policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train-1"><span class="nav-number">2.1.4.</span> <span class="nav-text">train</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vlaflowmatching"><span class="nav-number">2.1.5.</span> <span class="nav-text">VLAFlowMatching</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#init"><span class="nav-number">2.1.5.1.</span> <span class="nav-text">init</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#embed_prefix"><span class="nav-number">2.1.5.2.</span> <span class="nav-text">embed_prefix</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#embed_suffix"><span class="nav-number">2.1.5.3.</span> <span class="nav-text">embed_suffix</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#forward"><span class="nav-number">2.1.5.4.</span> <span class="nav-text">forward</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#smolvlmwithexpertmodel"><span class="nav-number">2.1.6.</span> <span class="nav-text">SmolVLMWithExpertModel</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#init-1"><span class="nav-number">2.1.6.1.</span> <span class="nav-text">init</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#forward-1"><span class="nav-number">2.1.6.2.</span> <span class="nav-text">forward</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="nav-number">2.1.7.</span> <span class="nav-text">è®­ç»ƒæµç¨‹</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dataloader"><span class="nav-number">3.</span> <span class="nav-text">dataloader</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#evaluate"><span class="nav-number">4.</span> <span class="nav-text">evaluate</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#config"><span class="nav-number">4.1.</span> <span class="nav-text">config</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#policy_server"><span class="nav-number">4.1.1.</span> <span class="nav-text">policy_server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#robot_clinet"><span class="nav-number">4.1.2.</span> <span class="nav-text">robot_clinet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lm-expert-%E8%BE%93%E5%85%A5"><span class="nav-number">4.1.3.</span> <span class="nav-text">2ï¸âƒ£ LM Expert è¾“å…¥</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#state_adapter-%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E5%92%8C%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">4.2.</span> <span class="nav-text">3ï¸âƒ£ state_adapter
è¾“å…¥ã€è¾“å‡ºå’Œæ¨¡å‹ç»“æ„</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%E6%96%B9%E6%A1%88"><span class="nav-number">4.3.</span> <span class="nav-text">4ï¸âƒ£ å¾®è°ƒæ–¹æ¡ˆ</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%BB%E7%BB%93%E9%A2%84%E8%AE%AD%E7%BB%83-lm-expert"><span class="nav-number">4.3.1.</span> <span class="nav-text">4.1 å†»ç»“é¢„è®­ç»ƒ LM expert</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">4.3.2.</span> <span class="nav-text">4.2 ä¼˜åŒ–å™¨</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B-1"><span class="nav-number">4.3.3.</span> <span class="nav-text">4.3 è®­ç»ƒæµç¨‹</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%BB%BA%E8%AE%AE"><span class="nav-number">4.3.4.</span> <span class="nav-text">4.4 è®­ç»ƒå»ºè®®</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">milong26</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">88</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">80</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhon.fun/2025/08/17/smolvla/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="milong26">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="æ²¡å•¥æ ‡é¢˜">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="smolvla | æ²¡å•¥æ ‡é¢˜">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          smolvla
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">å‘è¡¨äº</span>

      <time title="åˆ›å»ºæ—¶é—´ï¼š2025-08-17 22:39:53" itemprop="dateCreated datePublished" datetime="2025-08-17T22:39:53+08:00">2025-08-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">æ›´æ–°äº</span>
      <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-12-29 16:23:26" itemprop="dateModified" datetime="2025-12-29T16:23:26+08:00">2025-12-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">åˆ†ç±»äº</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>ä¸»è¦æ˜¯ä»£ç ç†è§£å’Œä¿®æ”¹</p>
<span id="more"></span>
<h1 id="æ¦‚å¿µç†è§£">æ¦‚å¿µç†è§£</h1>
<h2 id="åŸºç¡€">åŸºç¡€</h2>
<ol type="1">
<li>smolvlaçš„æ ¸å¿ƒè®­ç»ƒèŒƒå¼å§‹ç»ˆæ˜¯æ¨¡ä»¿å­¦ä¹ ã€‚</li>
<li>æ¨ç†çš„æ—¶å€™ï¼Œsmolvlaæ˜¯åªç”¨å½“å‰çš„ä¿¡æ¯æ¨ç†ï¼Œå®Œå…¨æ²¡æœ‰ç”¨ä¹‹å‰çš„ï¼Œä½†async
inferenceæœ‰èåˆä¸€ç‚¹ç‚¹ä»¥å‰çš„action
<ol type="1">
<li>ä¸æ˜¾å¼å¼•å…¥å†å²çŠ¶æ€æˆ–åŠ¨ä½œåºåˆ—</li>
<li>ä½†æ˜¯Action Chunkingæ˜¯æœ‰ä¸€ç‚¹çš„</li>
<li>ç„¶åsaæœ‰ä¸€ç‚¹ï¼š</li>
</ol>
<blockquote>
<p>We employ a causal attention mask for the SA layers, ensuring that
each action token can only attend to past tokens within the chunk,
preventing future action dependencies. Empirically, we find that
interleaving CA and SA layers provides higher success rates and faster
inference time.æˆ‘ä»¬ä¸º SA
å±‚é‡‡ç”¨å› æœæ³¨æ„åŠ›æ©ç ï¼Œç¡®ä¿æ¯ä¸ªä½œä»¤ç‰Œåªèƒ½å…³æ³¨å—å†…è¿‡å»çš„ä»¤ç‰Œï¼Œä»è€Œé˜²æ­¢æœªæ¥çš„ä½œä¾èµ–ã€‚æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬å‘ç°äº¤é”™
CA å’Œ SA å±‚æä¾›äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ›´å¿«çš„æ¨ç†æ—¶é—´ã€‚</p>
</blockquote>
ç¿»è¯‘ä¸€ä¸‹ï¼šSAå±‚ä»…å…è®¸æ¯ä¸ªåŠ¨ä½œTokenå…³æ³¨â€‹â€‹åŒä¸€åºåˆ—ä¸­çš„å†å²Tokenâ€‹â€‹ï¼Œæ— æ³•è®¿é—®å…¶ä»–æ ·æœ¬æˆ–æœªæ¥Tokenã€‚</li>
</ol>
<h2 id="self-attenå’Œcross-atten">self-attenå’Œcross-atten</h2>
<p>çœ‹å¾—ç¨€é‡Œç³Šæ¶‚çš„</p>
<p>å‡è®¾ä¸€ä¸ªbatchæ˜¯ä¸€ä¸ªè½¦é—´çš„å·¥äººï¼ˆå‡è®¾64ä¸ªï¼‰ï¼Œè¾“å…¥é›¶ä»¶ï¼Œè¾“å‡ºç»„è£…çš„ä¸œè¥¿ã€‚å·¥äººåˆ©ç”¨vlmå¾—åˆ°é«˜çº§ç‰¹å¾tokenï¼Œæ¯”å¦‚èºä¸å’Œèºæ¯æ­å¥½ç®—ä¸€ä¸ªã€‚
cross-attentionï¼šå¯¹æ¯”ç”Ÿæˆçš„åŠæˆå“å’Œå›¾çº¸
self-attentionï¼šç¡®ä¿å½“å‰åŠ¨ä½œå’Œå†å²åŠ¨ä½œè¿è´¯ã€‚ä»…å…³æ³¨â€‹â€‹è‡ªèº«åºåˆ—çš„å†å²â€‹â€‹</p>
<p>stateåœ¨æ•´ä¸ªç»“æ„é‡Œé¢å¤„åœ¨ä»€ä¹ˆä½ç½® SmolVLA
ç”±ä¸€ä¸ªç´§å‡‘çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ç»„æˆï¼Œä¸¢å¼ƒæœ€åçš„ L âˆ’ N
å±‚ï¼ˆå‰ªåˆ€å›¾æ ‡ï¼‰ã€‚å…¶ä½™å±‚åµŒå…¥äº†ä¸‰ä¸ªè¾“å…¥ï¼šï¼ˆiï¼‰è¯­è¨€æŒ‡ä»¤ï¼Œï¼ˆiiï¼‰RGBå›¾åƒï¼Œä»¥åŠï¼ˆiiiï¼‰æœºå™¨äººæ„Ÿè§‰è¿åŠ¨çŠ¶æ€ã€‚å®ƒä»¬åˆå¹¶çš„æ ‡è®°ä¸ºäº¤æ›¿äº¤å‰æ³¨æ„åŠ›ï¼ˆé‡‘è‰²ï¼‰å’Œè‡ªæˆ‘æ³¨æ„åŠ›ï¼ˆæµ…é»„è‰²ï¼‰å—çš„åŠ¨ä½œä¸“å®¶æä¾›ä¿¡æ¯ï¼Œè¿™äº›å—é€šè¿‡æµåŒ¹é…è¿›è¡Œè®­ç»ƒï¼Œä»¥è¾“å‡º
n ä¸ªä½çº§åŠ¨ä½œå—ï¼Œåœ¨ . . . ï¼Œ at+nã€‚SmolVLA
åœ¨å…¬å…±ç¤¾åŒºæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨ä½æˆæœ¬æœºå™¨äººä¸Šè¿›è¡Œè¯„ä¼°ã€‚</p>
<h1 id="train">train</h1>
<ol type="1">
<li>é™¤äº†åŸºç¡€å‚æ•°å¤–ï¼Œæˆ‘éœ€è¦åŠ çš„
<ol type="1">
<li>è®­ç»ƒçš„æ—¶å€™ç”¨/ä¸ç”¨æ·±åº¦å›¾ï¼Œéœ€è¦åœ¨è¾“å…¥ç‰¹å¾é‡Œé¢</li>
<li>è®­ç»ƒçš„æ—¶å€™ç”¨å“ªä¸€ç§languageæ–¹æ³•
<ol type="1">
<li>"":baseline</li>
<li>mtask_relative</li>
<li>mtask_grid_xxcm</li>
<li>add_task_to_stateå¥½åƒå¾—å•ç‹¬ä¸€ä¸ª</li>
</ol></li>
</ol></li>
</ol>
<h2 id="ä¸»å‡½æ•°">ä¸»å‡½æ•°</h2>
<p>def train(cfg: TrainPipelineConfig):</p>
<h3 id="validate">validate</h3>
<p>å¯¹policy_path</p>
<p>ä» CLIï¼ˆå‘½ä»¤è¡Œå‚æ•°ï¼‰é‡Œè·å– --policy
å‚æ•°å¯¹åº”çš„è·¯å¾„ã€‚ä¼šå†è§£æå‘½ä»¤è¡Œé‡Œçš„ policy
ç›¸å…³è¦†ç›–å‚æ•°ï¼ˆcli_overridesï¼‰ã€‚ç”¨ PreTrainedConfig.from_pretrained(...)
åŠ è½½ policy çš„é…ç½®ï¼ˆåªåŠ è½½é…ç½®ï¼Œä¸åŠ è½½æƒé‡ï¼‰ã€‚è®°å½•
self.policy.pretrained_path = policy_pathã€‚</p>
<p>ä¹Ÿå°±æ˜¯å½“trainçš„cfgæœ‰policyçš„æ—¶å€™ï¼Œcfg.policy=PreTrainedConfig.from_pretrained(policy_path...)åˆ›å»ºäº†ä¸€ä¸ªç±»ï¼ŒåŒ…å«çš„é€šå¸¸æ˜¯è¶…å‚æ•°ã€è·¯å¾„ã€æ¨¡å‹ç»“æ„ä¿¡æ¯ç­‰ï¼Œç„¶å
self.policy.pretrained_path = policy_pathã€‚</p>
<p>æ­¤æ—¶è¿˜æ²¡æœ‰åŠ è½½æ¨¡å‹çš„é…ç½®ï¼Œé‚£æ¨¡å‹çš„è¾“å…¥ç‰¹å¾æ˜¯åœ¨è¿™é‡Œå†³å®šçš„å—ï¼Ÿ</p>
<p>--policy.path=models/forsmolvla/smolvla_baseè¿™ä¸ªæ˜¯è®­ç»ƒçš„æ—¶å€™çš„è®¾ç½®ï¼Œæ‰€ä»¥åªæ˜¯åŠ è½½é¢„è®­ç»ƒçš„smolvla_baseçš„è¾“å…¥è®¾ç½®</p>
<p>æˆ‘çœ‹äº†è¿™ä¸ªsmolvla_baseçš„input
featureè§„å®šstateæ˜¯6ç»´ï¼Œæ‰€ä»¥æˆ‘å¦‚æœæƒ³fine-tuneçš„è¯ï¼Œå¾—æ”¹è¿™ä¸ªbaseçš„è®¾ç½®ğŸ¤”</p>
<h3 id="dataset">dataset</h3>
<p>dataset = make_dataset(cfg)</p>
<p>åº”è¯¥å°±æ˜¯æå–cfgé‡Œé¢datasetçš„éƒ¨åˆ†å§ï¼Ÿæ„Ÿè§‰è®­ç»ƒçš„æ—¶å€™å…³äºdatsetçš„å¤„ç†ï¼ˆç”¨ä¸ç”¨æ·±åº¦/æ–‡æœ¬å¤„ç†åº”è¯¥å†™åœ¨dataseté‡Œé¢ï¼Œè€Œä¸æ˜¯å†™åœ¨train.pyé‡Œé¢ï¼‰</p>
<p>è¿™ä¸ªè¦æ”¹å°±å¾—æ”¹lerobotdatasetçš„å¤„ç†ï¼Œå¸Œæœ›å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªé…ç½®ç±»ï¼Œå…ˆè¿™ä¹ˆäº›</p>
<ol type="1">
<li>lerobotdatasetç±»çš„initæ–°å¢ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¸‹é¢æ˜¯ä¸ºäº†trainçš„è‡ªå®šä¹‰å†…å®¹</span></span><br><span class="line">language_tip_mode: <span class="built_in">str</span>=<span class="string">&quot;&quot;</span>, <span class="comment"># å½“ç©ºçš„æ—¶å€™å°±ç­‰äºbaseline</span></span><br><span class="line">add_location_to_state: <span class="built_in">bool</span> = <span class="literal">False</span>,   <span class="comment"># æ§åˆ¶æ˜¯å°†locationåŠ åˆ°stateé‡Œé¢</span></span><br><span class="line">exclude_features: <span class="built_in">list</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,    <span class="comment"># æ§åˆ¶éœ€è¦è¿‡æ»¤çš„ keyï¼Œæœ€ç»ˆçš„è®­ç»ƒï¼ˆä¸»è¦æ˜¯ä¸ºäº†è¿‡æ»¤æ‰depthï¼‰</span></span><br><span class="line">obj_detector=<span class="literal">None</span>,                     <span class="comment"># å¯é€‰çš„ç›®æ ‡æ£€æµ‹å™¨ï¼Œä¸ºäº†stateåŠ çš„</span></span><br></pre></td></tr></table></figure></li>
<li>æŠŠ FilteredBatchLoader.add_location_to_state ç§»åˆ°
lerobotDataseté‡Œé¢ï¼š_add_location_to_stateå‡½æ•°ï¼ˆdatasetæ˜¯æ— è¾œçš„è¦æ€ªå°±æ€ªæ¨¡å‹ç»“æ„ï¼‰ï¼Œè°ƒç”¨åŠ åœ¨geitemå‡½æ•°é‡Œé¢</li>
<li>filteråŠ è½½getitemå‡½æ•°çš„æœ€åï¼Œreturn ä¹‹å‰</li>
<li>make_datasetä¼ å…¥å‚æ•°</li>
<li>trainæ¸…ç†ä¹‹å‰çš„configï¼Œä¼ å…¥å‚æ•°åˆ°make_dataseté‡Œè¾¹</li>
<li>æ­£å‘æµç¨‹
<ol type="1">
<li>æ•´ç†train cfgçš„æ¨¡æ¿ï¼Œå¢åŠ ï¼ŒåŒæ ·config/train.pyé‡Œé¢ä¹Ÿè¦ä¿®æ”¹
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">language_tip_mode:</span> <span class="string">str=&quot;&quot;,</span> <span class="comment"># å½“ç©ºçš„æ—¶å€™å°±ç­‰äºbaseline</span></span><br><span class="line"><span class="attr">add_location_to_state:</span> <span class="string">bool</span> <span class="string">=</span> <span class="literal">False</span><span class="string">,</span>   <span class="comment"># æ§åˆ¶æ˜¯å°†locationåŠ åˆ°stateé‡Œé¢</span></span><br></pre></td></tr></table></figure></li>
<li>trainé‡Œé¢dataste=make_dataset(cfg)</li>
<li>è°ƒç”¨lerobotdatasetåˆå§‹åŒ–</li>
<li>getitemå¾—åˆ°ä¸€å¸§å¯¹åº”çš„colorï¼Œdepthï¼Œstateï¼Œforceï¼Œactionç­‰</li>
<li>è¿”å›ä¸€å¸§çš„colorï¼Œstateï¼Œaction</li>
</ol></li>
<li>æµ‹è¯•ï¼š
<ol type="1">
<li>èƒ½ä¸èƒ½æ­£å¸¸train baseline</li>
<li>èƒ½ä¸èƒ½æ­£å¸¸train language_mode çœ‹taskå°±è¡Œ</li>
<li>èƒ½ä¸èƒ½train stateï¼ˆæ˜¾ç„¶ä¸è¡Œï¼‰ çœ‹state</li>
</ol></li>
</ol>
<h3 id="policy">policy</h3>
<ol type="1">
<li>ç”¨cfg.policyï¼ˆå°±æ˜¯validateçš„æ—¶å€™cfg.policy=PreTrainedConfig.from_pretrained(policy_path...)åˆ›å»ºçš„ç±»ï¼‰å’Œdataset.metaåˆ›å»ºpolicy
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">policy = make_policy(</span><br><span class="line">   cfg=cfg.policy,</span><br><span class="line">   ds_meta=dataset.meta,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>make_policyç”¨metaå¹²ä»€ä¹ˆï¼Œmetaä¹Ÿè¦ä¿®æ”¹ï¼Œä½†ä¸èƒ½åœ¨lerobotdataseté‡Œé¢ï¼Œè€Œæ˜¯make_policyçš„æ—¶å€™ã€‚è¿™ä¸ªpolicyæ˜¯æˆ‘æœ€åè¦å¾—åˆ°çš„policyå§ã€‚metaå°±è§„å®šäº†input
featureå’Œoutput feature
<ol type="1">
<li>è¿‡æ»¤depthå’Œforceè¿™ä¸¤ä¸ªfeature</li>
<li>å½“ä¸”ä»…å½“stateè¦æ±‚æ”¹çš„æ—¶å€™æ‰æ”¹</li>
</ol></li>
<li>policy = policy_cls.from_pretrained(**kwargs)
<ol type="1">
<li>smolvlaé‡Œé¢æ²¡æœ‰ä»£ç ï¼Œåº”è¯¥è°ƒç”¨çš„æ˜¯Pretrainedpolicyè¿™ä¸ªåŸºç±»çš„from_pretrained</li>
<li>èƒ½ä¿è¯ä¼ å…¥çš„kwargsæ˜¯æ­£ç¡®çš„ã€‚ç„¶åinstance = cls(config,
**kwargs)æ˜¯ç”ŸæˆSmolVLAPolicyå®ä¾‹çš„ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œçš„clsé‡Œé¢å·²ç»æ²¡æœ‰10é‚£ä¸ªä¿¡æ¯äº†ï¼Ÿ
<ol type="1">
<li>cls å®é™…ä¸Šæ˜¯ SmolVLAPolicy
ç±»çš„åˆå§‹åŒ–ï¼Œåˆå§‹åŒ–é‡Œé¢æœ‰ä¸€ä¸ªåŠ è½½ç©ºflowmatchingæ¨¡å‹çš„ï¼Œå°±æ˜¯cls.model=VLAFlowMatching(config)ã€‚ç„¶åVLAFlowMatchingçš„åˆå§‹åŒ–é‡Œé¢æœ‰self.state_proj
= nn.Linear( self.config.max_state_dim,
self.vlm_with_expert.config.text_config.hidden_size )ã€‚</li>
<li>æ‰€ä»¥instanceçš„ç»“æœé‡Œé¢åªæœ‰ä¸¤ä¸ªåŒ…å«stateçš„å†…å®¹
<ol type="1">
<li>SmolVLAPolicy( (normalize_inputs): Normalize(
(buffer_observation_state): ParameterDict( (mean): Parameter containing:
[torch.FloatTensor of size 6] (std): Parameter containing:
[torch.FloatTensor of size 6] )
)è¿™ä¸ªæ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„å‰6ç»´çš„æƒé‡ï¼Œåé¢è¿˜éœ€è¦ç”¨</li>
<li>(state_proj): Linear(in_features=32, out_features=960,
bias=True)è¿™ä¸ªæ˜¯æŠ•å½±åˆ°32ç»´</li>
</ol></li>
</ol></li>
</ol></li>
<li>from_pretrainedé‡Œé¢policy = cls._load_as_safetensor(instance,
model_file, config.device, strict)
<ol type="1">
<li>clsè¿˜æ˜¯ SmolVLAPolicyï¼Œå°±æ˜¯è°ƒç”¨_load_as_safetensorè¿™ä¸ªclass
method</li>
<li>ç”¨çš„æ˜¯smolvlapolicyé‡Œé¢çš„_load_as_safetensor
<ol type="1">
<li>safetensors.torch.load_model(model, model_file, strict=strict,
device=map_location)æŠŠæƒé‡åŠ è½½åˆ°æ¨¡å‹å®ä¾‹é‡Œ</li>
<li>return load_smolvla( model, model_file, device=map_location,
checkpoint_keys_mapping="model._orig_mod.//model.", )è°ƒç”¨ç‰¹å®šçš„ loader
åšé¢å¤–å¤„ç†</li>
</ol></li>
</ol></li>
<li>load_smolvlaå‡½æ•°ï¼Œæˆ‘éœ€è¦åœ¨è¿™é‡Œç¡®ä¿stateçš„æƒé‡èƒ½1.å‰6ä½æ­£å¸¸åŠ ï¼Œ7-10ä½åˆå§‹åŒ–ä¸€ä¸‹
<ol type="1">
<li>æœ‰æ²¡æœ‰åŠæ³•è¯†åˆ«input
featureæ­¤æ—¶æ˜¯6è¿˜æ˜¯10ï¼Ÿç›®å‰è¿™ä¸ªæ¨¡å‹ä¼ åˆ°è¿™é‡Œåªå‰©ä¸‹clsä¸¤ä¸ªå…³äºstateçš„éƒ¨åˆ†äº†ï¼Œæ‰€ä»¥ä»å¤–éƒ¨å¼•å…¥</li>
<li>ç»™7-10ç»´é‡ç½®ï¼Œå˜æˆç”¨ Xavier å‡åŒ€åˆå§‹åŒ–è¦†ç›–,bias æ¸…é›¶</li>
</ol></li>
</ol>
<h3 id="train-1">train</h3>
<ol type="1">
<li>policy.train()è¿è¡Œçš„æ˜¯SmolVLMWithExpertModelçš„trainï¼Œè®¾ç½®å†»ç»“å“ªä¸€å—</li>
<li>å¼€å§‹æŒ‰æ­¥æ•°è®­ç»ƒ train_tracker, output_dict =
update_policy(policy)</li>
<li>update_policyå‡½æ•°
<ol type="1">
<li>è®­ç»ƒé€»è¾‘ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss, output_dict = policy.forward(batch)</span><br><span class="line">grad_scaler.scale(loss).backward()</span><br><span class="line">grad_scaler.unscale_(optimizer)</span><br><span class="line">torch.nn.utils.clip_grad_norm_(policy.parameters(), ...)</span><br><span class="line">grad_scaler.step(optimizer)</span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
é—®é¢˜å¯èƒ½å‘ç”Ÿåœ¨ï¼Œstateæ›´æ–°å‚æ•°çš„æ—¶å€™7-10ç»´å˜åŒ–æ¯”è¾ƒå¤§</li>
</ol></li>
<li>normalizeå½’ä¸€åŒ–æ²¡æœ‰åŒ¹é…ç»´åº¦
<ol type="1">
<li>modeling_smolvlaé‡Œé¢SmolVLAPolicyçš„initæœ‰self.normalize_inputs =
Normalize(config.input_features, config.normalization_mapping,
dataset_stats)</li>
<li>stateå‰6ç»´çš„meanï¼Œstdæ˜¯ä»dataset_statsé‡Œæ¥çš„ï¼Œdataset_statsæ˜¯ä»SmolVLAPolicyçš„initä¼ å…¥çš„</li>
<li>ä¹‹å‰make_policyä¼ å…¥çš„kwargs["dataset_stats"] =
meta_for_policy.statsï¼Œç›¸å½“äºds_meta.stats</li>
<li>ds_metaæ˜¯ds_meta=dataset.metaï¼Œæ‰€ä»¥è¿˜æ˜¯æœ¬åœ°ä¼ å…¥çš„ï¼Œçœ‹åˆ°episode_stats.jsonlæ–‡ä»¶é‡Œé¢æœ‰ç±»ä¼¼minï¼Œmaxï¼Œstdçš„ï¼Œget
<ol type="1">
<li>æ€ä¹ˆå†™å…¥save_episodeæœ‰ä¸€ä¸ªwrite_episode_statså‡½æ•°</li>
<li>æ€ä¹ˆè¯» ç›´æ¥ä»metaé‡Œé¢è¯»çš„ï¼Œè¿™ä¸ªæˆ‘å¥½åƒæ”¹ä¸äº†</li>
<li>æ€ä¹ˆè®¡ç®—ï¼šep_stats = compute_episode_stats(episode_buffer,
self.features)</li>
<li>å½¢å¼ï¼š "observation.state": {"min": [-47.14912414550781,
-96.49805450439453, 9.134847640991211, -1.6986300945281982,
2.4175825119018555, 0.6323396563529968], "max": [8.552631378173828,
44.06614685058594, 99.51667785644531, 42.136985778808594,
52.91819381713867, 56.36856460571289], "mean": [-13.58438491821289,
-31.481496810913086, 56.781070709228516, 22.7191219329834,
25.275249481201172, 15.44166374206543], "std": [19.134763717651367,
52.86572265625, 35.76657485961914, 15.830253601074219,
18.950584411621094, 18.50875473022461], "count":
[181]},ä¹Ÿå°±æ˜¯ä¸€ä¸ªepisodeé‡Œé¢æ‰€æœ‰stateæ¯ä¸ªç»´åº¦çš„ç»Ÿè®¡ã€‚</li>
<li>ç„¶åå˜æˆå…¨å±€çš„ï¼šaggregate_feature_stats</li>
<li>ä¿®æ”¹è¿‡ç¨‹ï¼Œä½¿å¾—å¯ä»¥åœ¨ç®—å‡ºæ¥
<ol type="1">
<li></li>
<li></li>
<li></li>
<li></li>
<li>åœ¨æ­£å¼normalizeä¹‹å‰ï¼Œéœ€è¦ç¡®ä¿èƒ½ä¼ å…¥datset["observation.state"]ä»¥è®¡ç®—</li>
<li>batch =
self.normalize_inputs(batch)è¿™é‡Œç”¨çš„æ˜¯Normalizeçš„forwardï¼Œæ­£å¥½ä¼ å…¥äº†batch
<ol type="1">
<li>ä¸€ä¸ªbatché‡Œé¢æœ‰ä»€ä¹ˆï¼Ÿè‹¥å¹²ä¸ªepsiodeï¼Ÿ</li>
<li>æ€ä¹ˆnormalizeçš„forwardé‡Œé¢çš„meanå’Œstdåˆå˜æˆå…¨å±€çš„äº†ï¼Œå…¨å±€è¿˜ä¸å¥½ç®—ï¼Ÿ</li>
<li>normalizeç±»ç”¨create_stats_buffersè¿™ä¸ªå‡½æ•°æŠŠæ‰€æœ‰episodeçš„meanå’Œstdç®—åˆ°ä¸€ä¸ªå…¨å±€çš„é‡Œé¢</li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol>
<h3 id="vlaflowmatching">VLAFlowMatching</h3>
<h4 id="init">init</h4>
<ol type="1">
<li><p>self.vlm_with_expert =
SmolVLMWithExpertModel(...)æ„å»ºå¸¦ä¸“å®¶å¤´çš„å¤šæ¨¡æ€ä¸»å¹²ï¼Œè¿™ä¸ªå¯¹è±¡é‡ŒåŒ…å«ï¼š</p>
<p>è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ SigLIPï¼‰+ è¯­è¨€åµŒå…¥å±‚ï¼›</p>
<p>ä¸€ä¸ª Transformerï¼ˆâ€œVLMâ€ä¸»ä½“ï¼‰ï¼›</p>
<p>åŠ¨ä½œâ€œä¸“å®¶ï¼ˆExpertï¼‰â€åˆ†æ”¯ä»¥åŠå¤„ç†å™¨ï¼ˆprocessor / tokenizerï¼‰ã€‚</p>
<p>æ­¤æ—¶çš„self.config.vlm_model_name="models/forsmolvla/HuggingFaceTB/SmolVLM2-500M-Video-Instruct"</p>
<p>æ‰€ä»¥vlm_with_expertåŠ è½½çš„å°±æ˜¯å†»ç»“çš„vlmï¼Ÿ</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">self.vlm_with_expert = SmolVLMWithExpertModel(</span><br><span class="line">            model_id=self.config.vlm_model_name,</span><br><span class="line">            freeze_vision_encoder=self.config.freeze_vision_encoder,</span><br><span class="line">            train_expert_only=self.config.train_expert_only,</span><br><span class="line">            load_vlm_weights=self.config.load_vlm_weights,</span><br><span class="line">            attention_mode=self.config.attention_mode,</span><br><span class="line">            num_expert_layers=self.config.num_expert_layers,</span><br><span class="line">            num_vlm_layers=self.config.num_vlm_layers,</span><br><span class="line">            self_attn_every_n_layers=self.config.self_attn_every_n_layers,</span><br><span class="line">            expert_width_multiplier=self.config.expert_width_multiplier,</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>
<p>è¿™ä¸ªä»£ç åˆå§‹åŒ–äº†ä¸€ä¸ªä»€ä¹ˆï¼Ÿ</p></li>
<li><p>self.state_projè¿™ä¸ªæ˜¯æŠŠåŸæ¥çš„32ç»´çš„stateæŠ•å½±åˆ°æ–‡æœ¬éšè—ç»´ï¼ˆtext_config.hidden_sizeï¼‰ï¼Œè¿™ä¸ªç»´åº¦æ˜¯ä»
self.vlm_with_experté‡Œé¢æ¥çš„</p></li>
<li><p>action_in_proj / action_out_projï¼šå°† åŠ¨ä½œç»´ æ˜ å°„åˆ°ä¸“å®¶éšè—ç»´
D_expï¼Œä¾¿äºé€å…¥ä¸“å®¶åˆ†æ”¯ï¼›å°†ä¸“å®¶è¾“å‡ºå†æŠ•å›
åŠ¨ä½œç»´ï¼Œå¾—åˆ°å¯¹ç›®æ ‡é€Ÿåº¦åœºçš„é¢„æµ‹</p></li>
<li><p>set_requires_gradï¼Œå¦‚æœconfig.train_state_projæ˜¯trueçš„è¯ï¼Œå°±ä¼šè®¾ç½®state_projé‡Œé¢çš„æ‰€æœ‰å‚æ•°require_gradä¸ºtrueï¼Œæ‰€ä»¥è¿™é‡Œåº”è¯¥æ˜¯true</p></li>
<li><p>ä¸€äº›å…³äºå›¾åƒçš„tokenï¼Œä¸å…³æ³¨äº†</p></li>
<li><p>ä¸€ä¸ªprefix_lengthï¼Œé»˜è®¤æ˜¯ prefix_length: int = -1</p></li>
</ol>
<h4 id="embed_prefix">embed_prefix</h4>
<ol type="1">
<li>è¾“å…¥state</li>
<li>ç›®çš„ï¼šæŠŠå›¾åƒã€ï¼ˆå¯é€‰çš„ï¼‰å›¾åƒç‰¹æ®Š tokenã€è¯­è¨€
tokenã€çŠ¶æ€æ‹¼æˆä¸€æ®µâ€œå‰ç¼€åºåˆ—â€ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„ pad mask ä¸
è·¨æ®µæ³¨æ„åŠ›å±è”½æ ‡è®°ã€‚</li>
<li>å›¾åƒåµŒå…¥</li>
<li>è¯­è¨€åµŒå…¥</li>
<li>çŠ¶æ€åµŒå…¥ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># çŠ¶æ€åµŒå…¥ï¼Œè¦æ”¹è‚¯å®šæ”¹è¿™é‡Œï¼Œå¦‚æœè¦æ”¹çš„è¯è¿˜è¦åŠ ä¸€ä¸ªï¼Œself.configé‡Œé¢æœ‰å—ï¼Ÿ</span></span><br><span class="line">        <span class="comment"># å¯ä»¥ç”¨self.add_location_to_stateï¼ˆæˆ–è€…æ”¹æˆtrain_add_location_to_stateï¼‰</span></span><br><span class="line">        <span class="comment"># æŠ•å½±åˆ°self.vlm_with_expert.config.text_config.hidden_sizeå¯¹åº”çš„ç»´æ•°ï¼ŒæŠŠçŠ¶æ€ç»´åº¦æŠ•å½±åˆ° å’Œè¯­è¨€/å›¾åƒ token ç›¸åŒçš„ hidden_size</span></span><br><span class="line">        state_emb = self.state_proj(state)</span><br><span class="line">        <span class="comment"># å¦‚æœåªæœ‰2ç»´ï¼Ÿå°±åŠ ä¸ªç»´ã€‚è¿™é‡Œä¹Ÿåº”è¯¥è‚¯å®šæ˜¯2dçš„å§</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;prefix_embedè¿™é‡Œæ˜¯1dçš„&quot;</span>,state_emb.ndim == <span class="number">2</span>)</span><br><span class="line">        state_emb = state_emb[:, <span class="literal">None</span>, :] <span class="keyword">if</span> state_emb.ndim == <span class="number">2</span> <span class="keyword">else</span> state_emb</span><br><span class="line">        <span class="comment"># è¿½åŠ åˆ°æ€»çš„embs,embsä»ï¼ˆimage_end_tokenï¼Œlang_embï¼‰å˜æˆï¼ˆimage_end_tokenï¼Œlang_embï¼Œstate_embï¼‰</span></span><br><span class="line">        embs.append(state_emb)</span><br><span class="line">        <span class="comment"># bsizeå°±æ˜¯batchsize</span></span><br><span class="line">        bsize = state_emb.shape[<span class="number">0</span>]</span><br><span class="line">        device = state_emb.device</span><br><span class="line">        <span class="comment"># states_seq_lenå°±æ˜¯çŠ¶æ€tokençš„ä¸ªæ•°</span></span><br><span class="line">        states_seq_len = state_emb.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># è¿™æ®µtokenæ²¡æœ‰paddingï¼Œå½¢çŠ¶æ˜¯((B, states_seq_len))</span></span><br><span class="line">        state_mask = torch.ones(bsize, states_seq_len, dtype=torch.<span class="built_in">bool</span>, device=device)</span><br><span class="line">        pad_masks.append(state_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set attention masks so that image and language inputs do not attend to state or actions</span></span><br><span class="line">        att_masks += [<span class="number">1</span>] * (states_seq_len)</span><br></pre></td></tr></table></figure> pad_maskå’Œatt_mask</li>
</ol>
<p>pad_masksï¼ˆpadding maskï¼‰</p>
<p>ä½œç”¨ï¼šæ ‡è®°å“ªäº› token æ˜¯â€œçœŸå®æœ‰æ•ˆçš„â€ï¼Œå“ªäº›æ˜¯ paddingï¼ˆè¡¥é½é•¿åº¦ï¼‰ã€‚</p>
<p>å–å€¼ï¼šå¸ƒå°”å€¼ True/1 = æœ‰æ•ˆï¼ŒFalse/0 = paddingã€‚</p>
<p>å½¢çŠ¶ï¼š(B, L_total)ï¼Œå¯¹åº”æ•´ä¸ªåºåˆ—æ¯ä¸ªä½ç½®ã€‚</p>
<p>att_masksï¼šattention åŸŸæ©ç </p>
<p>ä½œç”¨ï¼šå‘Šè¯‰ make_att_2d_masks ä¸åŒ token
å±äºå“ªä¸ªâ€œåŸŸï¼ˆdomainï¼‰â€ï¼Œä»è€Œæ§åˆ¶è°å¯ä»¥ attend è°ã€‚</p>
<p>è¿™ä¸ªå‡½æ•°æœ€åï¼Œpad_masksæ˜¯è¡¨ç¤ºå“ªä¸ªtokenæ˜¯æœ‰æ•ˆçš„ï¼ˆstateè¿™é‡Œåº”è¯¥éƒ½æ˜¯æœ‰æ•ˆï¼‰ï¼Œatt_masksçš„è¯ï¼Œå›¾å’Œtaskæ˜¯0ï¼Œstateæ˜¯1</p>
<h4 id="embed_suffix">embed_suffix</h4>
<p>æŠŠ åŠ¨ä½œåºåˆ— ä¸ æ—¶é—´ t èåˆï¼Œä½œä¸ºâ€œåç¼€åºåˆ—â€ï¼ŒåªåŒ…å«åŠ¨ä½œç«¯çš„
tokenã€‚è·Ÿstateå€’æ˜¯æ²¡ä»€ä¹ˆå…³ç³» att_masks ä¸ºé•¿åº¦ T_act çš„ 1 åºåˆ—ã€‚
é…åˆå‰ç¼€é‡Œçš„ 0/1ï¼Œé€šå¸¸è¡¨ç¤ºâ€œåç¼€çš„åŠ¨ä½œ token
å±äºå¦ä¸€ä¸ªæ³¨æ„åŠ›åŸŸâ€ï¼Œä»è€Œé¿å…å›¾åƒ/æ–‡æœ¬å»çœ‹åŠ¨ä½œï¼›ä¹Ÿå¯å…è®¸çŠ¶æ€/åŠ¨ä½œä¹‹é—´çš„ç›¸äº’å¯è§æ€§ï¼ˆå…·ä½“ç”±
make_att_2d_masks å†³å®šï¼‰ã€‚</p>
<p>ä¸ºä»€ä¹ˆactionä¹Ÿè¦æ˜¯1ï¼Ÿå¤§æ¦‚æ‡‚äº†ç‚¹ï¼Œè¦åˆ†ä¸ºæ¡ä»¶è¾“å…¥å’Œå†³ç­–è¾“å…¥ï¼Œemmä¸»è¦æ˜¯æˆ‘ä¸çŸ¥é“è¿˜èƒ½æ”¾åœ¨å“ªå„¿ï¼Œç†è®ºä¸Šæ¥è¯´è¿™ä¸ªä½ç½®ä¿¡æ¯è‚¯å®šæ˜¯æ”¾åœ¨0æ¯”è¾ƒå¥½çš„</p>
<h4 id="forward">forward</h4>
<p>ç›®æ ‡ï¼šå­¦ä¹ ä¸€ä¸ª é€Ÿåº¦åœº v_Î¸(x_t, t)ï¼Œå»é€¼è¿‘çœŸé€Ÿåº¦ u_t = Îµ - x_0
çš„ç­‰ä»·å½¢å¼ï¼ˆæ­¤å®ç°é‡Œæ˜¯ Îµ - actionsï¼‰ï¼Œå±äº flow
matching/å™ªå£°é©±åŠ¨çš„åŠ¨ä½œå»ºæ¨¡æ€æƒ³ã€‚</p>
<h3 id="smolvlmwithexpertmodel">SmolVLMWithExpertModel</h3>
<h4 id="init-1">init</h4>
<ol type="1">
<li>self.vlmå’Œself.procesorçš„åŒºåˆ«
<ol type="1">
<li>self.vlmæ˜¯AutoModelForImageTextToText.from_pretrained(
model_id)ï¼Œgä¼šåŠ è½½ä¸€ä¸ª è§†è§‰-è¯­è¨€-æ–‡æœ¬ï¼ˆVLMï¼‰å¤§æ¨¡å‹</li>
<li>self.processor =
AutoProcessor.from_pretrained(model_id)çœ‹èµ·æ¥éƒ½æ˜¯from_pretrained(model_id)</li>
<li>self.vlmæ˜¯æ¨¡å‹æƒé‡ + å‰å‘æ¨ç†é€»è¾‘ï¼Œç±»ä¼¼ transformers é‡Œå¸¸è§çš„
AutoModelForCausalLMã€‚è¿™é‡Œæ˜¯ Image+Text â†’ Text
çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé‡Œé¢åº”è¯¥åŒ…å«
<ol type="1">
<li>model
<ol type="1">
<li>æ˜¯æ•´ä¸ª VLM çš„ ä¸»ä½“ backboneï¼Œç›´æ¥è°ƒç”¨ self.vlm.model(...) ä¼šè‡ªåŠ¨æ‰§è¡Œ
vision_model + connectorï¼Œç»™ä½ ä¸€ä¸ªå¯¹é½åˆ° text hidden_size çš„
embedding</li>
<li>.vision_modelè§†è§‰ç¼–ç å™¨ï¼ˆæŠŠå›¾ç‰‡è½¬æˆ
embeddingï¼‰çº¯ç²¹çš„encoderï¼Œåªè´Ÿè´£ã€ŒæŠŠå›¾åƒè½¬æˆ patch embeddingã€
<ol type="1">
<li>å’ŒVLAFlowMatchingé‡Œé¢çš„prefix_embedçš„åŒºåˆ«æ˜¯ï¼šembed_prefix
ä¸è‡ªå·±åšå›¾åƒç¼–ç ï¼Œå®ƒåªæ˜¯ä¸€ä¸ªã€Œæ‹¼æ¥å™¨ã€</li>
<li>img_emb =
self.vlm_with_expert.embed_image(img)è¿™é‡Œè°ƒç”¨çš„å°±æ˜¯self.vlm_with_experts.vlm.model</li>
</ol></li>
<li>.text_modelï¼šæ–‡æœ¬ç¼–ç å™¨/è§£ç å™¨
<ol type="1">
<li>åŒç†ï¼Œembed_prefixé‡Œé¢å¯¹stateå’Œlanguageçš„å¤„ç†ä¹Ÿç±»ä¼¼ï¼Œå…³æ³¨stateï¼š</li>
</ol></li>
<li>.connectorï¼šæ¨¡æ€å¯¹é½å±‚ï¼ˆæŠŠ vision hidden state æ˜ å°„åˆ° text hidden
spaceï¼‰</li>
</ol></li>
<li>forward</li>
<li>generate</li>
</ol></li>
<li>self.processoræ˜¯è¾“å…¥é¢„å¤„ç†å™¨ï¼Œè´Ÿè´£æŠŠåŸå§‹æ•°æ®ï¼ˆå›¾åƒã€æ–‡å­—å­—ç¬¦ä¸²ï¼‰è½¬æˆ
self.vlm èƒ½æ¥å—çš„å¼ é‡
<ol type="1">
<li>å¯¹å›¾åƒï¼šresizeã€å½’ä¸€åŒ–ã€è½¬ tensor</li>
<li>å¯¹æ–‡æœ¬ï¼štokenize â†’ token_id â†’ attention_mask</li>
</ol></li>
</ol></li>
<li>self.vlm.model.text_model.layerså‡å°‘åˆ°å‰num_vlm_layersï¼ˆ16ï¼‰å±‚</li>
<li>æ„å»ºæ›´çª„çš„ expert éƒ¨åˆ†lm_expert
<ol type="1">
<li>lm_expert_config =
copy.deepcopy(config.text_config)ä¸ºä»€ä¹ˆåªç”¨text_configï¼Ÿå› ä¸ºä¸“å®¶æ¨¡å‹åªç®¡
text éƒ¨åˆ†ï¼ˆä¸ç®¡
visionï¼‰ï¼Œvisionä¹Ÿè¢«æ‹¼æ¥åˆ°texté‡Œé¢äº†ã€‚å…ˆå¾—åˆ°ä¸€ä¸ªå¤§æ¦‚çš„configç»“æ„ï¼Œå’Œself.vlm.config.text_configæ˜¯ä¸€æ ·çš„å¤§å°ã€‚</li>
<li>æ›´çª„
<ol type="1">
<li>åŸæ¥mlpé‡Œé¢çš„ç»“æ„æ˜¯è¿™æ ·çš„ï¼šhidden_size â†’ intermediate_size â†’
hidden_sizeï¼Œä¹Ÿå°±æ˜¯å…ˆæŠŠå…ˆæŠŠ hidden
å‘é‡æŠ•å½±åˆ°ä¸€ä¸ªæ›´å®½çš„ç©ºé—´ï¼ˆintermediate_sizeï¼‰ï¼Œå†æŠ•å½±å›
hidden_sizeï¼Œè¿™æ ·æ¨¡å‹æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>hudden_size ï¼ˆæ¯ä¸ª token embedding çš„ç»´åº¦ï¼ŒTransformer
å±‚çš„è¾“å…¥/è¾“å‡ºç»´åº¦ï¼‰</li>
<li>intermediate_size FFN é‡Œé—´å±‚å®½åº¦ï¼ŒæŒºå¤æ‚çš„ä¸ç”¨ç®¡è¿™ä¸ªç»éªŒå…¬å¼</li>
<li>num_hidden_layers å †å¤šå°‘å±‚ Transformerï¼Œå› ä¸º Expert è¦â€œå¯¹åº”â€VLM
çš„å±‚ç»“æ„ï¼Œæ–¹ä¾¿åšè·¨æ³¨æ„åŠ›å¯¹é½ã€‚è¿™æ ·ä¸€å±‚ Expert å¯¹åº”ä¸€å±‚ VLMï¼Œæ›´å®¹æ˜“è®¾è®¡
cross-attnã€‚å¦‚æœé¢å¤–è®¾ç½®äº†ä¸€ä¸ªnum_expert_layersï¼Œvlmè·Ÿexpertä¸æ˜¯ä¸€å±‚å±‚å¯¹åº”ï¼Œä½†æ˜¯expert_layerè¦æ˜¯16çš„å› æ•°ï¼Œä¸ºäº†æ–¹ä¾¿ç¨€ç–äº¤äº’</li>
</ol></li>
<li>self.lm_expert = AutoModel.from_config(lm_expert_config)ç»“æ„å’Œ
VLM.text_model ä¸€æ ·ï¼Œä½†æ›´å°ã€æ›´çª„</li>
<li>å¤§å°ç¡®å®šäº†ï¼Œå€¼æ˜¯éœ€è¦è‡ªå·±è®­ç»ƒçš„ï¼Œä¹Ÿå°±æ˜¯expertéœ€è¦å•ç‹¬è®­ç»ƒã€‚</li>
</ol></li>
<li>atten_mode
<ol type="1">
<li>å› ä¸º cross attention æ¨¡å¼ä¸‹ï¼ŒExpert ä¸ç›´æ¥ç”¨è‡ªå·±çš„ K/Vï¼Œè€Œæ˜¯ç”¨å¤§ VLM
çš„ K/V è¡¨ç¤ºï¼Œæ”¹å€¼ï¼Œæ‰€ä»¥éœ€è¦ç»´åº¦ä¹ŸåŒ¹é…ä¸€ä¸‹ï¼Œä¸ç„¶vlmçš„å€¼æ‹¿ä¸è¿‡æ¥ï¼šå½“è¯¥å±‚åš
cross attention æ—¶ â†’ Expert çš„è¾“å…¥æ˜¯ VLM ä¼ ä¸‹æ¥çš„ç‰¹å¾ï¼ˆæ¯”å¦‚
hidden_size=1024ï¼‰ï¼Œå¿…é¡»ç»è¿‡è¿™ä¸ªæ›¿æ¢è¿‡çš„ Linearï¼ŒæŠ•å½±åˆ° Expert
è‡ªå·±çš„å®½åº¦ï¼ˆæ¯”å¦‚ hidden_size=512ï¼‰ã€‚ã€‚vlmä¸æ˜¯å†»ç»“çš„å—ï¼Œä¹Ÿæœ‰kvï¼Ÿ</li>
<li>ä¸æ˜¯æ¯ä¸€å±‚ Expert éƒ½ç”¨ cross attentionï¼Œæ¯ xx å±‚ä¿ç•™ä¸€æ¬¡
çº¯è‡ªæ³¨æ„åŠ›ï¼Œå…¶ä»–å±‚å°±æŠŠ K/V æ¢æˆæ¥è‡ª VLM çš„ï¼ˆæŠ•å½±è¿‡çš„ï¼‰K/Vï¼Œä¹Ÿå°±æ˜¯cross
attentionå’Œself-attention</li>
</ol></li>
<li>ä¸“å®¶ä¸è‡ªå·±è´Ÿè´£è¯åµŒå…¥ï¼Œç»Ÿä¸€ç”¨ VLM çš„è¯åµŒå…¥æˆ–ä¸Šæ¸¸ä¼ å…¥çš„
inputs_embedsã€‚self.lm_expert.embed_tokens = None</li>
<li>set_requires_grad
<ol type="1">
<li>å¦‚æœself.freeze_vision_encoderè®¾ç½®ä¸ºtrueï¼šæ•´ä¸ªvlmå†»ç»“ï¼Œexpertå¯ä»¥è®­ç»ƒï¼Œåªæ›´æ–°expertçš„æƒé‡</li>
<li>train_expert_only=True</li>
</ol></li>
<li>self.vlm,self.vlm.model,self.vlm.model.vision_encoder</li>
</ol>
<h4 id="forward-1">forward</h4>
<ol type="1">
<li>batch_size</li>
<li>å¯¹æ¯ä¸€å±‚è¦ä¹ˆself-attenè¦ä¹ˆcross-attenï¼Œè®°å½•ç»“æœatt_outputs,
past_key_values</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">att_outputs,past_key_values=self_or_cross-attention(</span><br><span class="line">   model_layers,     <span class="comment"># å½“å‰å±‚çš„å­æ¨¡å‹å±‚ (vlm å±‚ + expert å±‚) æ˜¯å¯è®­ç»ƒçš„éƒ¨åˆ†ï¼ŒåŒ…å«å…¨éƒ¨éœ€è¦æ›´æ–°çš„æƒé‡ã€‚</span></span><br><span class="line">inputs_embeds,    <span class="comment"># è¯¥å±‚çš„è¾“å…¥ hidden states [vlm_embeds, expert_embeds]</span></span><br><span class="line">layer_idx,        <span class="comment"># å½“å‰å±‚ç¼–å·</span></span><br><span class="line">position_ids,     <span class="comment"># æ¯ä¸ª token çš„ä½ç½® id (padding é€šå¸¸æ˜¯ 0 æˆ–ç‰¹æ®Šå¤„ç†)</span></span><br><span class="line">attention_mask,   <span class="comment"># æ³¨æ„åŠ› maskï¼Œæ§åˆ¶å“ªäº› token èƒ½è¢«çœ‹åˆ°</span></span><br><span class="line">batch_size,       <span class="comment"># æ‰¹å¤§å° (Bï¼‰</span></span><br><span class="line">head_dim,         <span class="comment"># æ¯ä¸ª head çš„ç»´åº¦ (D)</span></span><br><span class="line">use_cache,        <span class="comment"># æ˜¯å¦ä½¿ç”¨ KV cache (æ¨ç†æ—¶åŠ é€Ÿ)</span></span><br><span class="line">fill_kv_cache,    <span class="comment"># æ˜¯å¦å¾€ cache é‡Œå¡«æ–°çš„ K/V (True=è®­ç»ƒ; False=æ¨ç†å¤ç”¨æ—§KV)</span></span><br><span class="line">past_key_values   <span class="comment"># å­˜å‚¨å†å² KV çš„å­—å…¸</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>att_outputs å’Œ past_key_values åœ¨æ•´ä¸ªæ¨¡å‹ä¸­çš„ä½ç½®</p>
<p>è¿™ä¸¤ä¸ªæ˜¯ ä¸­é—´ç»“æœï¼Œåœ¨æ¨¡å‹ forward çš„æµæ°´çº¿ä¸Šæ‰®æ¼”ä¸åŒè§’è‰²ï¼š</p>
<p>att_outputs</p>
<p>è¿™æ˜¯è¿™ä¸€å±‚ï¼ˆvlm + expertï¼‰çš„ attention è¾“å‡º hidden statesã€‚</p>
<p>ä¼šä½œä¸º ä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œä¸€è·¯ä¼ åˆ°æœ€åä¸€å±‚ â†’ æ¥ decoder head / classifier
headã€‚</p>
<p>æ‰€ä»¥å®ƒå¤„äº ã€Œä¸»å¹²è®¡ç®—å›¾ã€ ä¸­ï¼Œæ¢¯åº¦ä¼šå¾€å›ä¼ ã€‚</p>
<p>ç›¸å½“äº transformer å±‚çš„ hidden_statesã€‚</p>
<p>past_key_values</p>
<p>ä¿å­˜æ¯ä¸€å±‚çš„ KV (key, value) å¼ é‡ã€‚</p>
<p>åœ¨è®­ç»ƒæ—¶ å¯ä»¥ä¸ç”¨ï¼ˆå› ä¸ºæˆ‘ä»¬æ¯æ¬¡éƒ½å…¨é‡è®¡ç®—ï¼‰ï¼Œ</p>
<p>åœ¨æ¨ç†æ—¶ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰ï¼Œç”¨å®ƒæ¥é¿å…é‡å¤è®¡ç®—æ—§ token çš„ KVã€‚</p>
<p>å®ƒå¤„äº ã€Œç¼“å­˜/ä¼˜åŒ–åˆ†æ”¯ã€ï¼Œé€šå¸¸ä¸ä¼šå‚ä¸æ¢¯åº¦è®¡ç®—ã€‚</p>
<p>åªå­˜å‚¨ forward çš„ä¸­é—´ç»“æœï¼Œä¸ä¼šå½±å“è®­ç»ƒæ›´æ–°</p>
<h3 id="è®­ç»ƒæµç¨‹">è®­ç»ƒæµç¨‹</h3>
<ol type="1">
<li>train
<ol type="1">
<li></li>
</ol></li>
<li>smolvlapolicy
<ol type="1">
<li>init
dataset_statså’Œconfig.input_featuresé‡Œé¢çš„stateç»´åº¦è¦å¯¹åº”ï¼Œæ”¹ä¸€ä¸‹dataset_stats
<ol type="1">
<li>policy =
make_policy(ds_meta)è¿™ä¸ªds_metaå°±æ˜¯dataset_statså—ï¼ŸåŠ ä¸Š</li>
</ol></li>
<li>forwardæ­¤æ—¶ä¼ å…¥çš„batchçš„stateæ˜¯10ç»´</li>
</ol></li>
<li>vlaflowmatching
<ol type="1">
<li>self.vlm_with_expert = SmolVLMWithExpertModel</li>
<li>self.state_proj = nn.Linear( self.config.max_state_dim,
self.vlm_with_expert.config.text_config.hidden_size )</li>
<li>(<em>, suffix_out), </em> = self.vlm_with_expert.forward(
attention_mask=att_2d_masks, position_ids=position_ids,
past_key_values=None, inputs_embeds=[prefix_embs, suffix_embs],
use_cache=False, fill_kv_cache=False, )</li>
</ol></li>
<li>smolvlmwithexpert
<ol type="1">
<li>init</li>
<li>forwardï¼Œä¼ å…¥çš„
<ol type="1">
<li>inputs_embedsï¼šå¤–éƒ¨å‡†å¤‡å¥½ä¸€ä¸ª [batch, seq_len, hidden_dim]
çš„å¼ é‡ï¼Œå®ƒéƒ½ä¼šè¿›å…¥ forward æµç¨‹</li>
<li>position_idsï¼šè¿™åªå½±å“ RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰ï¼Œå®ƒå†³å®šæ¯ä¸ª token
åœ¨åºåˆ—é‡Œçš„ä½ç½®ã€‚å’Œä½ è¦è®­ç»ƒçš„ state
çš„å€¼åŸŸ/ç»´åº¦æ²¡æœ‰ç›´æ¥å…³ç³»ï¼Œåªæ˜¯å‘Šè¯‰æ³¨æ„åŠ›è®¡ç®—â€œç¬¬ i ä¸ª token
çš„ä½ç½®æ˜¯å¤šå°‘â€ã€‚</li>
</ol></li>
</ol></li>
</ol>
<h1 id="dataloader">dataloader</h1>
<p>ä¹‹å‰å†™åœ¨train.pyé‡Œé¢çš„ï¼Œfor key in batch:ä¹‹åï¼Œä¸ºäº†ä¿å­˜æ•°æ®åˆ°æœ¬åœ°
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">states = batch[<span class="string">&quot;observation.state&quot;</span>].detach().cpu()</span><br><span class="line"><span class="keyword">if</span> states.dim() == <span class="number">3</span>:   <span class="comment"># e.g. [B, 1, 10] â†’ squeezeæ‰</span></span><br><span class="line">    states = states.squeeze(<span class="number">1</span>)</span><br><span class="line">episode_indices = batch[<span class="string">&quot;episode_index&quot;</span>].detach().cpu().tolist()</span><br><span class="line">frame_indices = batch[<span class="string">&quot;frame_index&quot;</span>].detach().cpu().tolist()</span><br><span class="line">states_list = states.numpy().tolist()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;modified_states.jsonl&quot;</span>, <span class="string">&quot;a&quot;</span>) <span class="keyword">as</span> f:  <span class="comment"># è¿½åŠ å†™å…¥</span></span><br><span class="line">    <span class="keyword">for</span> ep, fr, st <span class="keyword">in</span> <span class="built_in">zip</span>(episode_indices, frame_indices, states_list):</span><br><span class="line">        record = &#123;</span><br><span class="line">            <span class="string">&quot;episode_index&quot;</span>: <span class="built_in">int</span>(ep),</span><br><span class="line">            <span class="string">&quot;frame_index&quot;</span>: <span class="built_in">int</span>(fr),</span><br><span class="line">            <span class="string">&quot;state&quot;</span>: st</span><br><span class="line">        &#125;</span><br><span class="line">        f.write(json.dumps(record, ensure_ascii=<span class="literal">False</span>) + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="evaluate">evaluate</h1>
<ol type="1">
<li>è¦æ–°å¢çš„åŠŸèƒ½
<ol type="1">
<li>æœåŠ¡å™¨ç«¯æ¨ç†æ¬¡æ•°ç»Ÿè®¡ï¼šåªè¦åœ¨policy_serverç«¯å¢åŠ logå°±è¡Œï¼Œinité‡Œé¢æä¸ªè®¡æ•°å™¨</li>
<li>èƒ½æµ‹è¯•baselineï¼Œä¿®æ”¹taskå’Œä¿®æ”¹state
<ol type="1">
<li>taskå„ç§å½¢å¼</li>
<li>stateè¦projå’Œnormalize</li>
</ol></li>
</ol></li>
</ol>
<h2 id="config">config</h2>
<p>åŸºæœ¬ä¸Šå¤„ç†éƒ½å¯ä»¥ä»pretrained_name_or_pathé‡Œé¢çœ‹å‡ºæ¥å§ï¼Œåªè¦åœ¨é…ç½®çš„yamlæ–‡ä»¶é‡Œé¢å†™æ¸…æ¥šmodelçš„pathå°±è¡Œï¼Œç„¶åæœåŠ¡å™¨æ ¹æ®åå­—é€‰æ‹©å‡º
mtask,mstateå’Œbaseline</p>
<ol type="1">
<li>baseline</li>
<li>mtask_
<ol type="1">
<li>relative</li>
<li>grid_2cm</li>
<li>grid_5cm</li>
</ol></li>
<li>mstate_
<ol type="1">
<li>relative</li>
<li>1m</li>
</ol></li>
</ol>
<h3 id="policy_server">policy_server</h3>
<ol type="1">
<li>æ ¹æ®configå¾—åˆ°æ§åˆ¶</li>
<li>æ¨ç†ä¹‹å‰_predict_action_chunké‡Œé¢å¤„ç†stateå’Œtask</li>
<li>æ¨ç†stateçš„æ—¶å€™æ˜¯å¦è¿˜éœ€è¦ä¿®æ”¹normalizeï¼Ÿ
<ol type="1">
<li>prepare_batchå‡½æ•°é‡Œé¢çœ‹åˆ°äº†ï¼Œæ¥æ”¹å§</li>
<li>ä¹‹å‰forwardå‡½æ•°é‡Œé¢æœ‰batch =
self.normalize_inputs(batch,adding_state_stat)</li>
<li>éœ€è¦ç¡®ä¿self.add_location_to_stateæœ‰å¯¹åº”çš„å€¼
<ol type="1">
<li>self.add_location_to_state=config.add_location_to_state</li>
<li>cfgæ˜¯SmolVLAConfigï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹åˆå§‹åŒ–çš„æ—¶å€™è¦æœ‰add_location_to_stateè¿™ä¸ªè®¾ç½®</li>
</ol></li>
<li>self.add_state_dimåŠ è½½æ­£ç¡®</li>
</ol></li>
<li>æ¨ç†çš„æ—¶å€™å»æ‰side_depthè¿™ä¸ªå¤šä½™çš„feature</li>
</ol>
<h3 id="robot_clinet">robot_clinet</h3>
<ol type="1">
<li>æ³¨é‡Šæ‰validate_robot_cameras_for_policy(lerobot_features,
policy_image_features)ï¼Œå› ä¸ºæœ¬åœ°ä¸ç”¨è¿™ä¸ª</li>
</ol>
<p>8.29é‡æ–°æ•´ç†ä»£ç ï¼Œä»Šæ™šæ”¶é›†ç¬¬äºŒä¸ªç‰©ä½“+æ•´ç†ä»£ç </p>
<p>git remote add upstream https://github.com/huggingface/lerobot.git
ä¹‹å‰æ›´æ–°çš„æ—¶å€™ä¸å°å¿ƒä¸¢æ‰äº†</p>
<ol type="1">
<li>camera:realsense cameraæ›´æ”¹æ¯”è¾ƒå¤šï¼Œä¸»è¦æ˜¯æ–°å¢ä¿å­˜åŒé€šé“çš„æ·±åº¦å›¾</li>
<li>scripts/train.py</li>
<li>configs/train.py å¢åŠ äº†3ä¸ªå‘½ä»¤è¡Œå‚æ•° # è‡ªå®šä¹‰ #
å½“ç©ºçš„æ—¶å€™å°±ç­‰äºbaseline<br />
language_tip_mode: str = "" # æ”¹æˆæ¨¡å¼ï¼Œæœ‰ pureå’Œgrid #
æ§åˆ¶æ˜¯å°†locationåŠ åˆ°stateé‡Œé¢ add_location_to_state: str = ""
freeze_except_7_10: bool= False</li>
</ol>
<p>factory.py é‡Œé¢make_dataset</p>
<p>å¯¹smovlaæ•´ä½“çš„æ‹†è§£</p>
<p>smolvla_baseè¿™ä¸ªæ¨¡å‹é‡Œé¢éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># éªŒè¯æ–¹æ³•</span></span><br><span class="line"><span class="keyword">from</span> lerobot.policies.smolvla.modeling_smolvla <span class="keyword">import</span> SmolVLAPolicy</span><br><span class="line"></span><br><span class="line">policy = SmolVLAPolicy.from_pretrained(<span class="string">&quot;lerobot/smolvla_base&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. æŸ¥çœ‹æ‰€æœ‰å‚æ•°</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> policy.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;param.shape&#125;</span>, å¯è®­ç»ƒ: <span class="subst">&#123;param.requires_grad&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. æŒ‰æ¨¡å—ç»Ÿè®¡å‚æ•°é‡</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_parameters</span>(<span class="params">module</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> module.parameters())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;VLM+Expertæ€»å‚æ•°: <span class="subst">&#123;count_parameters(policy.model.vlm_with_expert):,&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;StateæŠ•å½±: <span class="subst">&#123;count_parameters(policy.model.state_proj):,&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Actionè¾“å…¥æŠ•å½±: <span class="subst">&#123;count_parameters(policy.model.action_in_proj):,&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Actionè¾“å‡ºæŠ•å½±: <span class="subst">&#123;count_parameters(policy.model.action_out_proj):,&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Actionæ—¶é—´MLP: <span class="subst">&#123;count_parameters(policy.model.action_time_mlp_in) + count_parameters(policy.model.action_time_mlp_out):,&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. æŸ¥çœ‹å“ªäº›å‚æ•°è¢«å†»ç»“</span></span><br><span class="line">vlm_params = count_parameters(policy.model.vlm_with_expert)</span><br><span class="line">trainable_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> policy.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\næ€»å‚æ•°: <span class="subst">&#123;count_parameters(policy):,&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;å¯è®­ç»ƒå‚æ•°: <span class="subst">&#123;trainable_params:,&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
æ‰§è¡Œè¿™ä¸ªä»£ç ä»¥åå¾—åˆ°çš„è¾“å‡ºæ˜¯ <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br></pre></td><td class="code"><pre><span class="line">Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...</span><br><span class="line">`torch_dtype` is deprecated! Use `dtype` instead!</span><br><span class="line">Reducing the number of VLM layers to 16 ...</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 16, 16]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding.weight: torch.Size([1024, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.post_layernorm.weight: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.vision_model.post_layernorm.bias: torch.Size([768]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.connector.modality_projection.proj.weight: torch.Size([960, 12288]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.embed_tokens.weight: torch.Size([49280, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj.weight: torch.Size([320, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj.weight: torch.Size([960, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj.weight: torch.Size([2560, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj.weight: torch.Size([960, 2560]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.model.text_model.norm.weight: torch.Size([960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.vlm.lm_head.weight: torch.Size([49280, 960]), å¯è®­ç»ƒ: False</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj.weight: torch.Size([320, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj.weight: torch.Size([960, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj.weight: torch.Size([320, 320]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj.weight: torch.Size([720, 960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.mlp.up_proj.weight: torch.Size([2048, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.mlp.down_proj.weight: torch.Size([720, 2048]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.input_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.vlm_with_expert.lm_expert.norm.weight: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.state_proj.weight: torch.Size([960, 32]), å¯è®­ç»ƒ: True</span><br><span class="line">model.state_proj.bias: torch.Size([960]), å¯è®­ç»ƒ: True</span><br><span class="line">model.action_in_proj.weight: torch.Size([720, 32]), å¯è®­ç»ƒ: True</span><br><span class="line">model.action_in_proj.bias: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.action_out_proj.weight: torch.Size([32, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.action_out_proj.bias: torch.Size([32]), å¯è®­ç»ƒ: True</span><br><span class="line">model.action_time_mlp_in.weight: torch.Size([720, 1440]), å¯è®­ç»ƒ: True</span><br><span class="line">model.action_time_mlp_in.bias: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.action_time_mlp_out.weight: torch.Size([720, 720]), å¯è®­ç»ƒ: True</span><br><span class="line">model.action_time_mlp_out.bias: torch.Size([720]), å¯è®­ç»ƒ: True</span><br><span class="line">VLM+Expertæ€»å‚æ•°: 448,411,024</span><br><span class="line">StateæŠ•å½±: 31,680</span><br><span class="line">Actionè¾“å…¥æŠ•å½±: 23,760</span><br><span class="line">Actionè¾“å‡ºæŠ•å½±: 23,072</span><br><span class="line">Actionæ—¶é—´MLP: 1,556,640</span><br><span class="line"></span><br><span class="line">æ€»å‚æ•°: 450,046,176</span><br><span class="line">å¯è®­ç»ƒå‚æ•°: 99,880,992</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>çœ‹åˆ°è¿™äº›æ‰“å°å‡ºæ¥çš„å‚æ•°åï¼Œå¯ä»¥åˆ†ç±»</p>
<p>SmolVLAPolicy â””â”€â”€ model â”œâ”€â”€ vlm_with_expert â”‚ â”œâ”€â”€ vlm â† å®Œæ•´
VLMï¼ˆå†»ç»“ï¼‰ â”‚ â”‚ â”œâ”€â”€ vision_model (ViT) â”‚ â”‚ â”œâ”€â”€ text_model (LLM) â”‚ â”‚ â””â”€â”€
connector (vision â†’ text) â”‚ â””â”€â”€ lm_expert â† å¯è®­ç»ƒçš„å°è¯­è¨€ä¸“å®¶ â”‚ â”œâ”€â”€
state_proj â† çŠ¶æ€è¾“å…¥æŠ•å½±ï¼ˆé€šå¸¸å¯è®­ç»ƒï¼‰ â”œâ”€â”€ action_in_proj â† action
token æŠ•å½± â”œâ”€â”€ action_out_proj â† action head â”œâ”€â”€ action_time_mlp_* â†
æ—¶é—´ / step embedding</p>
<p>ä¸»è¦çœ‹å¯ä»¥è®­ç»ƒçš„éƒ¨åˆ† 1. lm_expert - <strong>ä¹Ÿæ˜¯ 16 å±‚
Transformer</strong> - ä½† <strong>hidden size = 720ï¼ˆæ¯”ä¸» LLM çš„ 960
å°</strong> - æ˜¯ä¸€ä¸ª <strong>æ’åœ¨ VLM åé¢çš„â€œä¸“å®¶è¯­è¨€æ¨¡å‹â€</strong> -
lm_expert è´Ÿè´£â€œä¸ºåŠ¨ä½œè€Œæ€è€ƒâ€</p>
<p>ç»“åˆä»£ç ï¼Œsmolvlaçš„æ¡†æ¶æ˜¯ ### 1ï¸âƒ£ è¾“å…¥å¤„ç†</p>
<ul>
<li><p><strong>å›¾åƒ</strong> â†’ <code>prepare_images</code> â†’
<code>embed_prefix</code> â†’ VLM embedding
(<code>token_vlm</code>)</p></li>
<li><p><strong>çŠ¶æ€</strong> â†’ <code>prepare_state</code> â†’
<code>state_proj</code> â†’ <code>state_token</code></p></li>
<li><p><strong>åŠ¨ä½œ</strong> â†’ <code>prepare_action</code> â†’
<code>action_in_proj</code> â†’ <code>action_token</code></p></li>
</ul>
<blockquote>
<p>æ³¨æ„ï¼š<code>state_proj</code> å’Œ <code>action_in_proj</code>
éƒ½æ˜¯çº¿æ€§å±‚ï¼ŒæŠŠåŸå§‹çŠ¶æ€/åŠ¨ä½œæ˜ å°„åˆ° LM ä¸“å®¶è¾“å…¥ç»´åº¦ã€‚</p>
</blockquote>
<ul>
<li><strong>è¯­è¨€æŒ‡ä»¤</strong> â†’ <code>lang_tokens</code> â†’ VLM
embedding</li>
</ul>
<h3 id="lm-expert-è¾“å…¥">2ï¸âƒ£ LM Expert è¾“å…¥</h3>
<p>lm_expert_input = [token_vlm (VLM embedding), state_token,
action_token] æ³¨æ„åŠ› mask å’Œ ä½ç½®ç¼–ç ä¼šæ§åˆ¶å“ªäº› token å¯ä»¥äº’ç›¸
attendã€‚</p>
<p>LM expert çš„è¾“å‡ºé€šè¿‡ action_out_proj â†’ é¢„æµ‹åŠ¨ä½œ v_tã€‚</p>
<p>3ï¸âƒ£ Loss python å¤åˆ¶ä»£ç  loss = F.mse_loss(noisy_actions - actions,
v_t, reduction="none") åªæœ‰ LM
expertã€state_projã€action_in_projã€action_out_proj ä¼šæ›´æ–°</p>
<p>VLM frozen ä¸æ›´æ–°</p>
<p>åªæœ‰ LM expertã€state_projã€action_in_projã€action_out_proj
ä¼šæ›´æ–°</p>
<p>æˆ‘ç°åœ¨æ¸…æ¥šäº†ï¼Œé‡ç‚¹æ˜¯ä¿®æ”¹stateç›¸å…³çš„è¾“å…¥ï¼Œå› ä¸ºactionã€‚å†™æˆ‘çš„æ”»ç•¥</p>
<ol type="1">
<li>æ€æ ·ç¡®å®šä¸‹æ¥æˆ‘éœ€è¦æ”¹stateç›¸å…³çš„è¾“å…¥çš„ ä½ çš„é—®é¢˜èƒŒæ™¯ï¼š</li>
</ol>
<ul>
<li><p>å¾®è°ƒæ•°æ®é›†ï¼šä¸åŒç›¸æœºä½ç½®çš„å›¾åƒ +
æœ«ç«¯ä½å§¿åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„å€¼</p></li>
<li><p>LM expert é¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨çš„ state/action åˆ†å¸ƒ
<strong>ä¸å¾®è°ƒæ•°æ®å·®å¼‚å¤§</strong>ï¼ˆåŸæœ¬å¯èƒ½æ˜¯å…³èŠ‚è§’ç­‰ï¼‰</p></li>
<li><p>ç›®æ ‡ï¼šè®©å¾®è°ƒæ•°æ®èƒ½è¢«é¢„è®­ç»ƒæ¨¡å‹ç†è§£ï¼Œå¹¶ç”Ÿæˆå¯¹åº”åŠ¨ä½œ</p></li>
<li><p><strong>state åˆ†å¸ƒå·®å¼‚å¤§</strong></p>
<ul>
<li><p>å¾®è°ƒæ•°æ®çš„ <code>state</code> ç»è¿‡ FK +
ç›¸æœºå˜æ¢ï¼Œä¸åŸé¢„è®­ç»ƒæ¨¡å‹çš„ <code>state</code> åˆ†å¸ƒå®Œå…¨ä¸åŒ</p></li>
<li><p>LM expert å·²ç»ç†Ÿæ‚‰åŸå§‹ <code>state_proj</code>
è¾“å‡ºçš„åˆ†å¸ƒ</p></li>
<li><p>å°‘é‡å¾®è°ƒæ•°æ®æ— æ³•ç›´æ¥è®­ç»ƒ LM expert å­¦ä¹ æ–°åˆ†å¸ƒ</p></li>
</ul></li>
<li><p><strong>action ä¸éœ€è¦ä¿®æ”¹</strong></p>
<ul>
<li><p>ä½ å¾®è°ƒçš„ç›®æ ‡æ˜¯è¾“å‡ºåœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ action</p></li>
<li><p>LM expert å·²ç»ç”Ÿæˆåˆç†åŠ¨ä½œï¼Œåªéœ€è¦è¾“å…¥çš„ state
å¯¹åº”åŸç‰¹å¾ç©ºé—´å³å¯</p></li>
<li><p>å¦‚æœå¯¹ action æŠ•å½±ï¼Œä¼šæ”¹å˜ LM expert
çš„è¾“å‡ºç›®æ ‡ï¼Œå¾®è°ƒéš¾åº¦å¢åŠ </p></li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>ä»£ç æ€æ ·ä¿®æ”¹</li>
</ol>
<p>class VLAFlowMacthingé‡Œé¢å¢åŠ </p>
<ol type="1">
<li>åœ¨initä¸­æ–°å¢ <strong>state_adapter</strong>ï¼š</li>
</ol>
<p><code>self.state_adapter = StateAdapter(     input_dim=self.vlm_with_expert.config.text_config.hidden_size,     hidden_dim=self.vlm_with_expert.config.text_config.hidden_size * 2,     output_dim=self.vlm_with_expert.config.text_config.hidden_size )</code></p>
<ol start="2" type="1">
<li>åœ¨ <code>embed_prefix</code> ä¸­ä½¿ç”¨ï¼š</li>
</ol>
<p><code>state_emb = self.state_proj(state)          # åŸå§‹çº¿æ€§æŠ•å½± state_emb = self.state_adapter(state_emb)   # æ–°å¢ adapter state_emb = state_emb[:, None, :]</code></p>
<h2 id="state_adapter-è¾“å…¥è¾“å‡ºå’Œæ¨¡å‹ç»“æ„">3ï¸âƒ£ state_adapter
è¾“å…¥ã€è¾“å‡ºå’Œæ¨¡å‹ç»“æ„</h2>
<p><code>class StateAdapter(nn.Module):     def __init__(self, input_dim, hidden_dim, output_dim):         super().__init__()         self.mlp = nn.Sequential(             nn.Linear(input_dim, hidden_dim),             nn.SiLU(),             nn.Linear(hidden_dim, output_dim),         )      def forward(self, x):         return self.mlp(x)</code></p>
<ul>
<li><p><strong>è¾“å…¥ç»´åº¦</strong>ï¼š<code>state_proj</code>
è¾“å‡ºç»´åº¦</p></li>
<li><p><strong>éšè—å±‚ç»´åº¦</strong>ï¼š<code>state_proj</code> è¾“å‡ºç»´åº¦ *
2ï¼ˆç»éªŒå€¼ï¼Œå¯è°ƒï¼‰</p></li>
<li><p><strong>è¾“å‡ºç»´åº¦</strong>ï¼šä¸ LM expert hidden size ç›¸åŒ</p></li>
<li><p><strong>æ¿€æ´»å‡½æ•°</strong>ï¼šSiLUï¼ˆGELU/ReLU ä¹Ÿå¯ï¼‰</p></li>
</ul>
<h2 id="å¾®è°ƒæ–¹æ¡ˆ">4ï¸âƒ£ å¾®è°ƒæ–¹æ¡ˆ</h2>
<h3 id="å†»ç»“é¢„è®­ç»ƒ-lm-expert">4.1 å†»ç»“é¢„è®­ç»ƒ LM expert</h3>
<p><code>for param in model.vlm_with_expert.parameters():     param.requires_grad = False</code></p>
<ul>
<li><p>åªè®­ç»ƒ <code>state_adapter</code></p></li>
<li><p>å¦‚æœæ•°æ®é‡ç¨å¤§ï¼Œå¯ä»¥å¾®è°ƒ LM expert æœ€åå‡ å±‚</p></li>
</ul>
<h3 id="ä¼˜åŒ–å™¨">4.2 ä¼˜åŒ–å™¨</h3>
<p><code>optimizer = torch.optim.AdamW(model.state_adapter.parameters(), lr=1e-3)</code></p>
<ul>
<li><p>å°å­¦ä¹ ç‡å³å¯</p></li>
<li><p>å¯ä»¥æ·»åŠ  weight decay</p></li>
</ul>
<h3 id="è®­ç»ƒæµç¨‹-1">4.3 è®­ç»ƒæµç¨‹</h3>
<p><code>for batch in dataloader:     state, action = batch['state'], batch['action']     pred_action = model.embed_prefix(state, action)          loss = loss_fn(pred_action, action)     loss.backward()     optimizer.step()     optimizer.zero_grad()</code></p>
<ul>
<li><p>loss å¯ç”¨åŸ smolvla_base çš„ action loss</p></li>
<li><p>æ³¨æ„ state æ˜¯å¾®è°ƒæ•°æ®çš„ç›¸æœºåæ ‡ç³»çŠ¶æ€</p></li>
</ul>
<h3 id="è®­ç»ƒå»ºè®®">4.4 è®­ç»ƒå»ºè®®</h3>
<ul>
<li><p>æ•°æ®å°‘æ—¶ï¼š</p>
<ul>
<li><p>å†»ç»“ LM expert</p></li>
<li><p>ä»…è®­ç»ƒ adapter</p></li>
<li><p>å¯ä»¥ç”¨å°‘é‡ epoch æ”¶æ•›</p></li>
</ul></li>
<li><p>å¦‚æœæ•°æ®é‡ä¸­ç­‰ï¼š</p>
<ul>
<li><p>å†»ç»“å‰å‡ å±‚ LM expert</p></li>
<li><p>å¾®è°ƒæœ€åå‡ å±‚æˆ– cross attention</p></li>
</ul></li>
</ul>
<ol type="1">
<li><ul>
<li><p>å†»ç»“ï¼šVLM + LM expert + state_proj + action_proj</p></li>
<li><p>åªè®­ç»ƒï¼š<code>state_adapter</code></p></li>
<li><p>ç›®æ ‡ï¼š<strong>æŠŠæ–° state åˆ†å¸ƒå¯¹é½åˆ° LM expert
ç†Ÿæ‚‰çš„ç‰¹å¾ç©ºé—´</strong></p></li>
</ul></li>
<li><p><strong>é˜¶æ®µ 2ï¼ˆå¯é€‰ï¼‰</strong></p>
<ul>
<li><p>ç»§ç»­å†»ç»“ VLMï¼ˆvision + text backboneï¼‰</p></li>
<li><p>è§£å†»ï¼šLM expertï¼ˆæˆ–åªè§£å†»åå‡ å±‚ï¼‰+ state_proj +
action_proj</p></li>
<li><p>å°å­¦ä¹ ç‡è”åˆå¾®è°ƒ</p></li>
<li><p>ç›®æ ‡ï¼š<strong>è®© expert
çœŸæ­£é€‚é…â€œç›¸æœºåæ ‡ç³»åŠ¨ä½œè¯­ä¹‰â€</strong></p></li>
</ul></li>
</ol>
<p>è¿™æ˜¯ä¸€ä¸ª<strong>æ ‡å‡†çš„ Adapter â†’ Finetune çš„ curriculum
learning</strong>ã€‚</p>
<p>é¦–å…ˆå¤„ç†ä½å‚æ•°é‡çš„åˆ†å¸ƒå¯¹é½é—®é¢˜ ç„¶åå†é»˜è®¤çš„æ–¹å¼å¾®è°ƒ</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/vla/" rel="tag"># vla</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/08/04/cuda/" rel="prev" title="cuda">
                  <i class="fa fa-angle-left"></i> cuda
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/08/20/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%BB%91%E8%AF%9D/" rel="next" title="æ¨¡å‹è®­ç»ƒé»‘è¯">
                  æ¨¡å‹è®­ç»ƒé»‘è¯ <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">milong26</span>
  </div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> å¼ºåŠ›é©±åŠ¨
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="è¿”å›é¡¶éƒ¨">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
