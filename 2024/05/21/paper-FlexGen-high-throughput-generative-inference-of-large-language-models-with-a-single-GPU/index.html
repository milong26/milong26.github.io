<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhon.fun","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="推理llm里面搞卸载的。百篇阅读计划(5&#x2F;100)。看到和以前有点相关联的地方了，卸载和内存优化都是偏系统的。">
<meta property="og:type" content="article">
<meta property="og:title" content="paper|FlexGen: high-throughput generative inference of large language models with a single GPU">
<meta property="og:url" content="http://zhon.fun/2024/05/21/paper-FlexGen-high-throughput-generative-inference-of-large-language-models-with-a-single-GPU/index.html">
<meta property="og:site_name" content="RainBoarderSea">
<meta property="og:description" content="推理llm里面搞卸载的。百篇阅读计划(5&#x2F;100)。看到和以前有点相关联的地方了，卸载和内存优化都是偏系统的。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-05-21T02:37:15.000Z">
<meta property="article:modified_time" content="2024-05-28T08:17:50.627Z">
<meta property="article:author" content="milong26">
<meta property="article:tag" content="llm">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://zhon.fun/2024/05/21/paper-FlexGen-high-throughput-generative-inference-of-large-language-models-with-a-single-GPU/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://zhon.fun/2024/05/21/paper-FlexGen-high-throughput-generative-inference-of-large-language-models-with-a-single-GPU/","path":"2024/05/21/paper-FlexGen-high-throughput-generative-inference-of-large-language-models-with-a-single-GPU/","title":"paper|FlexGen: high-throughput generative inference of large language models with a single GPU"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>paper|FlexGen: high-throughput generative inference of large language models with a single GPU | RainBoarderSea</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">RainBoarderSea</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">何几久其世一生人</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#abstract"><span class="nav-number">1.</span> <span class="nav-text">abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">2.</span> <span class="nav-text">introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#related-works"><span class="nav-number">3.</span> <span class="nav-text">related works</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#backgroundllm-inference"><span class="nav-number">4.</span> <span class="nav-text">Background:LLM inference</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#offloading-strategy"><span class="nav-number">5.</span> <span class="nav-text">Offloading Strategy</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#approximate-methods"><span class="nav-number">6.</span> <span class="nav-text">Approximate Methods</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#evaluation"><span class="nav-number">7.</span> <span class="nav-text">Evaluation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#conclusion"><span class="nav-number">8.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93"><span class="nav-number">9.</span> <span class="nav-text">个人总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract-1"><span class="nav-number">9.1.</span> <span class="nav-text">abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction-1"><span class="nav-number">9.2.</span> <span class="nav-text">introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%9D%E5%A4%96"><span class="nav-number">9.3.</span> <span class="nav-text">额外</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">milong26</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhon.fun/2024/05/21/paper-FlexGen-high-throughput-generative-inference-of-large-language-models-with-a-single-GPU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="milong26">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RainBoarderSea">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="paper|FlexGen: high-throughput generative inference of large language models with a single GPU | RainBoarderSea">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          paper|FlexGen: high-throughput generative inference of large language models with a single GPU
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-21 10:37:15" itemprop="dateCreated datePublished" datetime="2024-05-21T10:37:15+08:00">2024-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-28 16:17:50" itemprop="dateModified" datetime="2024-05-28T16:17:50+08:00">2024-05-28</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>推理llm里面搞卸载的。百篇阅读计划(5/100)。看到和以前有点相关联的地方了，卸载和内存优化都是偏系统的。</p>
<span id="more"></span>
<ul>
<li>论文标题：FlexGen: high-throughput generative inference of large
language models</li>
<li>arxiv地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.06865">链接</a></li>
<li>code：<a
target="_blank" rel="noopener" href="https://github.com/FMInference/FlexGen">github</a></li>
<li>rank: ICML'23: Proceedings of the 40th International Conference on
Machine Learning</li>
<li>打标签：做llm推理的、</li>
</ul>
<h1 id="abstract">abstract</h1>
<ol type="1">
<li>小背景：<strong>批处理对延迟不敏感任务</strong>的新兴需求</li>
<li>干了什么：利用有限的资源，如单个商品GPU，展开了对<strong>高通量LLM推断</strong>的研究</li>
<li>做出来的成果：FlexGen，这是一个用于在GPU内存有限的情况下运行LLM的高通量生成引擎。</li>
<li>flexgen的特点：通过聚合来自GPU、CPU和磁盘的内存和计算，FlexGen可以在各种硬件资源约束下灵活配置。</li>
<li>它的原理：通过求解一个线性规划问题，它搜索有效的模式来存储和访问张量。FlexGen进一步将权重和注意力缓存<strong>压缩到4位</strong>，而精度损失可以忽略不计。这些技术使得FlexGen具有更大的批次大小选择空间，从而显著提高了最大吞吐量。</li>
<li>结果：当在单个16GB
GPU上运行OPT-175B时，FlexGen获得了显著高于当前最先进的装载系统的吞吐量，首次达到了1
token /
s的生成吞吐量，有效批处理大小为144。在HELM基准上，FlexGen可以在21小时内用16GB的GPU在7个具有代表性的子场景上对30B模型进行基准测试。</li>
</ol>
<ul class="task-list">
<li><label><input
type="checkbox" />批处理对延迟不敏感任务的新兴需求是啥</label></li>
<li><label><input type="checkbox" />高通量</label></li>
<li><label><input type="checkbox" />商品级的GPU有哪些</label></li>
<li><label><input type="checkbox" />怎么个灵活配置法</label></li>
<li><label><input type="checkbox" />先进的floading
system有什么</label></li>
<li><label><input type="checkbox" />HELM benchmark是什么</label></li>
</ul>
<h1 id="introduction">introduction</h1>
<ul>
<li><p>背景原因：还是因为做llm推理的计算和内存需求量大</p></li>
<li><p>基础设定：throughputoriented generative inference
面向吞吐量的生成式推理</p></li>
<li><p>研究思路：一些后台人物需要在大量token上批量llm推理，并且对延迟不太敏感。so可以在这些工作负载中权衡延迟以获得更高的吞吐量，从而为减少资源需求提供机会。</p></li>
<li><p>先前工作：3种方向</p>
<ul>
<li>模型压缩以减少总的内存占用</li>
<li>协同推理，通过去中心化来分摊推理成本</li>
<li>卸载，以从CPU和磁盘中使用内存</li>
</ul></li>
<li><p>先前工作的局限性：前两个方向的研究往往假设模型与GPU内存相契合，从而难以在单一商品GPU上运行175B规模的模型。另一方面，由于I
/
O调度和张量放置效率低下，第三类现有的基于卸载的系统在单个GPU上无法达到可接受的吞吐量。例如，这些系统可能会被小批量的(例如,在某些情况下,
OPT - 175B的批次大小只有一个或两个)难倒。</p></li>
<li><p>设计重点：<strong>在单个商品GPU上设计高效的卸载策略</strong>来进行<strong>高通量的生成式推理</strong>。</p></li>
<li><p>实现方法：为了在GPU内存有限的情况下运行LLM，我们可以将其卸载到二级存储，并通过部分加载来执行部分计算。</p></li>
<li><p>依据原理：在一个典型的机器上，有3个层次的内存层次结构，就是GPU、CPU、Disk的结构。在面向吞吐量的场景中，我们可以通过使用大批量来牺牲延迟，并在一个大批量的输入中，在不同的内存层次结构之间摊销昂贵的I/O操作，与计算重叠。值得注意的是，在资源有限的情况下，延迟和吞吐量方面的性能明显劣于资源充足的情况。</p></li>
<li><p>遇到挑战：offloading|compression</p>
<ul>
<li>设计高效的卸载策略：在generative推理过程中有3中张量：weight,activation,key-value缓存。batch,token,layer之间的计算也构成复杂的计算图，现有的基于卸载的推理系统(2022年)继承了来自训练的策略，执行了过多的I/O，实现了远低于理论硬件限制的吞吐量。</li>
<li>制定有效的压缩策略：prework已经证明了在压缩LLMs的权重和激活方面有很好的结果。然而，当结合压缩和卸载进行高通量推断时，权重和KV缓存的I
/ O成本和内存减少变得更为重要，这激发了替代的压缩方案。</li>
</ul></li>
<li><p>总结一下：FlexGen，一个用于高通量LLM推理的卸载框架。FlexGen聚合来自GPU、CPU和磁盘的内存，高效地调度I/O操作，以及可能的压缩方法和分布式流水线并行。</p></li>
<li><p>本文贡献：searching spacs|compress by fine-grained groupwise
quantization|demonstrate</p>
<ul>
<li>定义可能的卸载策略的搜索空间，考虑计算调度、张量放置、计算委托（computation
delegation）证明该搜索空间捕获了一个I / O复杂度在2
×最优性以内的计算顺序。然后，开发了一个基于线性规划的搜索算法来优化搜索空间内的吞吐量。该算法可以针对不同的硬件规格进行配置，并且可以很容易地扩展到包含延迟和吞吐量约束，从而帮助平滑地导航权衡空间。该算法可以针对不同的硬件规格进行配置，并且可以很容易地扩展到包含延迟和吞吐量约束，从而帮助平滑地导航权衡空间。与现有的策略相比，我们的解决方案统一了权重、激活和KV缓存的放置，从而实现了更高的批大小上限，这是实现高吞吐量的关键。</li>
<li>证明对于OPT -
175B这样的LLMs，可以在几乎不损失精度的情况下，将权重和KV缓存压缩到4比特，而无需重新训练或校准。这是通过细粒度的分组量化来实现的，适用于减少卸载时的I
/ O成本和内存使用。</li>
<li>我们通过在NVIDIA T4 ( 16GB ) GPU上运行OPT -
175B来展示FlexGen的效率。相比于深度零推理(DeepSpeed
Zero-Inference)和Hugging Face
Accelerate这两个最先进的基于卸载的推理系统，FlexGen往往允许更大数量级的批处理大小。因此，FlexGen可以获得更高的吞吐量。在单个T4
GPU上，CPU DRAM为208GB，SSD为1.5
TB，输入序列长度为512，输出序列长度为32：
<ul>
<li>同样在5000秒的延迟下，FlexGen (有效批次大小为64
,共2048个令牌)就可以实现比DeepSpeed Zero-Inference (批次大小为1
,共32个令牌)高40倍以上的吞吐率，而Hugged Face
Acceleration无法完成单个批次。</li>
<li>通过允许更高的延迟12000秒，FlexGen实现了比基线高69倍的最大吞吐量，因为它可以将有效批处理大小扩大到256个(总共生成了8192个令牌)，而DeepSpeed
Zero - Inference和Hugged Face
Acceleration由于内存溢出问题无法使用大于2的批处理大小。</li>
<li>如果允许4位压缩，FlexGen可以通过保持CPU中的所有权重和摆脱磁盘卸载，达到100
×高的最大吞吐量，有效批处理大小144 (共生成4608个令牌)，延迟4000秒。</li>
</ul></li>
</ul></li>
<li><p>额外验证：我们还比较了基于FlexGen和Petals
两个代表性系统的卸载和分散集体推理。我们从分散式网络的时延和带宽以及输出序列长度等方面对两个系统进行了比较。结果表明，FlexGen在单GPU吞吐量方面优于分散的Petals集群，在某些情况下甚至可以实现更低的延迟。</p></li>
<li><p><label><input type="checkbox" />Aminabadi et al., 2022;
HuggingFace,
2022的系统的基于卸载的推理系统是怎么实现的？论文里面只说它不好</label></p></li>
<li><p><label><input
type="checkbox" />看得有点懵，东西太多了。想整理一遍又无从下手</label></p></li>
</ul>
<h1 id="related-works">related works</h1>
<p>LLMs推理有两种方向：系统端和算法端。本文重点是<strong>系统端system
side</strong>。</p>
<p>目前大部分推理系统专注于具有<strong>高端加速器</strong>的<strong>面向延迟</strong>的场景，所以作者要做在<strong>易于访问的硬件上</strong>部署以<strong>吞吐量为导向</strong>的推理。</p>
<p>为了要在商品级的硬件上实现llm推理，需要好的offloading，但是现在专注这个的只有DeepSpeed
Zero-Inference 和 Hugging Face
Accelerate这两种，它们的原理是继承训练系统的offloading技术，忽略了生成推理的特殊计算特性。他们没有利用面向吞吐量的LLM推理计算的结构，错失了I/O流高效调度的大好机会。另一种在可访问硬件上实现LLM推理的尝试是Petals提出的协同计算。也有许多面向算法的工作，在LLM推断中放松某些方面的计算，以加速计算或减少内存占用，有稀疏化和量化两种方法。在量化方面，先前的工作已经表明，在不压缩激活的情况下，可以将权重压缩到3比特，或者将权重和激活都压缩到8比特。</p>
<p>在FlexGen中，我们将权重和KV缓存都压缩到4位，并展示了如何将压缩和卸载结合起来做进一步的改进。</p>
<p>在更广泛的领域内，内存优化和卸载已经被研究用于训练和线性代数。</p>
<h1 id="backgroundllm-inference">Background:LLM inference</h1>
<h1 id="offloading-strategy">Offloading Strategy</h1>
<h1 id="approximate-methods">Approximate Methods</h1>
<h1 id="evaluation">Evaluation</h1>
<h1 id="conclusion">Conclusion</h1>
<h1 id="个人总结">个人总结</h1>
<h2 id="abstract-1">abstract</h2>
<p>研究的东西：高通量(high throughput，说吞吐量也行？)LLM</p>
<p><em>我怎么越看越觉得它是其它领域的东西。。偏底层一点的？</em></p>
<h2 id="introduction-1">introduction</h2>
<h2 id="额外">额外</h2>
<ol type="1">
<li>做推理的llm有哪些？
<ol type="1">
<li>FasterTransformer (NVIDIA, 2022),</li>
<li>Orca (Yu et al., 2022),</li>
<li>LightSeq (Wang et al., 2021),</li>
<li>PaLM inference (Pope et al., 2022),</li>
<li>TurboTransformers (Fang et al., 2021),</li>
<li>DeepSpeed Inference (Aminabadi et al., 2022),</li>
<li>Hugging Face Accelerate (HuggingFace, 2022)</li>
</ol></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/llm/" rel="tag"># llm</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/05/21/%E9%87%91%E7%94%B0%E4%B8%80%E5%B0%91%E5%B9%B4%E4%BA%8B%E4%BB%B6%E7%B0%BF/" rel="prev" title="金田一少年事件簿">
                  <i class="fa fa-angle-left"></i> 金田一少年事件簿
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/05/21/paper-reading/" rel="next" title="paper reading">
                  paper reading <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">milong26</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
